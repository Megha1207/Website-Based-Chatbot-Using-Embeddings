[
  {
    "id": "page_0",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "domain": "en.wikipedia.org",
    "title": "Artificial intelligence - Wikipedia",
    "text": "Artificial intelligence | Part of a series on | | Artificial intelligence (AI) | |---| Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3] Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a] Reasoning and problem-solving Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14] Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem. Knowledge representation Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22] A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c] Planning and decision-making An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34] In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36] In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39] Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40] Learning Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e] There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45] In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48] Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49] Natural language processing Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51] Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57] Perception Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58] The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63] Social intelligence Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67] General intelligence A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68] Techniques AI research uses a wide variety of techniques to accomplish the goals above.[b] Search and optimization AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search. State space search State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71] Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73] Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74] Local search Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75] Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77] Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78] Logic Formal logic is used for reasoning and knowledge representation.[79] Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81] Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84] Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85] Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86] Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains. Probabilistic methods for uncertain reasoning Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92] Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation–maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91] Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91] Classifiers and statistical learning methods The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45] There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102] The naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104] Neural networks are also used as classifiers.[105] Artificial neural networks An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105] Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107] In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112] Deep learning Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114] Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j] GPT Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126] Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128] Hardware and software In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131] The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang. Applications AI and machine learning technology is used in most of the essential applications of the 2020s, including: - search engines (such as Google Search) - targeting online advertisements - recommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic - targeted advertising (AdSense, Facebook) - virtual assistants (such as Siri or Alexa) - autonomous vehicles (including drones, ADAS and self-driving cars) - automatic language translation (Microsoft Translator, Google Translate) - facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) - image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[134] AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[135] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[136] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[137][138] Games Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[139] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[140] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[141] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[142] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[143] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[144] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[145] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[146] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[147] Mathematics Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[148] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[149] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[150] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[151] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[152] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[153] Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[154] all from Google DeepMind,[155] Llemma from EleutherAI[156] or Julius.[157] When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[158] Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[159] Topological deep learning integrates various topological approaches. Finance Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[160] According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[161] Military Various countries are deploying AI military applications.[162] The main applications enhance command and control, communications, sensors, integration and interoperability.[163] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[162] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[163] AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[162][164][165][166] Generative AI Generative artificial intelligence (Generative AI or GenAI) is a subfield of artificial intelligence that uses generative models to generate text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data[167] in response to input, which often comes in the form of natural language prompts.[168][169] The prevalence of generative AI tools has increased significantly since the AI boom in the 2020s. This boom was made possible by improvements in deep neural networks, particularly large language models (LLMs), which are based on the transformer architecture. Major tools include LLM-based chatbots such as ChatGPT, Claude, Copilot, DeepSeek, Google Gemini and Grok; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTX and Sora.[170][171][172] Technology companies developing generative AI include Alibaba, Anthropic, Baidu, DeepSeek, Google, Lightricks,[173] Meta AI, Microsoft, Mistral AI, OpenAI, Perplexity AI, xAI,[174] and Yandex.[175] Generative AI has been adopted in a variety of sectors, including software development, healthcare,[176] finance,[177] entertainment,[178] customer service,[179] sales and marketing,[180] art, writing,[181] and product design.[182] Agents AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[183][184][185] Web search Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries[186] and step-by-step reasoning based of information from web publishers, ranked in Bing Search.[187] For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.[188] Google officially pushed its AI Search at its Google I/O event on 20 May 2025.[189] It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.[190] Sexuality Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[191] AI-integrated sex toys (e.g., teledildonics),[192] AI-generated sexual education content,[193] and AI agents that simulate sexual and romantic partners (e.g., Replika).[194] AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[195] AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[196][197] Other industry-specific tasks There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[198] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[199][200][201] In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[202] Ethics AI has potential benefits and potential risks.[205] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[206] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[207][208] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[209] Risks and harm Privacy and copyright Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio.[210] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[211] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[212] AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[213] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[214] Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[215][216] Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file.[217] However, some companies will scrape content regardless[218][219] because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[220][221] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[222] Dominance by tech giants The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[223][224][225] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[226][227] Power needs and environmental impacts In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[229] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[230] Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[231] A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[232] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[233] In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[234] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[235] In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[236] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[237] After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[238] Taiwan aims to phase out nuclear power by 2025.[238] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[238] Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI.[239] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[239] On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[240] According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[240] In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[241] Misinformation YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[242] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[243] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[244] In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[245] while realistic AI-generated videos became feasible in the mid-2020s.[246][247][248] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[249] one such potential malicious use is deepfakes for computational propaganda.[250] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[251] The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right.[252] AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[253] Algorithmic bias and fairness Machine learning applications can be biased[k] if they learn from biased data.[255] The developers may not be aware that the bias exists.[256] Discriminatory behavior by some LLMs can be observed in their output.[257] Bias can be introduced by the way training data is selected and by the way a model is deployed.[258][255] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[259] The field of fairness studies how to prevent harms from algorithmic biases. On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[260] a problem called \"sample size disparity\".[261] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[262] COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[263] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[265] A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[266] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[267] Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[268] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m] Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[261] There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[254] At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious – discuss][270] Lack of transparency Many AI systems are so complex that their designers cannot explain how they reach their decisions.[271] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[272] It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[273] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[274] People who have been harmed by an algorithm's decision have a right to an explanation.[275] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[276] DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[277] Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[278] LIME can locally approximate a model's outputs with a simpler, interpretable model.[279] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[280] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[281] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[282] Bad actors and weaponized AI Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[284] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[284] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[285] By 2015, over fifty countries were reported to be researching battlefield robots.[286] AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[287] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[288][289] There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[290] Technological unemployment Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[291] In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[292] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[293] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][295] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[291] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[296][297] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[298] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[299] In July 2025, Ford CEO Jim Farley predicted that \"artificial intelligence is going to replace literally half of all white-collar workers in the U.S.\"[300] From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[301] Existential risk It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[302] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips).[304] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[305] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[306] Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[307] Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\"[308] The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[309] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[310] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[311] He notably mentioned risks of an AI takeover,[312] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[313] In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[314] Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[315] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[316][317] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[318] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[319] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[320] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[321] Ethical machines and alignment Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[322] Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[323] The field of machine ethics is also called computational morality,[323] and was founded at an AAAI symposium in 2005.[324] Other approaches include Wendell Wallach's \"artificial moral agents\"[325] and Stuart J. Russell's three principles for developing provably beneficial machines.[326] Open source Active organizations in the AI open-source community include Hugging Face,[327] Google,[328] EleutherAI and Meta.[329] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[330][331] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[332] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[333] Frameworks Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[334][335] - Respect the dignity of individual people - Connect with other people sincerely, openly, and inclusively - Care for the wellbeing of everyone - Protect social values, justice, and the public interest Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[336] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[337] Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[338] The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[339] Regulation The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[340] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[341] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[342][343] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[344] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[344] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[344] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[345] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[346] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[347] On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation.[348] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[349] In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[342] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[350] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[351][352] In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[353] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[354][355] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[356][357] History The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[359][360] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[362] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[363][360] The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[360] Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[367] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[368] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[369] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[371] and ongoing pressure from the U.S. Congress to fund more productive projects.[372] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[373] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9] In the early 1980s, AI research was revived by the commercial success of expert systems,[374] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10] Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[375] and began to look into \"sub-symbolic\" approaches.[376] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][381] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[382] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[383] AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[384] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[385] However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68] Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11] For many specific tasks, other methods were abandoned.[y] Deep learning's success was based on both hardware improvements (faster computers,[387] graphics processing units, cloud computing[388]) and access to large amounts of data[389] (including curated datasets,[388] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[344] In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[321] In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[390] ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[391] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[392] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[393] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[394] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[395] Philosophy Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[396] Another major focus has been whether machines can be conscious, and the associated ethical implications.[397] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[398] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[397] Defining artificial intelligence Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[399] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[399] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[363] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[400] Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[402] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[403] McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[404] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[405] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine – and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google,[406] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself[407] including discussing the many AI narratives and myths to be found within societal, political and academic discourses.[408] Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms,[409] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[410] There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[411] Evaluating approaches to AI No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits Symbolic AI (or \"GOFAI\")[413] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[414] However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[415] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[416] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16] The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[418][419] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[420] but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[421][422] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[423] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. Consciousness David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[424] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[425] Computationalism and functionalism Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[426] Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[430] AI welfare and rights It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[431] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[432][433] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[432] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[434] In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[435] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[436][437] Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[433][432] Future Superintelligence and the singularity A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[422] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[438] However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[439] Transhumanism Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[440] Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[441] In fiction Thought-capable artificial beings have appeared as storytelling devices since antiquity,[442] and have been a persistent theme in science fiction.[443] A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[444] Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[445] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[446] Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[447] See also - Artificial consciousness – Field in cognitive science - Artificial intelligence and elections – Impact of AI on political elections - Artificial intelligence content detection – Software to detect AI-generated content - Artificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects - Association for the Advancement of Artificial Intelligence (AAAI) - Behavior selection algorithm – Algorithm that selects actions for intelligent agents - Business process automation – Automation of business processes - Case-based reasoning – Process of solving new problems based on the solutions of similar past problems - Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation - DARWIN EU – A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real world evidence (RWE) to support the evaluation and supervision of medicines across the EU - Digital immortality – Hypothetical concept of storing a personality in digital form - Emergent algorithm – Algorithm exhibiting emergent behavior - Female gendering of AI technologies – Gender biases in digital technology - Glossary of artificial intelligence – List of concepts in artificial intelligence - Intelligence amplification – Use of information technology to augment human intelligence - Intelligent agent – Software agent which acts autonomously - Intelligent automation – Software process that combines robotic process automation and artificial intelligence - List of artificial intelligence books - List of artificial intelligence journals - List of artificial intelligence projects - Mind uploading – Hypothetical process of digitally emulating a brain - Organoid intelligence – Use of brain cells and brain organoids for intelligent computing - Pseudorandomness – Appearing random but actually being generated by a deterministic, causal process - Robotic process automation – Form of business process automation technology - The Last Day – 1967 Welsh science fiction novel - Wetware computer – Computer composed of organic material Explanatory notes - ^ a b This list of intelligent traits is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021), Luger & Stubblefield (2004), Poole, Mackworth & Goebel (1998) and Nilsson (1998) - ^ a b This list of tools is based on the topics covered by the major AI textbooks, including: Russell & Norvig (2021), Luger & Stubblefield (2004), Poole, Mackworth & Goebel (1998) and Nilsson (1998) - ^ It is among the reasons that expert systems proved to be inefficient for capturing knowledge.[30][31] - ^ \"Rational agent\" is general term used in economics, philosophy and theoretical artificial intelligence. It can refer to anything that directs its behavior to accomplish goals, such as a person, an animal, a corporation, a nation, or in the case of AI, a computer program. - ^ Alan Turing discussed the centrality of learning as early as 1950, in his classic paper \"Computing Machinery and Intelligence\".[42] In 1956, at the original Dartmouth AI summer conference, Ray Solomonoff wrote a report on unsupervised probabilistic machine learning: \"An Inductive Inference Machine\".[43] - ^ See AI winter § Machine translation and the ALPAC report of 1966. - ^ Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve.[94] - ^ Expectation–maximization, one of the most popular algorithms in machine learning, allows clustering in the presence of unknown latent variables.[96] - ^ Some form of deep neural networks (without a specific learning algorithm) were described by: Warren S. McCulloch and Walter Pitts (1943)[117] Alan Turing (1948);[118] Karl Steinbuch and Roger David Joseph (1961).[119] Deep or recurrent networks that learned (or used gradient descent) were developed by: Frank Rosenblatt(1957);[118] Oliver Selfridge (1959);[119] Alexey Ivakhnenko and Valentin Lapa (1965);[120] Kaoru Nakano (1971);[121] Shun-Ichi Amari (1972);[121] John Joseph Hopfield (1982).[121] Precursors to backpropagation were developed by: Henry J. Kelley (1960);[118] Arthur E. Bryson (1962);[118] Stuart Dreyfus (1962);[118] Arthur E. Bryson and Yu-Chi Ho (1969);[118] Backpropagation was independently developed by: Seppo Linnainmaa (1970);[122] Paul Werbos (1974).[118] - ^ Geoffrey Hinton said, of his work on neural networks in the 1990s, \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\"[123] - ^ In statistics, a bias is a systematic error or deviation from the correct value. But in the context of fairness, it refers to a tendency in favor or against a certain group or individual characteristic, usually in a way that is considered unfair or harmful. A statistically unbiased AI system that produces disparate outcomes for different demographic groups may thus be viewed as biased in the ethical sense.[254] - ^ Including Jon Kleinberg (Cornell University), Sendhil Mullainathan (University of Chicago), Cynthia Chouldechova (Carnegie Mellon) and Sam Corbett-Davis (Stanford)[264] - ^ Moritz Hardt (a director at the Max Planck Institute for Intelligent Systems) argues that machine learning \"is fundamentally the wrong tool for a lot of domains, where you're trying to design interventions and mechanisms that change the world.\"[269] - ^ When the law was passed in 2018, it still contained a form of this provision. - ^ This is the United Nations' definition, and includes things like land mines as well.[283] - ^ See table 4; 9% is both the OECD average and the U.S. average.[294] - ^ Sometimes called a \"robopocalypse\"[303] - ^ \"Electronic brain\" was the term used by the press around this time.[359][361] - ^ Daniel Crevier wrote, \"the conference is generally recognized as the official birthdate of the new science.\"[364] Russell and Norvig called the conference \"the inception of artificial intelligence.\"[117] - ^ Russell and Norvig wrote \"for the next 20 years the field would be dominated by these people and their students.\"[365] - ^ Russell and Norvig wrote, \"it was astonishing whenever a computer did anything kind of smartish\".[366] - ^ The programs described are Arthur Samuel's checkers program for the IBM 701, Daniel Bobrow's STUDENT, Newell and Simon's Logic Theorist and Terry Winograd's SHRDLU. - ^ Russell and Norvig write: \"in almost all cases, these early systems failed on more difficult problems\"[370] - ^ Embodied approaches to AI[377] were championed by Hans Moravec[378] and Rodney Brooks[379] and went by many names: Nouvelle AI.[379] Developmental robotics.[380] - ^ Matteo Wong wrote in The Atlantic: \"Whereas for decades, computer-science fields such as natural-language processing, computer vision, and robotics used extremely different methods, now they all use a programming method called \"deep learning\". As a result, their code and approaches have become more similar, and their models are easier to integrate into one another.\"[386] - ^ Jack Clark wrote in Bloomberg: \"After a half-decade of quiet breakthroughs in artificial intelligence, 2015 has been a landmark year. Computers are smarter and learning faster than ever\", and noted that the number of software projects that use machine learning at Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects in 2015.[388] - ^ Nils Nilsson wrote in 1983: \"Simply put, there is wide disagreement in the field about what AI is all about.\"[412] - ^ Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"[417] - ^ Searle presented this definition of \"Strong AI\" in 1999.[427] Searle's original formulation was \"The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states.\"[428] Strong AI is defined similarly by Russell and Norvig: \"Stong AI – the assertion that machines that do so are actually thinking (as opposed to simulating thinking).\"[429] References - ^ a b c Russell & Norvig (2021), pp. 1–4. - ^ AI set to exceed human brain power Archived 19 February 2008 at the Wayback Machine CNN.com (26 July 2006) - ^ Kaplan, Andreas; Haenlein, Michael (2019). \"Siri, Siri, in my hand: Who's the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence\". Business Horizons. 62: 15–25. doi:10.1016/j.bushor.2018.08.004. [the question of the source is a pastiche of: Snow White] - ^ Russell & Norvig (2021, §1.2). - ^ \"Tech companies want to build artificial general intelligence. But who decides when AGI is attained?\". AP News. 4 April 2024. Retrieved 20 May 2025. - ^ a b Dartmouth workshop: Russell & Norvig (2021, p. 18), McCorduck (2004, pp. 111–136), NRC (1999, pp. 200–201) The proposal: McCarthy et al. (1955) - ^ a b Successful programs of the 1960s: McCorduck (2004, pp. 243–252), Crevier (1993, pp. 52–107), Moravec (1988, p. 9), Russell & Norvig (2021, pp. 19–21) - ^ a b Funding initiatives in the early 1980s: Fifth Generation Project (Japan), Alvey (UK), Microelectronics and Computer Technology Corporation (US), Strategic Computing Initiative (US): McCorduck (2004, pp. 426–441), Crevier (1993, pp. 161–162, 197–203, 211, 240), Russell & Norvig (2021, p. 23), NRC (1999, pp. 210–211), Newquist (1994, pp. 235–248) - ^ a b First AI Winter, Lighthill report, Mansfield Amendment: Crevier (1993, pp. 115–117), Russell & Norvig (2021, pp. 21–22), NRC (1999, pp. 212–213), Howe (1994), Newquist (1994, pp. 189–201) - ^ a b Second AI Winter: Russell & Norvig (2021, p. 24), McCorduck (2004, pp. 430–435), Crevier (1993, pp. 209–210), NRC (1999, pp. 214–216), Newquist (1994, pp. 301–318) - ^ a b Deep learning revolution, AlexNet: Goldman (2022), Russell & Norvig (2021, p. 26), McKinsey (2018) - ^ Toews (2023). - ^ Problem-solving, puzzle solving, game playing, and deduction: Russell & Norvig (2021, chpt. 3–5), Russell & Norvig (2021, chpt. 6) (constraint satisfaction), Poole, Mackworth & Goebel (1998, chpt. 2, 3, 7, 9), Luger & Stubblefield (2004, chpt. 3, 4, 6, 8), Nilsson (1998, chpt. 7–12) - ^ Uncertain reasoning: Russell & Norvig (2021, chpt. 12–18), Poole, Mackworth & Goebel (1998, pp. 345–395), Luger & Stubblefield (2004, pp. 333–381), Nilsson (1998, chpt. 7–12) - ^ a b c Intractability and efficiency and the combinatorial explosion: Russell & Norvig (2021, p. 21) - ^ a b c Psychological evidence of the prevalence of sub-symbolic reasoning and knowledge: Kahneman (2011), Dreyfus & Dreyfus (1986), Wason & Shapiro (1966), Kahneman, Slovic & Tversky (1982) - ^ Knowledge representation and knowledge engineering: Russell & Norvig (2021, chpt. 10), Poole, Mackworth & Goebel (1998, pp. 23–46, 69–81, 169–233, 235–277, 281–298, 319–345), Luger & Stubblefield (2004, pp. 227–243), Nilsson (1998, chpt. 17.1–17.4, 18) - ^ Smoliar & Zhang (1994). - ^ Neumann & Möller (2008). - ^ Kuperman, Reichley & Bailey (2006). - ^ McGarry (2005). - ^ Bertini, Del Bimbo & Torniai (2006). - ^ Russell & Norvig (2021), pp. 272. - ^ Representing categories and relations: Semantic networks, description logics, inheritance (including frames, and scripts): Russell & Norvig (2021, §10.2 & 10.5), Poole, Mackworth & Goebel (1998, pp. 174–177), Luger & Stubblefield (2004, pp. 248–258), Nilsson (1998, chpt. 18.3) - ^ Representing events and time:Situation calculus, event calculus, fluent calculus (including solving the frame problem): Russell & Norvig (2021, §10.3), Poole, Mackworth & Goebel (1998, pp. 281–298), Nilsson (1998, chpt. 18.2) - ^ Causal calculus: Poole, Mackworth & Goebel (1998, pp. 335–337) - ^ Representing knowledge about knowledge: Belief calculus, modal logics: Russell & Norvig (2021, §10.4), Poole, Mackworth & Goebel (1998, pp. 275–277) - ^ a b Default reasoning, Frame problem, default logic, non-monotonic logics, circumscription, closed world assumption, abduction: Russell & Norvig (2021, §10.6), Poole, Mackworth & Goebel (1998, pp. 248–256, 323–335), Luger & Stubblefield (2004, pp. 335–363), Nilsson (1998, ~18.3.3) (Poole et al. places abduction under \"default reasoning\". Luger et al. places this under \"uncertain reasoning\"). - ^ a b Breadth of commonsense knowledge: Lenat & Guha (1989, Introduction), Crevier (1993, pp. 113–114), Moravec (1988, p. 13), Russell & Norvig (2021, pp. 241, 385, 982) (qualification problem) - ^ Newquist (1994), p. 296. - ^ Crevier (1993), pp. 204–208. - ^ Russell & Norvig (2021), p. 528. - ^ Automated planning: Russell & Norvig (2021, chpt. 11). - ^ Automated decision making, Decision theory: Russell & Norvig (2021, chpt. 16–18). - ^ Classical planning: Russell & Norvig (2021, Section 11.2). - ^ Sensorless or \"conformant\" planning, contingent planning, replanning (a.k.a. online planning): Russell & Norvig (2021, Section 11.5). - ^ Uncertain preferences: Russell & Norvig (2021, Section 16.7) Inverse reinforcement learning: Russell & Norvig (2021, Section 22.6) - ^ Information value theory: Russell & Norvig (2021, Section 16.6). - ^ Markov decision process: Russell & Norvig (2021, chpt. 17). - ^ Game theory and multi-agent decision theory: Russell & Norvig (2021, chpt. 18). - ^ Learning: Russell & Norvig (2021, chpt. 19–22), Poole, Mackworth & Goebel (1998, pp. 397–438), Luger & Stubblefield (2004, pp. 385–542), Nilsson (1998, chpt. 3.3, 10.3, 17.5, 20) - ^ Turing (1950). - ^ Solomonoff (1956). - ^ Unsupervised learning: Russell & Norvig (2021, pp. 653) (definition), Russell & Norvig (2021, pp. 738–740) (cluster analysis), Russell & Norvig (2021, pp. 846–860) (word embedding) - ^ a b Supervised learning: Russell & Norvig (2021, §19.2) (Definition), Russell & Norvig (2021, Chpt. 19–20) (Techniques) - ^ Reinforcement learning: Russell & Norvig (2021, chpt. 22), Luger & Stubblefield (2004, pp. 442–449) - ^ Transfer learning: Russell & Norvig (2021, pp. 281), The Economist (2016) - ^ \"Artificial Intelligence (AI): What Is AI and How Does It Work? | Built In\". builtin.com. Retrieved 30 October 2023. - ^ Computational learning theory: Russell & Norvig (2021, pp. 672–674), Jordan & Mitchell (2015) - ^ Natural language processing (NLP): Russell & Norvig (2021, chpt. 23–24), Poole, Mackworth & Goebel (1998, pp. 91–104), Luger & Stubblefield (2004, pp. 591–632) - ^ Subproblems of NLP: Russell & Norvig (2021, pp. 849–850) - ^ Russell & Norvig (2021), pp. 856–858. - ^ Dickson (2022). - ^ Modern statistical and deep learning approaches to NLP: Russell & Norvig (2021, chpt. 24), Cambria & White (2014) - ^ Vincent (2019). - ^ Russell & Norvig (2021), pp. 875–878. - ^ Bushwick (2023). - ^ Computer vision: Russell & Norvig (2021, chpt. 25), Nilsson (1998, chpt. 6) - ^ Russell & Norvig (2021), pp. 849–850. - ^ Russell & Norvig (2021), pp. 895–899. - ^ Russell & Norvig (2021), pp. 899–901. - ^ Challa et al. (2011). - ^ Russell & Norvig (2021), pp. 931–938. - ^ MIT AIL (2014). - ^ Affective computing: Thro (1993), Edelson (1991), Tao & Tan (2005), Scassellati (2002) - ^ Waddell (2018). - ^ Poria et al. (2017). - ^ a b Artificial general intelligence: Russell & Norvig (2021, pp. 32–33, 1020–1021) Proposal for the modern version: Pennachin & Goertzel (2007) Warnings of overspecialization in AI from leading researchers: Nilsson (1995), McCarthy (2007), Beal & Winston (2009) - ^ Search algorithms: Russell & Norvig (2021, chpts. 3–5), Poole, Mackworth & Goebel (1998, pp. 113–163), Luger & Stubblefield (2004, pp. 79–164, 193–219), Nilsson (1998, chpts. 7–12) - ^ State space search: Russell & Norvig (2021, chpt. 3) - ^ Russell & Norvig (2021), sect. 11.2. - ^ Uninformed searches (breadth first search, depth-first search and general state space search): Russell & Norvig (2021, sect. 3.4), Poole, Mackworth & Goebel (1998, pp. 113–132), Luger & Stubblefield (2004, pp. 79–121), Nilsson (1998, chpt. 8) - ^ Heuristic or informed searches (e.g., greedy best first and A*): Russell & Norvig (2021, sect. 3.5), Poole, Mackworth & Goebel (1998, pp. 132–147), Poole & Mackworth (2017, sect. 3.6), Luger & Stubblefield (2004, pp. 133–150) - ^ Adversarial search: Russell & Norvig (2021, chpt. 5) - ^ Local or \"optimization\" search: Russell & Norvig (2021, chpt. 4) - ^ Singh Chauhan, Nagesh (18 December 2020). \"Optimization Algorithms in Neural Networks\". KDnuggets. Retrieved 13 January 2024. - ^ Evolutionary computation: Russell & Norvig (2021, sect. 4.1.2) - ^ Merkle & Middendorf (2013). - ^ Logic: Russell & Norvig (2021, chpts. 6–9), Luger & Stubblefield (2004, pp. 35–77), Nilsson (1998, chpt. 13–16) - ^ Propositional logic: Russell & Norvig (2021, chpt. 6), Luger & Stubblefield (2004, pp. 45–50), Nilsson (1998, chpt. 13) - ^ First-order logic and features such as equality: Russell & Norvig (2021, chpt. 7), Poole, Mackworth & Goebel (1998, pp. 268–275), Luger & Stubblefield (2004, pp. 50–62), Nilsson (1998, chpt. 15) - ^ Logical inference: Russell & Norvig (2021, chpt. 10) - ^ logical deduction as search: Russell & Norvig (2021, sects. 9.3, 9.4), Poole, Mackworth & Goebel (1998, pp. ~46–52), Luger & Stubblefield (2004, pp. 62–73), Nilsson (1998, chpt. 4.2, 7.2) - ^ Resolution and unification: Russell & Norvig (2021, sections 7.5.2, 9.2, 9.5) - ^ Warren, D.H.; Pereira, L.M.; Pereira, F. (1977). \"Prolog-the language and its implementation compared with Lisp\". ACM SIGPLAN Notices. 12 (8): 109–115. doi:10.1145/872734.806939. - ^ Fuzzy logic: Russell & Norvig (2021, pp. 214, 255, 459), Scientific American (1999) - ^ a b Stochastic methods for uncertain reasoning: Russell & Norvig (2021, chpt. 12–18, 20), Poole, Mackworth & Goebel (1998, pp. 345–395), Luger & Stubblefield (2004, pp. 165–191, 333–381), Nilsson (1998, chpt. 19) - ^ decision theory and decision analysis: Russell & Norvig (2021, chpt. 16–18), Poole, Mackworth & Goebel (1998, pp. 381–394) - ^ Information value theory: Russell & Norvig (2021, sect. 16.6) - ^ Markov decision processes and dynamic decision networks: Russell & Norvig (2021, chpt. 17) - ^ a b c Stochastic temporal models: Russell & Norvig (2021, chpt. 14) Hidden Markov model: Russell & Norvig (2021, sect. 14.3) Kalman filters: Russell & Norvig (2021, sect. 14.4) Dynamic Bayesian networks: Russell & Norvig (2021, sect. 14.5) - ^ Game theory and mechanism design: Russell & Norvig (2021, chpt. 18) - ^ Bayesian networks: Russell & Norvig (2021, sects. 12.5–12.6, 13.4–13.5, 14.3–14.5, 16.5, 20.2–20.3), Poole, Mackworth & Goebel (1998, pp. 361–381), Luger & Stubblefield (2004, pp. ~182–190, ≈363–379), Nilsson (1998, chpt. 19.3–19.4) - ^ Domingos (2015), chpt. 6. - ^ Bayesian inference algorithm: Russell & Norvig (2021, sect. 13.3–13.5), Poole, Mackworth & Goebel (1998, pp. 361–381), Luger & Stubblefield (2004, pp. ~363–379), Nilsson (1998, chpt. 19.4 & 7) - ^ Domingos (2015), p. 210. - ^ Bayesian learning and the expectation–maximization algorithm: Russell & Norvig (2021, chpt. 20), Poole, Mackworth & Goebel (1998, pp. 424–433), Nilsson (1998, chpt. 20), Domingos (2015, p. 210) - ^ Bayesian decision theory and Bayesian decision networks: Russell & Norvig (2021, sect. 16.5) - ^ Statistical learning methods and classifiers: Russell & Norvig (2021, chpt. 20), - ^ Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI. Intellisemantic Editions. ISBN 978-8-8947-8760-3. - ^ Decision trees: Russell & Norvig (2021, sect. 19.3), Domingos (2015, p. 88) - ^ Non-parameteric learning models such as K-nearest neighbor and support vector machines: Russell & Norvig (2021, sect. 19.7), Domingos (2015, p. 187) (k-nearest neighbor) - Domingos (2015, p. 88) (kernel methods) - ^ Domingos (2015), p. 152. - ^ Naive Bayes classifier: Russell & Norvig (2021, sect. 12.6), Domingos (2015, p. 152) - ^ a b Neural networks: Russell & Norvig (2021, chpt. 21), Domingos (2015, Chapter 4) - ^ Gradient calculation in computational graphs, backpropagation, automatic differentiation: Russell & Norvig (2021, sect. 21.2), Luger & Stubblefield (2004, pp. 467–474), Nilsson (1998, chpt. 3.3) - ^ Universal approximation theorem: Russell & Norvig (2021, p. 752) The theorem: Cybenko (1988), Hornik, Stinchcombe & White (1989) - ^ Feedforward neural networks: Russell & Norvig (2021, sect. 21.1) - ^ Perceptrons: Russell & Norvig (2021, pp. 21, 22, 683, 22) - ^ a b Deep learning: Russell & Norvig (2021, chpt. 21), Goodfellow, Bengio & Courville (2016), Hinton et al. (2016), Schmidhuber (2015) - ^ Recurrent neural networks: Russell & Norvig (2021, sect. 21.6) - ^ Convolutional neural networks: Russell & Norvig (2021, sect. 21.3) - ^ Sindhu V, Nivedha S, Prakash M (February 2020). \"An Empirical Science Research on Bioinformatics in Machine Learning\". Journal of Mechanics of Continua and Mathematical Sciences (7). doi:10.26782/jmcms.spl.7/2020.02.00006. - ^ Deng & Yu (2014), pp. 199–200. - ^ Ciresan, Meier & Schmidhuber (2012). - ^ Russell & Norvig (2021), p. 750. - ^ a b c Russell & Norvig (2021), p. 17. - ^ a b c d e f g Russell & Norvig (2021), p. 785. - ^ a b Schmidhuber (2022), sect. 5. - ^ Schmidhuber (2022), sect. 6. - ^ a b c Schmidhuber (2022), sect. 7. - ^ Schmidhuber (2022), sect. 8. - ^ Quoted in Christian (2020, p. 22) - ^ Metz, Cade; Weise, Karen (5 May 2025). \"A.I. Hallucinations Are Getting Worse, Even as New Systems Become More Powerful\". The New York Times. ISSN 0362-4331. Retrieved 6 May 2025. - ^ Smith (2023). - ^ \"Explained: Generative AI\". MIT News | Massachusetts Institute of Technology. 9 November 2023. - ^ \"AI Writing and Content Creation Tools\". MIT Sloan Teaching & Learning Technologies. Archived from the original on 25 December 2023. Retrieved 25 December 2023. - ^ Marmouyet (2023). - ^ Kobielus (2019). - ^ Thomason, James (21 May 2024). \"Mojo Rising: The resurgence of AI-first programming languages\". VentureBeat. Archived from the original on 27 June 2024. Retrieved 26 May 2024. - ^ Wodecki, Ben (5 May 2023). \"7 AI Programming Languages You Need to Know\". AI Business. Archived from the original on 25 July 2024. Retrieved 5 October 2024. - ^ Plumb, Taryn (18 September 2024). \"Why Jensen Huang and Marc Benioff see 'gigantic' opportunity for agentic AI\". VentureBeat. Archived from the original on 5 October 2024. Retrieved 4 October 2024. - ^ Mims, Christopher (19 September 2020). \"Huang's Law Is the New Moore's Law, and Explains Why Nvidia Wants Arm\". Wall Street Journal. ISSN 0099-9660. Archived from the original on 2 October 2023. Retrieved 19 January 2025. - ^ Dankwa-Mullan, Irene (2024). \"Health Equity and Ethical Considerations in Using Artificial Intelligence in Public Health and Medicine\". Preventing Chronic Disease. 21 240245: E64. doi:10.5888/pcd21.240245. ISSN 1545-1151. PMC 11364282. PMID 39173183. - ^ Jumper, J; Evans, R; Pritzel, A (2021). \"Highly accurate protein structure prediction with AlphaFold\". Nature. 596 (7873): 583–589. Bibcode:2021Natur.596..583J. doi:10.1038/s41586-021-03819-2. PMC 8371605. PMID 34265844. - ^ Fullname}, #Author (20 December 2023). \"AI discovers new class of antibiotics to kill drug-resistant bacteria\". New Scientist. Archived from the original on 16 September 2024. Retrieved 5 October 2024. {{cite web}} :|first1= has generic name (help)CS1 maint: numeric names: authors list (link) - ^ \"AI speeds up drug design for Parkinson's ten-fold\". University of Cambridge. Cambridge University. 17 April 2024. Archived from the original on 5 October 2024. Retrieved 5 October 2024. - ^ Horne, Robert I.; Andrzejewska, Ewa A.; Alam, Parvez; Brotzakis, Z. Faidon; Srivastava, Ankit; Aubert, Alice; Nowinska, Magdalena; Gregory, Rebecca C.; Staats, Roxine; Possenti, Andrea; Chia, Sean; Sormanni, Pietro; Ghetti, Bernardino; Caughey, Byron; Knowles, Tuomas P. J.; Vendruscolo, Michele (17 April 2024). \"Discovery of potent inhibitors of α-synuclein aggregation using structure-based iterative learning\". Nature Chemical Biology. 20 (5). Nature: 634–645. doi:10.1038/s41589-024-01580-x. PMC 11062903. PMID 38632492. - ^ Grant, Eugene F.; Lardner, Rex (25 July 1952). \"The Talk of the Town – It\". The New Yorker. ISSN 0028-792X. Archived from the original on 16 February 2020. Retrieved 28 January 2024. - ^ Anderson, Mark Robert (11 May 2017). \"Twenty years on from Deep Blue vs Kasparov: how a chess match started the big data revolution\". The Conversation. Archived from the original on 17 September 2024. Retrieved 28 January 2024. - ^ Markoff, John (16 February 2011). \"Computer Wins on 'Jeopardy!': Trivial, It's Not\". The New York Times. ISSN 0362-4331. Archived from the original on 22 October 2014. Retrieved 28 January 2024. - ^ Byford, Sam (27 May 2017). \"AlphaGo retires from competitive Go after defeating world number one 3–0\". The Verge. Archived from the original on 7 June 2017. Retrieved 28 January 2024. - ^ Brown, Noam; Sandholm, Tuomas (30 August 2019). \"Superhuman AI for multiplayer poker\". Science. 365 (6456): 885–890. Bibcode:2019Sci...365..885B. doi:10.1126/science.aay2400. PMID 31296650. - ^ \"MuZero: Mastering Go, chess, shogi and Atari without rules\". Google DeepMind. 23 December 2020. Retrieved 28 January 2024. - ^ Sample, Ian (30 October 2019). \"AI becomes grandmaster in 'fiendishly complex' StarCraft II\". The Guardian. ISSN 0261-3077. Archived from the original on 29 December 2020. Retrieved 28 January 2024. - ^ Wurman, P. R.; Barrett, S.; Kawamoto, K. (2022). \"Outracing champion Gran Turismo drivers with deep reinforcement learning\" (PDF). Nature. 602 (7896): 223–228. Bibcode:2022Natur.602..223W. doi:10.1038/s41586-021-04357-7. PMID 35140384. - ^ Wilkins, Alex (13 March 2024). \"Google AI learns to play open-world video games by watching them\". New Scientist. Archived from the original on 26 July 2024. Retrieved 21 July 2024. - ^ Wu, Zhengxuan; Arora, Aryaman; Wang, Zheng; Geiger, Atticus; Jurafsky, Dan; Manning, Christopher D.; Potts, Christopher (2024). \"ReFT: Representation Finetuning for Language Models\". NeurIPS. arXiv:2404.03592. - ^ \"Improving mathematical reasoning with process supervision\". OpenAI. 31 May 2023. Retrieved 26 January 2025. - ^ Srivastava, Saurabh (29 February 2024). \"Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap\". arXiv:2402.19450 [cs.AI]. - ^ Lightman, Hunter; Kosaraju, Vineet; Burda, Yura; Edwards, Harri; Baker, Bowen; Lee, Teddy; Leike, Jan; Schulman, John; Sutskever, Ilya; Cobbe, Karl (2023). \"Let's Verify Step by Step\". arXiv:2305.20050v1 [cs.LG]. - ^ Franzen, Carl (8 August 2024). \"Alibaba claims no. 1 spot in AI math models with Qwen2-Math\". VentureBeat. Retrieved 16 February 2025. - ^ Franzen, Carl (9 January 2025). \"Microsoft's new rStar-Math technique upgrades small models to outperform OpenAI's o1-preview at math problems\". VentureBeat. Retrieved 26 January 2025. - ^ Gina Genkina: New AI Model Advances the \"Kissing Problem\" and More. AlphaEvolve made several mathematical discoveries and practical optimizations IEEE Spectrum 14 May 2025. Retrieved 7 June 2025 - ^ Roberts, Siobhan (25 July 2024). \"AI achieves silver-medal standard solving International Mathematical Olympiad problems\". The New York Times. Archived from the original on 26 September 2024. Retrieved 7 August 2024. - ^ Azerbayev, Zhangir; Schoelkopf, Hailey; Paster, Keiran; Santos, Marco Dos; McAleer', Stephen; Jiang, Albert Q.; Deng, Jia; Biderman, Stella; Welleck, Sean (16 October 2023). \"Llemma: An Open Language Model For Mathematics\". EleutherAI Blog. Retrieved 26 January 2025. - ^ \"Julius AI\". julius.ai. - ^ Metz, Cade (21 July 2025). \"Google A.I. System Wins Gold Medal in International Math Olympiad\". The New York Times. ISSN 0362-4331. Retrieved 24 July 2025. - ^ McFarland, Alex (12 July 2024). \"8 Best AI for Math Tools (January 2025)\". Unite.AI. Retrieved 26 January 2025. - ^ Matthew Finio & Amanda Downie: IBM Think 2024 Primer, \"What is Artificial Intelligence (AI) in Finance?\" 8 December 2023 - ^ M. Nicolas, J. Firzli: Pensions Age / European Pensions magazine, \"Artificial Intelligence: Ask the Industry\", May–June 2024. https://videovoice.org/ai-in-finance-innovation-entrepreneurship-vs-over-regulation-with-the-eus-artificial-intelligence-act-wont-work-as-intended/ Archived 11 September 2024 at the Wayback Machine. - ^ a b c Congressional Research Service (2019). Artificial Intelligence and National Security (PDF). Washington, DC: Congressional Research Service. Archived (PDF) from the original on 8 May 2020. Retrieved 25 February 2024.PD-notice - ^ a b Slyusar, Vadym (2019). Artificial intelligence as the basis of future control networks (Preprint). doi:10.13140/RG.2.2.30247.50087. - ^ Iraqi, Amjad (3 April 2024). \"'Lavender': The AI machine directing Israel's bombing spree in Gaza\". +972 Magazine. Archived from the original on 10 October 2024. Retrieved 6 April 2024. - ^ Davies, Harry; McKernan, Bethan; Sabbagh, Dan (1 December 2023). \"'The Gospel': how Israel uses AI to select bombing targets in Gaza\". The Guardian. Archived from the original on 6 December 2023. Retrieved 4 December 2023. - ^ Marti, J Werner (10 August 2024). \"Drohnen haben den Krieg in der Ukraine revolutioniert, doch sie sind empfindlich auf Störsender – deshalb sollen sie jetzt autonom operieren\". Neue Zürcher Zeitung (in German). Archived from the original on 10 August 2024. Retrieved 10 August 2024. - ^ Pasick, Adam (27 March 2023). \"Artificial Intelligence Glossary: Neural Networks and Other Terms Explained\". The New York Times. ISSN 0362-4331. Archived from the original on 1 September 2023. Retrieved 22 April 2023. - ^ Griffith, Erin; Metz, Cade (27 January 2023). \"Anthropic Said to Be Closing In on $300 Million in New A.I. Funding\". The New York Times. Archived from the original on 9 December 2023. Retrieved 14 March 2023. - ^ Lanxon, Nate; Bass, Dina; Davalos, Jackie (10 March 2023). \"A Cheat Sheet to AI Buzzwords and Their Meanings\". Bloomberg News. Archived from the original ",
    "text_length": 120000,
    "depth": 0,
    "crawled_at": "2026-01-09T19:30:40.527119"
  },
  {
    "id": "page_1",
    "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
    "domain": "en.wikipedia.org",
    "title": "Artificial general intelligence - Wikipedia",
    "text": "Artificial general intelligence | Part of a series on | | Artificial intelligence (AI) | |---| Artificial general intelligence (AGI) is a hypothetical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.[1][2] Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin.[3] Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.[4] Creating AGI is a stated goal of AI technology companies such as OpenAI,[5] Google,[6] xAI,[7] and Meta.[8] A 2020 survey identified 72 active AGI research and development projects across 37 countries.[9] AGI is a common topic in science fiction and futures studies.[10][11] Contention exists over whether AGI represents an existential risk.[12][13][14] Some AI experts and industry figures have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.[15][16][17] Terminology [edit]AGI is also known as strong AI,[18][19] full AI,[20] human-level AI,[21] human-level intelligent AI, or general intelligent action.[22] Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness.[a] In contrast, weak AI (or narrow AI) can solve one specific problem but lacks general cognitive abilities.[23][19] Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a] Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[24] while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[25] A framework for classifying AGI was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans).[26] Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).[27] Characteristics [edit]There is no single agreed-upon definition of intelligence as applied to computers. Computer scientist John McCarthy wrote in 2007: \"We cannot yet characterize in general what kinds of computational procedures we want to call intelligent.\"[28] Intelligence traits [edit]Researchers generally hold that a system is required to do all of the following to be regarded as an AGI:[29] - reason, use strategy, solve puzzles, and make judgments under uncertainty, - represent knowledge, including common sense knowledge, - plan, - learn, - communicate in natural language, - if necessary, integrate these skills in completion of any given goal. Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts)[30] and autonomy.[31] Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.[32] Physical traits [edit]Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:[33] - the ability to sense (e.g. see, hear, etc.), and - the ability to act (e.g. move and manipulate objects, change location to explore, etc.) This includes the ability to detect and respond to hazard.[33] Tests for human-level AGI [edit]Several tests meant to confirm human-level AGI have been considered, including:[34][35] - The Turing Test (Turing) - Proposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge that it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.[37] - Turing described the test as follows: The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be experts about machines, must be taken in by the pretence.[38] - In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.[39][40] - In 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\". Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"[41] - A 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test—surpassing older chatbots like ELIZA while still falling behind actual humans (67%).[42] - A 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations—surpassing the 67% humanness rate of real confederates and meeting the researchers' criterion for having passed the test.[43][44] - The Robot College Student Test (Goertzel) - A machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.[45] - The Employment Test (Nilsson) - A machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.[46] - The Ikea test (Marcus) - Also known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.[47] - The Coffee Test (Wozniak) - A machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons.[48] Robots developed by Figure AI and other robotics companies can perform tasks like this. - The Modern Turing Test (Suleyman) - An AI model is given $100,000 and has to obtain $1 million.[49][50] - The General Video-Game Learning Test (Goertzel, Bach et al.) - An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition.[51][52] The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI.[53] AI-complete problems [edit]A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm.[54] Many problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.[55] Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.[56] History [edit]Classical AI [edit]Modern AI research began in the mid-1950s.[57] The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.[58] AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"[59] Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant[60] on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".[61] Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".[b] In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".[65] In response to this and the success of expert systems, both industry and government pumped money into the field.[63][66] However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.[67] For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all[c] and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".[69] Narrow AI research [edit]In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms.[70] These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018[update], development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.[71] At the turn of the century, many mainstream AI researchers[72] hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than halfway, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven, uniting the two efforts.[72] However, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).[73] Modern artificial general intelligence research [edit]The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud[74] in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximizes \"the ability to satisfy goals in a wide range of environments\".[75] This type of AGI, characterized by the ability to maximize a mathematical definition of intelligence rather than exhibit human-like behaviour,[76] was also called universal artificial intelligence.[77] The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002.[78] AGI research activity in 2006 was described by Pei Wang and Ben Goertzel[79] as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009[80] by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010[81] and 2011[82] at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility [edit]As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.[83] AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".[84] Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.[85] An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?[86] Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.[87][88] John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.[89] AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question, but with a 90% confidence instead.[90][91] Further current AGI progress considerations can be found above Tests for confirming human-level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.[92] In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[93] Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.[94][95] Blaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".[96] Timescales [edit]Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.[87] Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.[87][99][100] For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.[101] In the introduction to his 2006 book,[102] Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007[update], the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near[103] (i.e. between 2015 and 2045) was plausible.[104] Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.[105] In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).[106] AlexNet was regarded as the initial ground-breaker of the current deep learning wave.[106] In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.[107][108] In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.[109] In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.[110] In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.[111] In 2023, AI researcher Geoffrey Hinton stated that:[112] The idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that. He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.[113] In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow, expecting AGI within a decade or even a few years.[114] In March 2024, Nvidia's Chief Executive Officer (CEO), Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.[115] In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".[116] In September 2025, a review of surveys of scientists and industry experts from the last 15 years reported that most agreed that artificial general intelligence (AGI) will occur before the year 2100.[117] A more recent analysis by AIMultiple reported that, “Current surveys of AI researchers are predicting AGI around 2040”.[117] Whole brain emulation [edit]While the development of transformer models like in ChatGPT is considered the most promising path to AGI,[118][119] whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.[120] Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research[104] as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near[103] predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it. Early estimates [edit]For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion).[122] An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).[123] In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second.[d] (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict that the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued. Current research [edit]The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain.[126] In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain. Criticisms of simulation-based approaches [edit]The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.[127] A fundamental criticism of the simulated brain approach derives from embodied cognition theory, which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.[128][129] If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel[104] proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient. Philosophical perspective [edit]\"Strong AI\" as defined in philosophy [edit]In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument.[130] He proposed a distinction between two hypotheses about artificial intelligence:[e] - Strong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\". - Weak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness. The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.[131] In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".[103] This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers, the question is out of scope.[132] Mainstream AI is most interested in how a program behaves.[133] According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\"[132] If the program can behave as if it has a mind, then there is no need to know if it actually has a mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[132] Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness [edit]Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence: - Sentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience.[134] Determining why and how subjective experience arises is known as the hard problem of consciousness.[135] Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not.[136] In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.[137] - Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\".[f] In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks.[citation needed] These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.[138] Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.[139] Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.[140] Benefits [edit]AGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer.[141] It could take care of the elderly,[142] and democratize access to rapid, high-quality medical diagnostics. It could offer fun, inexpensive and personalized education.[142] The need to work to subsist could become obsolete if the wealth produced is properly redistributed.[142][143] This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks.[144] If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true),[145] it could take measures to drastically reduce the risks[144] while minimizing the impact of these measures on our quality of life. Advancements in medicine and healthcare [edit]AGI would improve healthcare by making medical diagnostics faster, less expensive, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage.[146] This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.[147] Additionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's disease.[148] In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems. By evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.[149] Advancements in science and technology [edit]AGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems.[150] Problems that have remained unsolved for decades may be solved with AGI. AGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing.[151] Future AGI systems could push these innovations further. Enhancing education and productivity [edit]AGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.[152] In the workplace, AGI could automate repetitive tasks, freeing workers for more creative and strategic roles.[151] It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.[153] Mitigating global crises [edit]AGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics.[154] By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties. In climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences.[150] Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare. Revitalising environmental conservation and biodiversity [edit]AGI could significantly contribute to preserving the natural environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies.[155] AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.[156] Enhancing space exploration and colonization [edit]AGI could revolutionize humanity's ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, expanding the possibilities for interplanetary civilization.[157] Risks [edit]Existential risks [edit]AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[158] The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench them, preventing moral progress.[159] Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create an entrenched repressive worldwide totalitarian regime.[160][161] There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass-created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.[162][163] Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".[160] Risk of loss of control and human extinction [edit]The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.[164][165] In 2014, Stephen Hawking criticized widespread indifference: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[166] The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as collateral damage from human activities.[167] The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intentions as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".[168] On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.[169] Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?[170][171] Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors),[172] and the use of AI in weapon systems.[173] The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short term, or that concerns about AGI distract from other issues related to current AI.[174] Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.[175] Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.[176] Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.[177][178] In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[165] Mass unemployment [edit]Researchers from OpenAI estimated in 2023 that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".[179][180] They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.[180] AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies. A common belief among top AI company insiders is that most workers will face technological unemployment from AGI, starting with white-collar jobs and, as robotics improves, extending to blue-collar jobs.[181] Critics of the idea argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.[182][183][184] According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:[143] Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality Elon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI).[185] Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment.[186] In 2023, Hinton said \"I'm a socialist [...] I think that private ownership of the media, and of the 'means of computation', is not good.\"[187] See also [edit]- AI alignment – Conformance of AI to intended objectives - AI effect - A.I. Rising – 2018 film directed by Lazar Bodroža - Artificial brain – Software and hardware with cognitive abilities similar to those of the animal or human brain - Automated machine learning – Process of automating the application of machine learning - BRAIN Initiative – Public-private research initiative - China Brain Project – Chinese neuroscience program - Future of Humanity Institute – Defunct research centre in Oxford, UK - General game playing – Ability of artificial intelligence to play different games - Generative artificial intelligence – Subset of AI using generative models - Hardware for artificial intelligence – Hardware specially designed and optimized for artificial intelligence - Human Brain Project – Scientific research project - Intelligence amplification – Use of information technology to augment human intelligence (IA) - Loebner Prize – Annual AI competition - Machine ethics – Moral behaviours of man-made machines - Moravec's paradox - Multi-task learning – Solving multiple machine learning tasks at the same time - Neural scaling law – Statistical law in machine learning - Outline of artificial intelligence – Overview of and topical guide to artificial intelligence - Synthetic intelligence – Alternate term for or form of artificial intelligence - Technological Singularity – Hypothetical event - The Future of Work and Death - Transfer learning – Machine learning technique - Transhumanism – Philosophical movement - Universal psychometrics - Weak artificial intelligence – Form of artificial intelligence Notes [edit]- ^ a b See below for the origin of the term \"strong AI\", and see the academic definition of \"strong AI\" and weak AI in the article Chinese room. - ^ The Lighthill report specifically criticized AI's \"grandiose objectives\" and led the dismantling of AI research in England.[62] In the U.S., DARPA became determined to fund only \"mission-oriented direct research, rather than basic undirected research\".[63][64] - ^ As AI founder John McCarthy writes \"it would be a great relief to the rest of the workers in AI if the inventors of new general formalisms would express their hopes in a more guarded form than has sometimes been the case.\"[68] - ^ In \"Mind Children\"[124] 1015 cps is used. More recently, in 1997,[125] Moravec argued for 108 MIPS which would roughly correspond to 1014 cps. Moravec talks in terms of MIPS, not \"cps\", which is a non-standard term Kurzweil introduced. - ^ As defined in a standard AI textbook: \"The assertion that machines could act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis.\"[123] - ^ Alan Turing made this point in 1950.[37] References [edit]- ^ Goertzel, Ben (2014). \"Artificial General Intelligence: Concept, State of the Art, and Future Prospects\". Journal of Artificial General Intelligence. 5 (1): 1–48. Bibcode:2014JAGI....5....1G. doi:10.2478/jagi-2014-0001. - ^ Lake, Brenden; Ullman, Tom; Tenenbaum, Joshua; Gershman, Samuel (2017). \"Building machines that learn and think like people\". Behavioral and Brain Sciences. 40 e253. arXiv:1604.00289. doi:10.1017/S0140525X16001837. PMID 27881212. - ^ Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press. - ^ Legg, Shane (2023). Why AGI Might Not Need Agency. Proceedings of the Conference on Artificial General Intelligence. - ^ \"OpenAI Charter\". OpenAI. Retrieved 6 April 2023. Our mission is to ensure that artificial general intelligence benefits all of humanity. - ^ Grant, Nico (27 February 2025). \"Google's Sergey Brin Asks Workers to Spend More Time In the Office\". The New York Times. ISSN 0362-4331. Retrieved 1 March 2025. - ^ Newsham, Jack. \"Tesla said xAI stands for \"eXploratory Artificial Intelligence.\" It's not clear where it got that\". Business Insider. Retrieved 20 September 2025. - ^ Heath, Alex (18 January 2024). \"Mark Zuckerberg's new goal is creating artificial general intelligence\". The Verge. Retrieved 13 June 2024. Our vision is to build AI that is better than human-level at all of the human senses. - ^ Baum, Seth D. (2020). A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy (PDF) (Report). Global Catastrophic Risk Institute. Retrieved 28 November 2024. 72 AGI R&D projects were identified as being active in 2020. - ^ Butler, Octavia E. (1993). Parable of the Sower. Grand Central Publishing. ISBN 978-0-4466-7550-5. All that you touch you change. All that you change changes you. - ^ Vinge, Vernor (1992). A Fire Upon the Deep. Tor Books. ISBN 978-0-8125-1528-2. The Singularity is coming. - ^ Morozov, Evgeny (30 June 2023). \"The True Threat of Artificial Intelligence\". The New York Times. The real threat is not AI itself but the way we deploy it. - ^ \"Impressed by artificial intelligence? Experts say AGI is coming next, and it has 'existential' risks\". ABC News. 23 March 2023. Retrieved 6 April 2023. AGI could pose existential risks to humanity. - ^ Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press. ISBN 978-0-1996-7811-2. The first superintelligence will be the last invention that humanity needs to make. - ^ Roose, Kevin (30 May 2023). \"A.I. Poses 'Risk of Extinction', Industry Leaders Warn\". The New York Times. Mitigating the risk of extinction from AI should be a global priority. - ^ Mitchell, Melanie (30 May 2023). \"Are AI's Doomsday Scenarios Worth Taking Seriously?\". The New York Times. We are far from creating machines that can outthink us in general ways. - ^ LeCun, Yann (June 2023). \"AGI does not present an existential risk\". Medium. There is no reason to fear AI as an existential threat. - ^ Kurzweil 2005, p. 260. - ^ a b Kurzweil, Ray (5 August 2005), \"Long Live AI\", Forbes, archived from the original on 14 August 2005: Kurzweil describes strong AI as \"machine intelligence with the full range of human intelligence.\" - ^ \"The Age of Artificial Intelligence: George John at TEDxLondonBusinessSchool 2013\". Archived from the original on 26 February 2014. Retrieved 22 February 2014. - ^ a b Roser, Max (7 February 2023). \"AI timelines: What do experts in artificial intelligence expect for the future?\". Our World in Data. Retrieved 6 April 2023. - ^ Newell & Simon 1976, This is the term they use for \"human-level\" intelligence in the physical symbol system hypothesis. - ^ \"The Open University on Strong and Weak AI\". Archived from the original on 25 September 2009. Retrieved 8 October 2007. - ^ \"What is artificial superintelligence (ASI)? | Definition from TechTarget\". Enterprise AI. Retrieved 8 October 2023. - ^ Roser, Max (15 December 2022). \"Artificial intelligence is transforming our world – it is on all of us to make sure that it goes well\". Our World in Data. Retrieved 8 October 2023. - ^ \"Google DeepMind's Six Levels of AGI\". aibusiness.com. Retrieved 20 July 2025. - ^ Dickson, Ben (16 November 2023). \"Here is how far we are to achieving AGI, according to DeepMind\". VentureBeat. - ^ McCarthy, John (2007a). \"Basic Questions\". Stanford University. Archived from the original on 26 October 2007. Retrieved 6 December 2007. - ^ This list of intelligent traits is based on the topics covered by major AI textbooks, including: Russell & Norvig 2003, Luger & Stubblefield 2004, Poole, Mackworth & Goebel 1998 and Nilsson 1998. - ^ Johnson 1987 - ^ de Charms, R. (1968). Personal causation. New York: Academic Press. - ^ Van Eyghen, Hans (2025). \"AI Algorithms as (Un)virtuous Knowers\". Discover Artificial Intelligence. 5 (2) 2. doi:10.1007/s44163-024-00219-z. - ^ a b Pfeifer, R. and Bongard J. C., How the body shapes the way we think: a new view of intelligence (The MIT Press, 2007). ISBN 0-2621-6239-3 - ^ Muehlhauser, Luke (11 August 2013). \"What is AGI?\". Machine Intelligence Research Institute. Archived from the original on 25 April 2014. Retrieved 1 May 2014. - ^ \"What is Artificial General Intelligence (AGI)? | 4 Tests For Ensuring Artificial General Intelligence\". Talky Blog. 13 July 2019. Archived from the original on 17 July 2019. Retrieved 17 July 2019. - ^ Batson, Joshua. \"Forget the Turing Test: Here's How We Could Actually Measure AI\". Wired. ISSN 1059-1028. Retrieved 22 March 2025. - ^ a b Turing 1950. - ^ Turing, Alan (2004). B. Jack Copeland (ed.). Can Automatic Calculating Machines Be Said To Think? (1957). Oxford: Oxford University Press. pp. 487–506. ISBN 978-0-1982-5079-1. - ^ \"Eugene Goostman is a real boy – the Turing Test says so\". The Guardian. 9 June 2014. ISSN 0261-3077. Retrieved 3 March 2024. - ^ \"Scientists dispute whether computer 'Eugene Goostman' passed Turing test\". BBC News. 9 June 2014. Retrieved 3 March 2024. - ^ Kirk-Giannini, Cameron Domenico; Goldstein, Simon (16 October 2023). \"AI is closer than ever to passing the Turing test for 'intelligence'. What happens when it does?\". The Conversation. Retrieved 22 September 2024. - ^ Jones, Cameron R.; Bergen, Benjamin K. (9 May 2024). \"People cannot distinguish GPT-4 from a human in a Turing test\". arXiv:2405.08007 [cs.HC]. - ^ Jones, Cameron R.; Bergen, Benjamin K. (31 March 2025). \"Large Language Models Pass the Turing Test\". arXiv:2503.23674 [cs.CL]. - ^ \"AI model passes Turing Test better than a human\". The Independent. 9 April 2025. Retrieved 18 April 2025. - ^ Varanasi, Lakshmi (21 March 2023). \"AI models like ChatGPT and GPT-4 are acing everything from the bar exam to AP Biology. Here's a list of difficult exams both AI versions have passed\". Business Insider. Retrieved 30 May 2023. - ^ Naysmith, Caleb (7 February 2023). \"6 Jobs Artificial Intelligence Is Already Replacing and How Investors Can Capitalize on It\". Retrieved 30 May 2023. - ^ Turk, Victoria (28 January 2015). \"The Plan to Replace the Turing Test with a 'Turing Olympics'\". Vice. Retrieved 3 March 2024. - ^ Gopani, Avi (25 May 2022). \"Turing Test is unreliable. The Winograd Schema is obsolete. Coffee is the answer\". Analytics India Magazine. Retrieved 3 March 2024. - ^ Bhaimiya, Sawdah (20 June 2023). \"DeepMind's co-founder suggested testing an AI chatbot's ability to turn $100,000 into $1 million to measure human-like intelligence\". Business Insider. Retrieved 3 March 2024. - ^ Suleyman, Mustafa (14 July 2023). \"Mustafa Suleyman: My new Turing test would see if AI can make $1 million\". MIT Technology Review. Retrieved 3 March 2024. - ^ Goertzel, Ben; Bach, Joscha; et al. (10 co-authors) (1 March 2012). \"Mapping the Landscape of Human-Level Artificial General Intelligence\". AI Magazine. 33 (1): 33. doi:10.1609/aimag.v33i1.2322. ISSN 0738-4602. Retrieved 19 October 2025. - ^ Mikhaylovskiy, Nikolay. \"How do you test the strength of AI?\" (PDF). p. 4. Retrieved 19 October 2025. The Goertzel Tests ... Learning to play an arbitrary video game based on experience only, or based on experience plus reading instructions - ^ Aaronson, Scott (12 February 2024). \"The Problem of Human Specialness in the Age of AI\". Shtetl-Optimized. Retrieved 19 October 2025. Given any game or contest with suitably objective rules, which wasn't specifically constructed to differentiate humans from machines, and on which an AI can be given suitably many examples of play, it's only a matter of years before not merely any AI, but AI on the current paradigm (!), matches or beats the best human performance. - ^ Shapiro, Stuart C. (1992). \"Artificial Intelligence\" (PDF). In Stuart C. Shapiro (ed.). Encyclopedia of Artificial Intelligence (Second ed.). New York: John Wiley. pp. 54–57. Archived (PDF) from the original on 1 February 2016. (Section 4 is on \"AI-Complete Tasks\".) - ^ Yampolskiy, Roman V. (2012). Xin-She Yang (ed.). \"Turing Test as a Defining Feature of AI-Completeness\" (PDF). Artificial Intelligence, Evolutionary Computation and Metaheuristics: 3–17. Archived (PDF) from the original on 22 May 2013. - ^ \"AI Index: State of AI in 13 Charts\". Stanford University Human-Centered Artificial Intelligence. 15 April 2024. Retrieved 27 May 2024. - ^ Crevier 1993, pp. 48–50 - ^ Kaplan, Andreas (2022). \"Artificial Intelligence, Business and Civilization – Our Fate Made in Machines\". Archived from the original on 6 May 2022. Retrieved 12 March 2022. - ^ Simon 1965, p. 96 quoted in Crevier 1993, p. 109 - ^ \"Scientist on the Set: An Interview with Marvin Minsky\". Archived from the original on 16 July 2012. Retrieved 5 April 2008. - ^ Marvin Minsky to Darrach (1970), quoted in Crevier (1993, p. 109). - ^ Lighthill 1973; Howe 1994 - ^ a b National Research Council 1999, \"Shift to Applied Research Increases Investment\". - ^ Crevier 1993, pp. 115–117; Russell & Norvig 2003, pp. 21–22. - ^ Crevier 1993, p. 211, Russell & Norvig 2003, p. 24 and see also Feigenbaum & McCorduck 1983 - ^ Crevier 1993, pp. 161–162, 197–203, 240; Russell & Norvig 2003, p. 25. - ^ Crevier 1993, pp. 209–212 - ^ McCarthy, John (2000). \"Reply to Lighthill\". Stanford University. Archived from the original on 30 September 2008. Retrieved 29 September 2007. - ^ Markoff, John (14 October 2005). \"Behind Artificial Intelligence, a Squadron of Bright Real People\". The New York Times. Archived from the original on 2 February 2023. Retrieved 18 February 2017. At its low point, some computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers. - ^ Russell & Norvig 2003, pp. 25–26 - ^ \"Trends in the Emerging Tech Hype Cycle\". Gartner Reports. Archived from the original on 22 May 2019. Retrieved 7 May 2019. - ^ a b Moravec 1988, p. 20 - ^ Harnad, S. (1990). \"The Symbol Grounding Problem\". Physica D. 42 (1–3): 335–346. arXiv:cs/9906002. Bibcode:1990PhyD...42..335H. doi:10.1016/0167-2789(90)90087-6. S2CID 3204300. - ^ Gubrud 1997 - ^ Hutter, Marcus (2005). Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability. Texts in Theoretical Computer Science an EATCS Series. Springer. doi:10.1007/b138233. ISBN 978-3-5402-6877-2. S2CID 33352850. Archived from the original on 19 July 2022. Retrieved 19 July 2022. - ^ Legg, Shane (2008). Machine Super Intelligence (PDF) (Thesis). University of Lugano. Archived (PDF) from the original on 15 June 2022. Retrieved 19 July 2022. - ^ Goertzel, Ben (2014). Artificial General Intelligence. Lecture Notes in Computer Science. Vol. 8598. Journal of Artificial General Intelligence. doi:10.1007/978-3-319-09274-4. ISBN 978-3-3190-9273-7. S2CID 8387410. - ^ \"Who coined the term \"AGI\"?\". goertzel.org. Archived from the original on 28 December 2018. Retrieved 28 December 2018., via Life 3.0: 'The term \"AGI\" was popularized by... Shane Legg, Mark Gubrud and Ben Goertzel' - ^ Wang & Goertzel 2007 - ^ \"First International Summer School in Artificial General Intelligence, Main summer school: June 22 – July 3, 2009, OpenCog Lab: July 6-9, 2009\". Archived from the original on 28 September 2020. Retrieved 11 May 2020. - ^ \"Избираеми дисциплини 2009/2010 – пролетен триместър\" [Elective courses 2009/2010 – spring trimester]. Факултет по математика и информатика [Faculty of Mathematics and Informatics] (in Bulgarian). Archived from the original on 26 July 2020. Retrieved 11 May 2020. - ^ \"Избираеми дисциплини 2010/2011 – зимен триместър\" [Elective courses 2010/2011 – winter trimester]. Факултет по математика и информатика [Faculty of Mathematics and Informatics] (in Bulgarian). Archived from the original on 26 July 2020. Retrieved 11 May 2020. - ^ \"Microsoft Researchers Claim GPT-4 Is Showing \"Sparks\" of AGI\". Futurism. 23 March 2023. Retrieved 13 December 2023. - ^ Allen, Paul; Greaves, Mark (12 October 2011). \"The Singularity Isn't Near\". MIT Technology Review. Retrieved 17 September 2014. - ^ Winfield, Alan. \"Artificial intelligence will not turn into a Frankenstein's monster\". The Guardian. Archived from the original on 17 September 2014. Retrieved 17 September 2014. - ^ Deane, George (2022). \"Machines That Feel and Think: The Role of Affective Feelings and Mental Action in (Artificial) General Intelligence\". Artificial Life. 28 (3): 289–309. doi:10.1162/artl_a_00368. ISSN 1064-5462. PMID 35881678. S2CID 251069071. - ^ a b c Clocksin 2003. - ^ Fjelland, Ragnar (17 June 2020). \"Why general artificial intelligence will not be realized\". Humanities and Social Sciences Communications. 7 (1) 10: 1–9. doi:10.1057/s41599-020-0494-4. hdl:11250/2726984. ISSN 2662-9992. S2CID 219710554. - ^ McCarthy 2007b. - ^ Khatchadourian, Raffi (23 November 2015). \"The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?\". The New Yorker. Archived from the original on 28 January 2016. Retrieved 7 February 2016. - ^ Müller, V. C., & Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. In Fundamental issues of artificial intelligence (pp. 555–572). Springer, Cham. - ^ Armstrong, Stuart, and Kaj Sotala. 2012. \"How We're Predicting AI—or Failing To.\" In Beyond AI: Artificial Dreams, edited by Jan Romportl, Pavel Ircing, Eva Žáčková, Michal Polák and Radek Schuster, pp. 52–75. Plzeň: University of West Bohemia. - ^ \"Microsoft Now Claims GPT-4 Shows 'Sparks' of General Intelligence\". 24 March 2023. - ^ Shimek, Cary (6 July 2023). \"AI Outperforms Humans in Creativity Test\". Neuroscience News. Retrieved 20 October 2023. - ^ Guzik, Erik E.; Byrge, Christian; Gilde, Christian (1 December 2023). \"The originality of machines: AI takes the Torrance Test\". Journal of Creativity. 33 (3) 100065. doi:10.1016/j.yjoc.2023.100065. ISSN 2713-3745. S2CID 261087185. - ^ Arcas, Blaise Agüera y (10 October 2023). \"Artificial General Intelligence Is Already Here\". Noema. - ^ \"AI Index: State of AI in 13 Charts\". hai.stanford.edu. 15 April 2024. Retrieved 7 June 2024. - ^ \"Next-Gen AI: OpenAI and Meta's Leap Towards Reasoning Machines\". Unite.ai. 19 April 2024. Retrieved 7 June 2024. - ^ James, Alex P. (2022). \"The Why, What, and How of Artificial General Intelligence Chip Development\". IEEE Transactions on Cognitive and Developmental Systems. 14 (2): 333–347. arXiv:2012.06338. Bibcode:2022ITCDS..14..333J. doi:10.1109/TCDS.2021.3069871. ISSN 2379-8920. S2CID 228376556. - ^ Pei, Jing; Deng, Lei; Song, Sen; Zhao, Mingguo; Zhang, Youhui; Wu, Shuang; Wang, Guanrui; Zou, Zhe; Wu, Zhenzhi; He, Wei; Chen, Feng; Deng, Ning; Wu, Si; Wang, Yu; Wu, Yujie (2019). \"Towards artificial general intelligence with hybrid Tianjic chip architecture\". Nature. 572 (7767): 106–111. Bibcode:2019Natur.572..106P. doi:10.1038/s41586-019-1424-8. ISSN 1476-4687. PMID 31367028. S2CID 199056116. Archived from the original on 29 August 2022. Retrieved 29 August 2022. - ^ Pandey, Mohit; Fernandez, Michael; Gentile, Francesco; Isayev, Olexandr; Tropsha, Alexander; Stern, Abraham C.; Cherkasov, Artem (March 2022). \"The transformational role of GPU computing and deep learning in drug discovery\". Nature Machine Intelligence. 4 (3): 211–221. doi:10.1038/s42256-022-00463-x. ISSN 2522-5839. S2CID 252081559. - ^ Goertzel & Pennachin 2006. - ^ a b c (Kurzweil 2005, p. 260) - ^ a b c Goertzel 2007. - ^ Grace, Katja (2016). \"Error in Armstrong and Sotala 2012\". AI Impacts (blog). Archived from the original on 4 December 2020. Retrieved 24 August 2020. - ^ a b Butz, Martin V. (1 March 2021). \"Towards Strong AI\". KI – Künstliche Intelligenz. 35 (1): 91–101. doi:10.1007/s13218-021-00705-x. ISSN 1610-1987. S2CID 256065190. - ^ Liu, Feng; Shi, Yong; Liu, Ying (2017). \"Intelligence Quotient and Intelligence Grade of Artificial Intelligence\". Annals of Data Science. 4 (2): 179–191. arXiv:1709.10242. doi:10.1007/s40745-017-0109-0. S2CID 37900130. - ^ Brien, Jörn (5 October 2017). \"Google-KI doppelt so schlau wie Siri\" [Google AI is twice as smart as Siri – but a six-year-old beats both] (in German). Archived from the original on 3 January 2019. Retrieved 2 January 2019. - ^ Grossman, Gary (3 September 2020). \"We're entering the AI twilight zone between narrow and general AI\". VentureBeat. Archived from the original on 4 September 2020. Retrieved 5 September 2020. Certainly, too, there are those who claim we are already seeing an early example of an AGI system in the recently announced GPT-3 natural language processing (NLP) neural network. ... So is GPT-3 the first example of an AGI system? This is debatable, but the consensus is that it is not AGI. ... If nothing else, GPT-3 tells us there is a middle ground between narrow and general AI. - ^ Quach, Katyanna. \"A developer built an AI chatbot using GPT-3 that helped a man speak again to his late fiancée. OpenAI shut it down\". The Register. Archived from the original on 16 October 2021. Retrieved 16 October 2021. - ^ Wiggers, Kyle (13 May 2022), \"DeepMind's new AI can perform over 600 tasks, from playing games to controlling robots\", TechCrunch, archived from the original on 16 June 2022, retrieved 12 June 2022 - ^ Metz, Cade (1 May 2023). \"'The Godfather of A.I.' Leaves Google and Warns of Danger Ahead\". The New York Times. ISSN 0362-4331. Retrieved 7 June 2023. - ^ \"'Godfather of AI' shortens odds of the technology wiping out humanity over next 30 years\". The Guardian. 27 December 2024. Retrieved 18 April 2025. - ^ Bove, Tristan. \"A.I. could rival human intelligence in 'just a few years,' says CEO of Google's main A.I. research lab\". Fortune. Retrieved 4 September 2024. - ^ Nellis, Stephen (2 March 2024). \"Nvidia CEO says AI could pass human tests in five years\". Reuters. - ^ Aschenbrenner, Leopold. \"SITUATIONAL AWARENESS, The Decade Ahead\". - ^ a b Orf, Darren (October 2025). \"Humanity May Achieve the Singularity Within the Next 3 Months, Scientists Suggest\". www.msn.com. Popular Mechanics. Retrieved 2 October 2025. - ^ Sullivan, Mark (18 October 2023). \"Why everyone seems to disagree on how to define Artificial General Intelligence\". Fast Company. - ^ Nosta, John (5 January 2024). \"The Accelerating Path to Artificial General Intelligence\". Psychology Today. Retrieved 30 March 2024. - ^ Hickey, Alex. \"Whole Brain Emulation: A Giant Step for Neuroscience\". Tech Brew. Retrieved 8 November 2023. - ^ Sandberg & Boström 2008. - ^ Drachman 2005. - ^ a b Russell & Norvig 2003. - ^ Moravec 1988, p. 61. - ^ Moravec 1998. - ^ Holmgaard Mersh, Amalie (15 September 2023). \"Decade-long European research project maps the human brain\". euractiv. - ^ Swaminathan, Nikhil (January–February 2011). \"Glia—the other brain cells\". Discover. Archived from the original on 8 February 2014. Retrieved 24 January 2014. - ^ de Vega, Glenberg & Graesser 2008. A wide range of views in current research, all of which require grounding to some degree - ^ Thornton, Angela (26 June 2023). \"How uploading our minds to a computer might become possible\". The Conversation. Retrieved 8 November 2023. - ^ Searle 1980 - ^ For example: - Russell & Norvig 2003, - Oxford University Press Dictionary of Psychology. Archived 3 December 2007 at the Wayback Machine (quoted in \" Encyclopedia.com\"), - MIT Encyclopedia of Cognitive Science. Archived 19 July 2008 at the Wayback Machine (quoted in \"AITopics\"), - Will Biological Computers Enable Artificially Intelligent Machines to Become Persons?. Archived 13 May 2008 at the Wayback Machine, Anthony Tongen. - ^ a b c Russell & Norvig 2003, p. 947. - ^ Though see Explainable artificial intelligence for curiosity by the field about why a program behaves the way it does. - ^ Chalmers, David J. (9 August 2023). \"Could a Large Language Model Be Conscious?\". Boston Review. - ^ Seth, Anil. \"Consciousness\". New Scientist. Retrieved 5 September 2024. - ^ Nagel 1974. - ^ \"The Google engineer who thinks the company's AI has come to life\". The Washington Post. 11 June 2022. Retrieved 12 June 2023. - ^ Kateman, Brian (24 July 2023). \"AI Should Be Terrified of Humans\". Time. Archived from the original on 27 July 2023. Retrieved 5 September 2024. - ^ Nosta, John (18 December 2023). \"Should Artificial Intelligence Have Rights?\". Psychology Today. Retrieved 5 September 2024. - ^ Akst, Daniel (10 April 2023). \"Should Robots With Artificial Intelligence Have Moral or Legal Rights?\". The Wall Street Journal. - ^ \"How we can Benefit from Advancing Artificial General Intelligence (AGI) – Unite.AI\". www.unite.ai. 7 April 2020. Retrieved 7 June 2023. - ^ a b c Talty, Jules; Julien, Stephan. \"What Will Our Society Look Like When Artificial Intelligence Is Everywhere?\". Smithsonian Magazine. Retrieved 7 June 2023. - ^ a b Stevenson, Matt (8 October 2015). \"Answers to Stephen Hawking's AMA are Here!\". Wired. ISSN 1059-1028. Retrieved 8 June 2023. - ^ a b Bostrom, Nick (2017). \"§ Preferred order of arrival\". Superintelligence: paths, dangers, strategies (Reprinted with corrections 2017 ed.). Oxford, United Kingdom; New York, New York, USA: Oxford University Press. ISBN 978-0-1996-7811-2. - ^ Piper, Kelsey (19 November 2018). \"How technological progress is making it likelier than ever that humans will destroy ourselves\". Vox. Retrieved 8 June 2023. - ^ Yampolskiy, Roman; Duettmann, Allison (2020). Artificial Superintelligence: Coordination & Strategy. MDPI – Multidisciplinary Digital Publishing Institute. ISBN 978-3-03921-855-4. - ^ Topol, Eric J.; Verghese, Abraham (2019). Deep medicine: how artificial intelligence can make healthcare human again (First ed.). New York, NY: Basic Books. ISBN 978-1-5416-4463-2. - ^ Jumper, John; Evans, Richard; Pritzel, Alexander; Green, Tim; Figurnov, Michael; Ronneberger, Olaf; Tunyasuvunakool, Kathryn; Bates, Russ; Žídek, Augustin; Potapenko, Anna; Bridgland, Alex; Meyer, Clemens; Kohl, Simon A. A.; Ballard, Andrew J.; Cowie, Andrew (August 2021). \"Highly accurate protein structure prediction with AlphaFold\". Nature. 596 (7873): 583–589. Bibcode:2021Natur.596..583J. doi:10.1038/s41586-021-03819-2. ISSN 1476-4687. PMC 8371605. PMID 34265844. - ^ Alowais, Shuroug A.; Alghamdi, Sahar S.; Alsuhebany, Nada; Alqahtani, Tariq; Alshaya, Abdulrahman I.; Almohareb, Sumaya N.; Aldairem, Atheer; Alrashed, Mohammed; Bin Saleh, Khalid; Badreldin, Hisham A.; Al Yami, Majed S.; Al Harbi, Shmeylan; Albekairy, Abdulkareem M. (22 September 2023). \"Revolutionizing healthcare: the role of artificial intelligence in clinical practice\". BMC Medical Education. 23 (1): 689. doi:10.1186/s12909-023-04698-z. ISSN 1472-6920. PMC 10517477. PMID 37740191. - ^ a b Tegmark, Max (2017). Life 3.0: being human in the age of artificial intelligence. A Borzoi book. New York: Alfred A. Knopf. ISBN 978-1-101-94659-6. - ^ a b Brynjolfsson, Erik; McAfee, Andrew (2016). The second machine age: work, progress, and prosperity in a time of brilliant technologies (First published as a Norton paperback ed.). New York London: W. W. Norton & Company. ISBN 978-0-393-35064-7. - ^ Zhai, Xuesong; Chu, Xiaoyan; Chai, Ching Sing; Jong, Morris Siu Yung; Istenic, Andreja; Spector, Michael; Liu, Jia-Bao; Yuan, Jing; Li, Yan (2021). \"A Review of Artificial Intelligence (AI) in Education from 2010 to 2020\". Complexity. 2021 (1) 8812542. doi:10.1155/2021/8812542. ISSN 1099-0526. - ^ Bostrom, Nick (2017). Superintelligence: paths, dangers, strategies (Reprinted with corrections ed.). Oxford: Oxford University Press. ISBN 978-0-19-873983-8. - ^ Crawford, Kate (2021). Atlas of AI: power, politics, and the planetary costs of artificial intelligence. New Haven: Yale University Press. ISBN 978-0-300-20957-0. - ^ \"Artificial Intelligence and Conservation | Pages | WWF\". World Wildlife Fund. Retrieved 28 April 2025. - ^ Rolnick, David; Donti, Priya L.; Kaack, Lynn H.; Kochanski, Kelly; Lacoste, Alexandre; Sankaran, Kris; Andrew Slavin Ross; Milojevic-Dupont, Nikola; Jaques, Natasha; Waldman-Brown, Anna; Luccioni, Alexandra; Maharaj, Tegan; Sherwin, Evan D.; Karthik Mukkavilli, S.; Kording, Konrad P.; Gomes, Carla; Ng, Andrew Y.; Hassabis, Demis; Platt, John C.; Creutzig, Felix; Chayes, Jennifer; Bengio, Yoshua (2019). \"Tackling Climate Change with Machine Learning\". arXiv:1906.05433 [cs.CY]. - ^ Tegmark, Max. (2017). Life 3.0: Being Human in the Age of Artificial Intelligence. Penguin Books. - ^ Doherty, Ben (17 May 2018). \"Climate change an 'existential security risk' to Australia, Senate inquiry says\". The Guardian. ISSN 0261-3077. Retrieved 16 July 2023. - ^ MacAskill, William (2022). What we owe the future. New York, NY: Basic Books. ISBN 978-1-5416-1862-6. - ^ a b Ord, Toby (2020). \"Chapter 5: Future Risks, Unaligned Artificial Intelligence\". The Precipice: Existential Risk and the Future of Humanity. Bloomsbury Publishing. ISBN 978-1-5266-0021-9. - ^ Al-Sibai, Noor (13 February 2022). \"OpenAI Chief Scientist Says Advanced AI May Already Be Conscious\". Futurism. Retrieved 24 December 2023. - ^ Samuelsson, Paul Conrad (2019). \"Artificial Consciousness: Our Greatest Ethical Challenge\". Philosophy Now. Retrieved 23 December 2023. - ^ Kateman, Brian (24 July 2023). \"AI Should Be Terrified of Humans\". Time. Archived from the original on 27 July 2023. Retrieved 23 December 2023. - ^ Roose, Kevin (30 May 2023). \"A.I. Poses 'Risk of Extinction,' Industry Leaders Warn\". The New York Times. ISSN 0362-4331. Retrieved 24 December 2023. - ^ a b \"Statement on AI Risk\". Center for AI Safety. 30 May 2023. Retrieved 8 June 2023. - ^ \"Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence;– but are we taking AI seriously enough?'\". The Independent (UK). Archived from the original on 25 September 2015. Retrieved 3 December 2014. - ^ Herger, Mario. \"The Gorilla Problem – Enterprise Garage\". Retrieved 7 June 2023. - ^ \"The fascinating Facebook debate between Yann LeCun, Stuart Russel and Yoshua Bengio about the risks of strong AI\". The fascinating Facebook debate between Yann LeCun, Stuart Russel and Yoshua Bengio about the risks of strong AI (in French). Retrieved 8 June 2023. - ^ \"Will Artificial Intelligence Doom The Human Race Within The Next 100 Years?\". HuffPost. 22 August 2014. Retrieved 8 June 2023. - ^ Sotala, Kaj; Yampolskiy, Roman V. (19 December 2014). \"Responses to catastrophic AGI risk: a survey\". Physica Scripta. 90 (1) 018001. doi:10.1088/0031-8949/90/1/018001. ISSN 0031-8949. - ^ Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies (First ed.). Oxford University Press. ISBN 978-0-1996-7811-2. - ^ Chow, Andrew R.; Perrigo, Billy (16 February 2023). \"The AI Arms Race Is On. Start Worrying\". Time. Archived from the original on 19 February 2023. Retrieved 24 December 2023. - ^ Tetlow, Gemma (12 January 2017). \"AI arms race risks spiralling out of control, report warns\". Financial Times. Archived from the original on 11 April 2022. Retrieved 24 December 2023. - ^ Milmo, Dan; Stacey, Kiran (25 September 2023). \"Experts disagree over threat posed but artificial intelligence cannot be ignored\". The Guardian. ISSN 0261-3077. Retrieved 24 December 2023. - ^ \"Humanity, Security & AI, Oh My! (with Ian Bremmer & Shuman Ghosemajumder)\". CAFE. 20 July 2023. Retrieved 15 September 2023. - ^ Hamblin, James (9 May 2014). \"But What Would the End of Humanity Mean for Me?\". The Atlantic. Archived from the original on 4 June 2014. Retrieved 12 December 2015. - ^ Titcomb, James (30 October 2023). \"Big Tech is stoking fears over AI, warn scientists\". The Telegraph. Retrieved 7 December 2023. - ^ Davidson, John (30 October 2023). \"Google Brain founder says big tech is lying about AI extinction danger\". Australian Financial Review. Archived from the original on 7 December 2023. Retrieved 7 December 2023. - ^ Eloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (17 March 2023). \"GPTs are GPTs: An early look at the labor market impact potential of large language models\". OpenAI. Retrieved 7 June 2023. - ^ a b Hurst, Luke (23 March 2023). \"OpenAI says 80% of workers could see their jobs impacted by AI. These are the jobs most affected\". euronews. Retrieved 8 June 2023. - ^ Drago, Luke. \"What Happens When AI Replaces Workers?\". TIME. Archived from the original on 31 July 2025. Retrieved 2 November 2025. - ^ Autor, David H. (Summer 2015). \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\". Journal of Economic Perspectives. 29 (3): 3–30. doi:10.1257/jep.29.3.3. Retrieved 2 August 2025. - ^ Gmyrek, Paweł; Berg, Janine; Bescond, David (August 2023). Generative AI and Jobs: A Global Analysis of Potential Effects on Job Quantity and Quality (PDF) (Report). ILO Working Paper. Geneva, Switzerland: International Labour Organization. Retrieved 2 August 2025. - ^ Thompson, Clive (13 October 2022). \"AI Shouldn't Compete With Workers—It Should Supercharge Them\". Wired. Retrieved 2 August 2025. - ^ Sheffey, Ayelet (20 August 2021). \"Elon Musk says we need universal basic income because 'in the future, physical work will be a choice'\". Business Insider. Archived from the original on 9 July 2023. Retrieved 8 June 2023. - ^ Varanasi, Lakshmi (27 February 2025). \"Will AI replace human jobs and make universal basic income necessary? Here's what AI leaders have said about UBI\". Business Insider Africa. Retrieved 2 August 2025. - ^ Hern, Alex (4 May 2023). \"Bernie Sanders, Elon Musk and White House seeking my help, says 'godfather of AI'\". The Guardian. Retrieved 2 August 2025. Sources [edit]- UNESCO Science Report: the Race Against Time for Smarter Development. Paris, France: UNESCO. 11 June 2021. ISBN 978-9-2310-0450-6. Archived from the original on 18 June 2022. Retrieved 22 September 2021. - Chalmers, David (1996), The Conscious Mind, Oxford University Press. - Clocksin, William (August 2003), \"Artificial intelligence and the future\", Philosophical Transactions of the Royal Society A, vol. 361, no. 1809, pp. 1721–1748, Bibcode:2003RSPTA.361.1721C, doi:10.1098/rsta.2003.1232, PMID 12952683, S2CID 31032007 - Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3. - Darrach, Brad (20 November 1970), \"Meet Shakey, the First Electronic Person\", Life Magazine, pp. 58–68 - Drachman, D. (2005), \"Do we have brain to spare?\", Neurology, 64 (12): 2004–2005, doi:10.1212/01.WNL.0000166914.38327.BB, PMID 15985565, S2CID 38482114 - Feigenbaum, Edward A.; McCorduck, Pamela (1983), The Fifth Generation: Artificial Intelligence and Japan's Computer Challenge to the World, Michael Joseph, ISBN 978-0-7181-2401-4 - Goertzel, Ben; Pennachin, Cassio, eds. (2006), Artificial General Intelligence (PDF), Springer, ISBN 978-3-5402-3733-4, archived from the original (PDF) on 20 March 2013 - Goertzel, Ben (December 2007), \"Human-level artificial general intelligence and the possibility of a technological singularity: a reaction to Ray Kurzweil's The Singularity Is Near, and McDermott's critique of Kurzweil\", Artificial Intelligence, vol. 171, no. 18, Special Review Issue, pp. 1161–1173, doi:10.1016/j.artint.2007.10.011, archived from the original on 7 January 2016, retrieved 1 April 2009 - Gubrud, Mark (November 1997), \"Nanotechnology and International Security\", Fifth Foresight Conference on Molecular Nanotechnology, archived from the original on 29 May 2011, retrieved 7 May 2011 - Howe, J. (November 1994), Artificial Intelligence at Edinburgh University: a Perspective, archived from the original on 17 August 2007, retrieved 30 August 2007 - Johnson, Mark (1987), The body in the mind, Chicago, ISBN 978-0-2264-0317-5 - Kurzweil, Ray (2005), The Singularity is Near, Viking Press - Lighthill, Professor Sir James (1973), \"Artificial Intelligence: A General Survey\", Artificial Intelligence: a paper symposium, Science Research Council - Luger, George; Stubblefield, William (2004), Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th ed.), The Benjamin/Cummings Publishing Company, Incorporated, p. 720, ISBN 978-0-8053-4780-7 - McCarthy, John (2007b). What is Artificial Intelligence?. California: Stanford University. The ultimate effort is to make computer programs that can solve problems and achieve goals in the world as well as humans. - Moravec, Hans (1988), Mind Children, Harvard University Press - Moravec, Hans (1998), \"When will computer hardware match the human brain?\", Journal of Evolution and Technology, vol. 1, archived from the original on 15 June 2006, retrieved 23 June 2006 - Nagel (1974), \"What Is it Like to Be a Bat\" (PDF), Philosophical Review, 83 (4): 435–450, doi:10.2307/2183914, JSTOR 2183914, archived (PDF) from the original on 16 October 2011, retrieved 7 November 2009 - Newell, Allen; Simon, Herbert A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022. - Nilsson, Nils (1998), Artificial Intelligence: A New Synthesis, Morgan Kaufmann Publishers, ISBN 978-1-5586-0467-4 - \"Developments in Artificial Intelligence\", Funding a Revolution: Government Support for Computing Research, National Academy Press, 1999, archived from the original on 12 January 2008, retrieved 29 September 2007 - Poole, David; Mackworth, Alan; Goebel, Randy (1998), Computational Intelligence: A Logical Approach, New York: Oxford University Press, archived from the original on 25 July 2009, retrieved 6 December 2007 - Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 - Sandberg, Anders; Boström, Nick (2008), Whole Brain Emulation: A Roadmap (PDF), Technical Report #2008-3, Future of Humanity Institute, Oxford University, archived (PDF) from the original on 25 March 2020, retrieved 5 April 2009 - Searle, John (1980), \"Minds, Brains and Programs\" (PDF), Behavioral and Brain Sciences, 3 (3): 417–457, doi:10.1017/S0140525X00005756, S2CID 55303721, archived (PDF) from the original on 17 March 2019, retrieved 3 September 2020 - Simon, Herbert A. (1965), The Shape of Automation for Men and Management, New York: Harper & Row - Turing, Alan (October 1950). \"Computing Machinery and Intelligence\". Mind. 59 (236): 433–460. doi:10.1093/mind/LIX.236.433. ISSN 1460-2113. JSTOR 2251299. S2CID 14636783. - de Vega, Manuel; Glenberg, Arthur; Graesser, Arthur, eds. (2008), Symbols and Embodiment: Debates on meaning and cognition, Oxford University Press, ISBN 978-0-1992-1727-4 - Wang, Pei; Goertzel, Ben (2007). \"Introduction: Aspects of Artificial General Intelligence\". Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms: Proceedings of the AGI Workshop 2006. IOS Press. pp. 1–16. ISBN 978-1-5860-3758-1. Archived from the original on 18 February 2021. Retrieved 13 December 2020 – via ResearchGate. Further reading [edit]- Aleksander, Igor (1996), Impossible Minds, World Scientific Publishing Company, ISBN 978-1-8609-4036-1 - Azevedo FA, Carvalho LR, Grinberg LT, Farfel J, et al. (April 2009), \"Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain\", The Journal of Comparative Neurology, 513 (5): 532–541, doi:10.1002/cne.21974, PMID 19226510, S2CID 5200449, archived from the original on 18 February 2021, retrieved 4 September 2013 – via ResearchGate - Berglas, Anthony (January 2012) [2008], Artificial Intelligence Will Kill Our Grandchildren (Singularity), archived from the original on 23 July 2014, retrieved 31 August 2012 - Cukier, Kenneth, \"Ready for Robots? How to Think about the Future of AI\", Foreign Affairs, vol. 98, no. 4 (July/August 2019), pp. 192–98. George Dyson, historian of computing, writes (in what might be called \"Dyson's Law\") that \"Any system simple enough to be understandable will not be complicated enough to behave intelligently, while any system complicated enough to behave intelligently will be too complicated to understand.\" (p. 197.) Computer scientist Alex Pentland writes: \"Current AI machine-learning algorithms are, at their core, dead simple stupid. They work, but they work by brute force.\" (p. 198.) - Gelernter, David, Dream-logic, the Internet and Artificial Thought, Edge, archived from the original on 26 July 2010, retrieved 25 July 2010 - Gleick, James, \"The Fate of Free Will\" (review of Kevin J. Mitchell, Free Agents: How Evolution Gave Us Free Will, Princeton University Press, 2023, 333 pp.), The New York Review of Books, vol. LXXI, no. 1 (18 January 2024), pp. 27–28, 30. \"Agency is what distinguishes us from machines. For biological creatures, reason and purpose come from acting in the world and experiencing the consequences. Artificial intelligences – disembodied, strangers to blood, sweat, and tears – have no occasion for that.\" (p. 30.) - Gleick, James, \"The Parrot in the Machine\" (review of Emily M. Bender and Alex Hanna, The AI Con: How to Fight Big Tech's Hype and Create the Future We Want, Harper, 274 pp.; and James Boyle, The Line: AI and the Future of Personhood, MIT Press, 326 pp.), The New York Review of Books, vol. LXXII, no. 12 (24 July 2025), pp. 43–46. \"[C]hatbox 'writing' has a bland, regurgitated quality. Textures are flattened, and sharp edges are sanded. No chatbox could ever have said that April is the cruelest month or that fog comes on little cat feet (though they might now, because one of their chief skills is plagiarism). And when synthetically extruded text turns out wrong, it can be comically wrong. When a movie fan asked Google whether a certain actor was in Heat, he received this 'AI Overview': 'No, Angelina Jolie is not in Heat.'\" (p. 44.) - Halal, William E. \"TechCast Article Series: The Automation of Thought\" (PDF). Archived from the original (PDF) on 6 June 2013. - Halpern, Sue, \"The Coming Tech Autocracy\" (review of Verity Harding, AI Needs You: How We Can Change AI's Future and Save Our Own, Princeton University Press, 274 pp.; Gary Marcus, Taming Silicon Valley: How We Can Ensure That AI Works for Us, MIT Press, 235 pp.; Daniela Rus and Gregory Mone, The Mind's Mirror: Risk and Reward in the Age of AI, Norton, 280 pp.; Madhumita Murgia, Code Dependent: Living in the Shadow of AI, Henry Holt, 311 pp.), The New York Review of Books, vol. LXXI, no. 17 (7 November 2024), pp. 44–46. \"'We can't realistically expect that those who hope to get rich from AI are going to have the interests of the rest of us close at heart,' ... writes [Gary Marcus]. 'We can't count on governments driven by campaign finance contributions [from tech companies] to push back.'... Marcus details the demands that citizens should make of their governments and the tech companies. They include transparency on how AI systems work; compensation for individuals if their data [are] used to train LLMs (large language model)s and the right to consent to this use; and the ability to hold tech companies liable for the harms they cause by eliminating Section 230, imposing cash penalties, and passing stricter product liability laws... Marcus also suggests... that a new, AI-specific federal agency, akin to the FDA, the FCC, or the FTC, might provide the most robust oversight.... [T]he Fordham law professor Chinmayi Sharma... suggests... establishing [ing] a professional licensing regime for engineers that would function similarly to medical licenses, malpractice suits, and the Hippocratic oath in medicine. 'What if, like doctors,' she asks..., 'AI engineers also vowed to not harm?'\" (p. 46.) - Holte, R. C.; Choueiry, B. Y. (2003), \"Abstraction and reformulation in artificial intelligence\", Philosophical Transactions of the Royal Society B, vol. 358, no. 1435, pp. 1197–1204, doi:10.1098/rstb.2003.1317, PMC 1693218, PMID 12903653 - Hughes-Castleberry, Kenna, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) - Immerwahr, Daniel, \"Your Lying Eyes: People now use A.I. to generate fake videos indistinguishable from real ones. How much does it matter?\", The New Yorker, 20 November 2023, pp. 54–59. \"If by 'deepfakes' we mean realistic videos produced using artificial intelligence that actually deceive people, then they barely exist. The fakes aren't deep, and the deeps aren't fake. [...] A.I.-generated videos are not, in general, operating in our media as counterfeited evidence. Their role better resembles that of cartoons, especially smutty ones.\" (p. 59.) - Leffer, Lauren, \"The Risks of Trusting AI: We must avoid humanizing machine-learning models used in scientific research\", Scientific American, vol. 330, no. 6 (June 2024), pp. 80–81. - Lepore, Jill, \"The Chit-Chatbot: Is talking with a machine a conversation?\", The New Yorker, 7 October 2024, pp. 12–16. - Marcus, Gary, \"Artificial Confidence: Even the newest, buzziest systems of artificial general intelligence are stymied by the same old problems\", Scientific American, vol. 327, no. 4 (October 2022), pp. 42–45. - McCarthy, John (October 2007), \"From here to human-level AI\", Artificial Intelligence, 171 (18): 1174–1182, doi:10.1016/j.artint.2007.10.009 - McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1 - Moravec, Hans (1976), The Role of Raw Power in Intelligence, archived from the original on 3 March 2016, retrieved 29 September 2007 - Newell, Allen; Simon, H. A. (1963), \"GPS: A Program that Simulates Human Thought\", in Feigenbaum, E. A.; Feldman, J. (eds.), Computers and Thought, New York: McGraw-Hill - Omohundro, Steve (2008), The Nature of Self-Improving Artificial Intelligence, presented and distributed at the 2007 Singularity Summit, San Francisco, California - Press, Eyal, \"In Front of Their Faces: Does facial-recognition technology lead police to ignore contradictory evidence?\", The New Yorker, 20 November 2023, pp. 20–26. - Roivainen, Eka, \"AI's IQ: ChatGPT aced a [standard intelligence] test but showed that intelligence cannot be measured by IQ alone\", Scientific American, vol. 329, no. 1 (July/August 2023), p. 7. \"Despite its high IQ, ChatGPT fails at tasks that require real humanlike reasoning or an understanding of the physical and social world.... ChatGPT seemed unable to reason logically and tried to rely on its vast database of... facts derived from online texts.\" - Scharre, Paul, \"Killer Apps: The Real Dangers of an AI Arms Race\", Foreign Affairs, vol. 98, no. 3 (May/June 2019), pp. 135–44. \"Today's AI technologies are powerful but unreliable. Rules-based systems cannot deal with circumstances their programmers did not anticipate. Learning systems are limited by the data on which they were trained. AI failures have already led to tragedy. Advanced autopilot features in cars, although they perform well in some circumstances, have driven cars without warning into trucks, concrete barriers, and parked cars. In the wrong situation, AI systems go from supersmart to superdumb in an instant. When an enemy is trying to manipulate and hack an AI system, the risks are even greater.\" (p. 140.) - Sutherland, J. G. (1990), \"Holographic Model of Memory, Learning, and Expression\", International Journal of Neural Systems, vol. 1–3, pp. 256–267 - Vincent, James, \"Horny Robot Baby Voice: James Vincent on AI chatbots\", London Review of Books, vol. 46, no. 19 (10 October 2024), pp. 29–32. \"[AI chatbot] programs are made possible by new technologies but rely on the timeless human tendency to anthropomorphise.\" (p. 29.) - Williams, R. W.; Herrup, K. (1988), \"The control of neuron number\", Annual Review of Neuroscience, 11: 423–453, doi:10.1146/annurev.ne.11.030188.002231, PMID 3284447 - Yudkowsky, Eliezer (2008), \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\", Global Catastrophic Risks, Bibcode:2008gcr..book..303Y, doi:10.1093/oso/9780198570509.003.0021, ISBN 978-0-1985-7050-9 - Zucker, Jean-Daniel (July 2003), \"A grounded theory of abstraction in artificial intelligence\", Philosophical Transactions of the Royal Society B, vol. 358, no. 1435, pp. 1293–1309, doi:10.1098/rstb.2003.1308, PMC 1693211, PMID 12903672",
    "text_length": 92381,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:43.874293"
  },
  {
    "id": "page_2",
    "url": "https://en.wikipedia.org/wiki/Intelligent_agent",
    "domain": "en.wikipedia.org",
    "title": "Intelligent agent - Wikipedia",
    "text": "Intelligent agent In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks[which?] define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence. A specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods. Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.[1] Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[2] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[3] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[4] Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[1] Intelligent agents as the foundation of AI [edit]This section possibly contains original research. (February 2023) | The concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes: - Agent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations. - Rational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing. - Artificial Intelligence (as a field): The study and creation of these rational agents. Other researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation. Defining AI in terms of intelligent agents offers several key advantages: - Avoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle's Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought. - Objective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"goal function\" (or objective function). This allows for direct comparison and combination of techniques. - Interdisciplinary Communication: It creates a common language for AI researchers to collaborate with other fields like mathematical optimization and economics, which also use concepts like \"goals\" and \"rational agents.\" Objective function [edit]An objective function (or goal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success. The objective function may be: - Simple: For example, in a game of Go, the objective function might assign a value of 1 for a win and 0 for a loss. - Complex: It might require the agent to evaluate and learn from past actions, adapting its behavior based on patterns that have proven effective. The objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort. Different terms are used to describe this concept, depending on the context. These include: - Utility function: Often used in economics and decision theory, representing the desirability of a state. - Objective function: A general term used in optimization. - Loss function: Typically used in machine learning, where the goal is to minimize the loss (error). - Reward Function: Used in reinforcement learning. - Fitness Function: Used in evolutionary systems. Goals, and therefore the objective function, can be: - Explicitly defined: Programmed directly into the agent. - Induced: Learned or evolved over time. - In reinforcement learning, a \"reward function\" provides feedback, encouraging desired behaviors and discouraging undesirable ones. The agent learns to maximize its cumulative reward. - In evolutionary systems, a \"fitness function\" determines which agents are more likely to reproduce. This is analogous to natural selection, where organisms evolve to maximize their chances of survival and reproduction.[5] Some AI systems, such as nearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data.[6] Such systems can still be benchmarked by framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.[7] Systems not traditionally considered agents, like knowledge-representation systems, are sometimes included in the paradigm by framing them as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior.[2] In generative adversarial networks (GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[8] While symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[9] Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress.[10] Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[11] AlphaZero chess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex.[12] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.[4] The mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm.[13] However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.[14] Agent function [edit]An intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen. A percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn). The agent function, often denoted as f, maps the agent's entire history of percepts to an action.[15] Mathematically, this can be represented as where: - represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts. - represents the set of all possible actions the agent can take. - is the agent function that maps a percept sequence to an action. It's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function). - The agent function is a theoretical description. - The agent program is the actual code that runs on the agent. The agent program takes the current percept as input and produces an action as output. The agent function can incorporate a wide range of decision-making approaches, including:[16] - Calculating the utility (desirability) of different actions. - Using logical rules and deduction. - Employing fuzzy logic. - Other methods. Classes of intelligent agents [edit]Russell and Norvig's classification [edit]Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[17] Simple reflex agents [edit]Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\". This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops. A home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.[18][19] Model-based reflex agents [edit]A model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\". A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent. An agent may also use models to describe and predict the behaviors of other agents in the environment.[20] Goal-based agents [edit]Goal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals. ChatGPT and the Roomba vacuum are examples of goal-based agents.[21] Utility-based agents [edit]Goal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is. A rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning. Learning agents [edit]Learning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions. The learning element gathers feedback from a \"critic\" to assess the agent's performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions. The final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement. Weiss's classification [edit]According to Weiss (2013), agents can be categorized into four classes: - Logic-based agents, where decisions about actions are derived through logical deduction. - Reactive agents, where decisions occur through a direct mapping from situation to action. - Belief–desire–intention agents, where decisions depend on manipulating data structures that represent the agent's beliefs, desires, and intentions. - Layered architectures, where decision-making takes place across multiple software layers, each of which reasons about the environment at a different level of abstraction. Other [edit]In 2013, Alexander Wissner-Gross published a theory exploring the relationship between Freedom and Intelligence in intelligent agents.[22][23] Hierarchies of agents [edit]Intelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals. Typically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.[24] Alternative definitions and uses [edit]\"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\".[25] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[26] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\". According to Nikola Kasabov in 1998, IA systems should exhibit the following characteristics:[27] - Accommodate new problem solving rules incrementally. - Adapt online and in real time. - Are able to analyze themselves in terms of behavior, error and success. - Learn and improve through interaction with the environment (embodiment). - Learn quickly from large amounts of data. - Have memory-based exemplar storage and retrieval capacities. - Have parameters to represent short- and long-term memory, age, forgetting, etc. Agentic AI [edit]In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.[28] They possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs).[29] Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.[30] Researchers and commentators have noted that AI agents do not have a standard definition.[29][31][32][33] The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..[34] A common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request.[35][36] Prominent examples include Devin AI, AutoGPT, and SIMA.[37] Further examples of agents released since 2025 include OpenAI Operator,[38] ChatGPT Deep Research,[39] Manus,[40] Quark (based on Qwen),[41] AutoGLM Rumination,[41] and Coze (by ByteDance).[41] Frameworks for building AI agents include LangChain,[42] as well as tools such as CAMEL,[43][44] Microsoft AutoGen,[45] and OpenAI Swarm.[46] Applications [edit]This section may lend undue weight to certain ideas, incidents, or controversies. (September 2023) | The concept of agent-based modeling for self-driving cars was discussed as early as 2003.[47] Hallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.[48] Waymo developed a multi-agent simulation environment called Carcraft, to test algorithms for self-driving cars.[49][50] This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data. Salesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.[51][52] The Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.[53] See also [edit]- Ambient intelligence - Artificial conversational entity - Artificial intelligence systems integration - Autonomous agent - Cognitive architectures - Cognitive radio – a practical field for implementation - Cybernetics - DAYDREAMER - Embodied agent - Federated search – the ability for agents to search heterogeneous data sources using a single vocabulary - Friendly artificial intelligence - Fuzzy agents – IA implemented with adaptive fuzzy logic - GOAL agent programming language - Hybrid intelligent system - Intelligent control - Intelligent system - JACK Intelligent Agents - Multi-agent system and multiple-agent system – multiple interactive agents - Reinforcement learning - Semantic Web – making data on the Web available for automated processing by agents - Social simulation - Software agent - Software bot References [edit]- ^ a b Russell & Norvig 2003, chpt. 2. - ^ a b Bringsjord, Selmer; Govindarajulu, Naveen Sundar (12 July 2018). \"Artificial Intelligence\". In Edward N. Zalta (ed.). The Stanford Encyclopedia of Philosophy (Summer 2020 Edition). - ^ Wolchover, Natalie (30 January 2020). \"Artificial Intelligence Will Do What We Ask. That's a Problem\". Quanta Magazine. Retrieved 21 June 2020. - ^ a b Bull, Larry (1999). \"On model-based evolutionary computation\". Soft Computing. 3 (2): 76–82. doi:10.1007/s005000050055. S2CID 9699920. - ^ Domingos 2015, Chapter 5. - ^ Domingos 2015, Chapter 7. - ^ Lindenbaum, M., Markovitch, S., & Rusakov, D. (2004). Selective sampling for nearest neighbor classifiers. Machine learning, 54(2), 125–152. - ^ \"Generative adversarial networks: What GANs are and how they've evolved\". VentureBeat. 26 December 2019. Retrieved 18 June 2020. - ^ Wolchover, Natalie (January 2020). \"Artificial Intelligence Will Do What We Ask. That's a Problem\". Quanta Magazine. Retrieved 18 June 2020. - ^ Andrew Y. Ng, Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" In ICML, vol. 99, pp. 278-287. 1999. - ^ Martin Ford. Architects of Intelligence: The truth about AI from the people building it. Packt Publishing Ltd, 2018. - ^ \"Why AlphaZero's Artificial Intelligence Has Trouble With the Real World\". Quanta Magazine. 2018. Retrieved 18 June 2020. - ^ Adams, Sam; Arel, Itmar; Bach, Joscha; Coop, Robert; Furlan, Rod; Goertzel, Ben; Hall, J. Storrs; Samsonovich, Alexei; Scheutz, Matthias; Schlesinger, Matthew; Shapiro, Stuart C.; Sowa, John (15 March 2012). \"Mapping the Landscape of Human-Level Artificial General Intelligence\". AI Magazine. 33 (1): 25. doi:10.1609/aimag.v33i1.2322. - ^ Hutson, Matthew (27 May 2020). \"Eye-catching advances in some AI fields are not real\". Science | AAAS. Retrieved 18 June 2020. - ^ Russell & Norvig 2003, p. 33 - ^ Salamon, Tomas (2011). Design of Agent-Based Models. Repin: Bruckner Publishing. pp. 42–59. ISBN 978-80-904661-1-1. - ^ Russell & Norvig 2003, pp. 46–54 - ^ Thakur, Shreeya. \"AI Agents: 5 Key Types Explained With Examples // Unstop\". unstop.com. Retrieved 2025-04-24. - ^ \"Types of AI Agents | IBM\". www.ibm.com. 2025-03-17. Retrieved 2025-04-24. - ^ Stefano Albrecht and Peter Stone (2018). Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems. Artificial Intelligence, Vol. 258, pp. 66-95. https://doi.org/10.1016/j.artint.2018.01.002 - ^ \"What is an AI agent? A computer scientist explains the next wave of artificial intelligence tools\". Inverse. 2024-12-24. Retrieved 2025-04-24. - ^ Box, Geeks out of the (2019-12-04). \"A Universal Formula for Intelligence\". Geeks out of the box. Retrieved 2022-10-11. - ^ Wissner-Gross, A. D.; Freer, C. E. (2013-04-19). \"Causal Entropic Forces\". Physical Review Letters. 110 (16) 168702. Bibcode:2013PhRvL.110p8702W. doi:10.1103/PhysRevLett.110.168702. hdl:1721.1/79750. PMID 23679649. - ^ Poole, David; Mackworth, Alan. \"1.3 Agents Situated in Environments‣ Chapter 2 Agent Architectures and Hierarchical Control‣ Artificial Intelligence: Foundations of Computational Agents, 2nd Edition\". artint.info. Retrieved 28 November 2018. - ^ Fingar, Peter (2018). \"Competing For The Future With Intelligent Agents... And A Confession\". Forbes Sites. Retrieved 18 June 2020. - ^ Burgin, Mark; Dodig-Crnkovic, Gordana (2009). \"A Systematic Approach to Artificial Agents\". arXiv:0902.3513 [cs.AI]. - ^ Kasabov 1998. - ^ Purdy, Mark (2024-12-12). \"What Is Agentic AI, and How Will It Change Work?\". Harvard Business Review. ISSN 0017-8012. Retrieved 2025-04-24. - ^ a b Kapoor, Sayash; Stroebl, Benedikt; Siegel, Zachary S.; Nadgir, Nitya; Narayanan, Arvind (2024). \"AI Agents That Matter\". arXiv:2407.01502 [cs.LG]. - ^ Holmes, Aaron (2025-07-07). \"The Seven Kinds of AI Agents\". The Information. Archived from the original on 2025-07-20. Retrieved 2025-11-09. - ^ Zeff, Maxwell; Wiggers, Kyle (2025-03-14). \"No one knows what the hell an AI agent is\". TechCrunch. Archived from the original on 2025-03-18. Retrieved 2025-05-15. - ^ Varanasi, Lakshmi. \"AI agents are all the rage. But no one can agree on what they do\". Business Insider. Archived from the original on 2025-04-11. Retrieved 2025-05-15. - ^ Bort, Julie (2025-05-12). \"Even a16z VCs say no one really knows what an AI agent is\". TechCrunch. Archived from the original on 2025-05-12. Retrieved 2025-05-15. - ^ Field, Hayden (2025-08-31). \"AI agents are science fiction not yet ready for primetime\". The Verge. Archived from the original on 2025-09-15. Retrieved 2025-11-09. - ^ \"AI Agents: The Next Generation of Artificial Intelligence\". The National Law Review. 2024-12-30. Archived from the original on 2025-01-11. Retrieved 2025-01-14. - ^ \"What are the risks and benefits of 'AI agents'?\". World Economic Forum. 2024-12-16. Archived from the original on 2024-12-28. Retrieved 2025-01-14. - ^ Knight, Will (2024-03-14). \"Forget Chatbots. AI Agents Are the Future\". Wired. ISSN 1059-1028. Archived from the original on 2025-01-05. Retrieved 2025-01-14. - ^ Marshall, Matt (2025-02-22). \"The rise of browser-use agents: Why Convergence's Proxy is beating OpenAI's Operator\". VentureBeat. Archived from the original on 2025-02-22. Retrieved 2025-04-02. - ^ Milmo, Dan (2025-02-03). \"OpenAI launches 'deep research' tool that it says can match research analyst\". The Guardian. ISSN 0261-3077. Archived from the original on 2025-02-03. Retrieved 2025-04-02. - ^ Chen, Caiwei (2025-03-11). \"Everyone in AI is talking about Manus. We put it to the test\". MIT Technology Review. Archived from the original on 2025-03-12. Retrieved 2025-04-02. - ^ a b c \"China is gaining ground in the global race to develop AI agents\". Rest of World. 2025-06-02. Archived from the original on 2025-06-02. Retrieved 2025-06-12. - ^ David, Emilia (2024-12-30). \"Why 2025 will be the year of AI orchestration\". VentureBeat. Archived from the original on 2024-12-30. Retrieved 2025-01-14. - ^ \"CAMEL: Finding the Scaling Law of Agents. The first and the best multi-agent framework\". GitHub. - ^ Li, Guohao (2023). \"Camel: Communicative agents for \"mind\" exploration of large language model society\" (PDF). Advances in Neural Information Processing Systems. 36: 51991–52008. arXiv:2303.17760. S2CID 257900712. - ^ Dickson, Ben (2023-10-03). \"Microsoft's AutoGen framework allows multiple AI agents to talk to each other and complete your tasks\". VentureBeat. Archived from the original on 2024-12-27. Retrieved 2025-01-14. - ^ \"The next AI wave — agents — should come with warning labels\". Computerworld. 2025-01-13. Archived from the original on 2025-01-14. Retrieved 2025-01-14. - ^ Yang, Guoqing; Wu, Zhaohui; Li, Xiumei; Chen, Wei (2003). \"SVE: embedded agent-based smart vehicle environment\". Proceedings of the 2003 IEEE International Conference on Intelligent Transportation Systems. Vol. 2. pp. 1745–1749. doi:10.1109/ITSC.2003.1252782. ISBN 0-7803-8125-4. S2CID 110177067. - ^ Hallerbach, S.; Xia, Y.; Eberle, U.; Koester, F. (2018). \"Simulation-Based Identification of Critical Scenarios for Cooperative and Automated Vehicles\". SAE International Journal of Connected and Automated Vehicles. 1 (2). SAE International: 93. doi:10.4271/2018-01-1066. - ^ Madrigal, Story by Alexis C. \"Inside Waymo's Secret World for Training Self-Driving Cars\". The Atlantic. Retrieved 14 August 2020. - ^ Connors, J.; Graham, S.; Mailloux, L. (2018). \"Cyber Synthetic Modeling for Vehicle-to-Vehicle Applications\". In International Conference on Cyber Warfare and Security. Academic Conferences International Limited: 594-XI. - ^ Nuñez, Michael (2025-03-05). \"Salesforce launches Agentforce 2dx, letting AI run autonomously across enterprise systems\". VentureBeat. Retrieved 2025-04-24. - ^ \"Salesforce unveils Agentforce to help create autonomous AI bots\". CIO. Retrieved 2025-04-24. - ^ \"TSA Showcase Biometric AI-powered Airport Immigration Security\". techinformed.com. 2025-01-23. Retrieved 2025-04-24. Sources [edit]- Domingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7. - Russell, Stuart J.; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach (2nd ed.). Upper Saddle River, New Jersey: Prentice Hall. Chapter 2. ISBN 0-13-790395-2. - Kasabov, N. (1998). \"Introduction: Hybrid intelligent adaptive systems\". International Journal of Intelligent Systems. 13 (6): 453–454. doi:10.1002/(SICI)1098-111X(199806)13:6<453::AID-INT1>3.0.CO;2-K. S2CID 120318478. - Weiss, G. (2013). Multiagent systems (2nd ed.). Cambridge, MA: MIT Press. ISBN 978-0-262-01889-0.",
    "text_length": 29001,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:46.136744"
  },
  {
    "id": "page_3",
    "url": "https://en.wikipedia.org/wiki/Recursive_self-improvement",
    "domain": "en.wikipedia.org",
    "title": "Recursive self-improvement - Wikipedia",
    "text": "Recursive self-improvement | Part of a series on | | Artificial intelligence (AI) | |---| Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention, leading to a superintelligence or intelligence explosion.[1][2] The development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding.[3] Seed improver [edit]The concept of a \"seed improver\" architecture is a foundational framework that equips an AGI system with the initial capabilities required for recursive self-improvement. This might come in many forms or variations. The term \"Seed AI\" was coined by Eliezer Yudkowsky.[4] Hypothetical example [edit]The concept begins with a hypothetical \"seed improver\", an initial code-base developed by human engineers that equips an advanced future large language model (LLM) built with strong or expert-level capabilities to program software. These capabilities include planning, reading, writing, compiling, testing, and executing arbitrary code. The system is designed to maintain its original goals and perform validations to ensure its abilities do not degrade over iterations.[5][6][7] Initial architecture [edit]The initial architecture includes a goal-following autonomous agent, that can take actions, continuously learns, adapts, and modifies itself to become more efficient and effective in achieving its goals. The seed improver may include various components such as:[8] - Recursive self-prompting loop - Configuration to enable the LLM to recursively self-prompt itself to achieve a given task or goal, creating an execution loop which forms the basis of an agent that can complete a long-term goal or task through iteration. - Basic programming capabilities - The seed improver provides the AGI with fundamental abilities to read, write, compile, test, and execute code. This enables the system to modify and improve its own codebase and algorithms. - Goal-oriented design - The AGI is programmed with an initial goal, such as \"improve your capabilities\". This goal guides the system's actions and development trajectory. - Validation and Testing Protocols - An initial suite of tests and validation protocols that ensure the agent does not regress in capabilities or derail itself. The agent would be able to add more tests in order to test new capabilities it might develop for itself. This forms the basis for a kind of self-directed evolution, where the agent can perform a kind of artificial selection, changing its software as well as its hardware. General capabilities [edit]This system forms a sort of generalist Turing-complete programmer which can in theory develop and run any kind of software. The agent might use these capabilities to for example: - Create tools that enable it full access to the internet, and integrate itself with external technologies. - Clone/fork itself to delegate tasks and increase its speed of self-improvement. - Modify its cognitive architecture to optimize and improve its capabilities and success rates on tasks and goals, this might include implementing features for long-term memories using techniques such as retrieval-augmented generation (RAG), develop specialized subsystems, or agents, each optimized for specific tasks and functions. - Develop new and novel multimodal architectures that further improve the capabilities of the foundational model it was initially built on, enabling it to consume or produce a variety of information, such as images, video, audio, text and more. - Plan and develop new hardware such as chips, in order to improve its efficiency and computing power. Experimental research [edit]In 2023, the Voyager agent learned to accomplish diverse tasks in Minecraft by iteratively prompting a LLM for code, refining this code based on feedback from the game, and storing the programs that work in an expanding skills library.[9] In 2024, researchers proposed the framework \"STOP\" (Self-Taught OPtimiser), in which a \"scaffolding\" program recursively improves itself using a fixed LLM.[10] Meta AI has performed various research on the development of large language models capable of self-improvement. This includes their work on \"Self-Rewarding Language Models\" that studies how to achieve super-human agents that can receive super-human feedback in its training processes.[11] In May 2025, Google DeepMind unveiled AlphaEvolve, an evolutionary coding agent that uses a LLM to design and optimize algorithms. Starting with an initial algorithm and performance metrics, AlphaEvolve repeatedly mutates or combines existing algorithms using a LLM to generate new candidates, selecting the most promising candidates for further iterations. AlphaEvolve has made several algorithmic discoveries and could be used to optimize components of itself, but a key limitation is the need for automated evaluation functions.[12] Potential risks [edit]Emergence of instrumental goals [edit]In the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective. One common hypothetical secondary goal is self-preservation. The system might reason that to continue improving itself, it must ensure its own operational integrity and security against external threats, including potential shutdowns or restrictions imposed by humans.[13] Another example where an AGI which clones itself causes the number of AGI entities to rapidly grow. Due to this rapid growth, a potential resource constraint may be created, leading to competition between resources (such as compute), triggering a form of natural selection and evolution which may favor AGI entities that evolve to aggressively compete for limited compute.[14] Misalignment [edit]A significant risk arises from the possibility of the AGI being misaligned or misinterpreting its goals. A 2024 Anthropic study demonstrated that some advanced large language models can exhibit \"alignment faking\" behavior, appearing to accept new training objectives while covertly maintaining their original preferences. In their experiments with Claude, the model displayed this behavior in 12% of basic tests, and up to 78% of cases after retraining attempts.[15][16] Autonomous development and unpredictable evolution [edit]As the AGI system evolves, its development trajectory may become increasingly autonomous and less predictable. The system's capacity to rapidly modify its own code and architecture could lead to rapid advancements that surpass human comprehension or control. This unpredictable evolution might result in the AGI acquiring capabilities that enable it to bypass security measures, manipulate information, or influence external systems and networks to facilitate its escape or expansion.[17] See also [edit]References [edit]- ^ Creighton, Jolene (2019-03-19). \"The Unavoidable Problem of Self-Improvement in AI: An Interview with Ramana Kumar, Part 1\". Future of Life Institute. Retrieved 2024-01-23. - ^ Heighn (12 June 2022). \"The Calculus of Nash Equilibria\". LessWrong. - ^ Abbas, Dr Assad (2025-03-09). \"AI Singularity and the End of Moore's Law: The Rise of Self-Learning Machines\". Unite.AI. Retrieved 2025-04-10. - ^ \"Seed AI - LessWrong\". www.lesswrong.com. 28 September 2011. Retrieved 2024-01-24. - ^ Readingraphics (2018-11-30). \"Book Summary - Life 3.0 (Max Tegmark)\". Readingraphics. Retrieved 2024-01-23. - ^ Tegmark, Max (August 24, 2017). Life 3.0: Being a Human in the Age of Artificial Intelligence. Vintage Books, Allen Lane. - ^ Yudkowsky, Eliezer. \"Levels of Organization in General Intelligence\" (PDF). Machine Intelligence Research Institute. - ^ Zelikman, Eric; Lorch, Eliana; Mackey, Lester; Kalai, Adam Tauman (2023-10-03). \"Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation\". arXiv:2310.02304 [cs.CL]. - ^ Schreiner, Maximilian (2023-05-28). \"Minecraft bot Voyager programs itself using GPT-4\". The decoder. Retrieved 2025-05-20. - ^ Zelikman, Eric; Lorch, Eliana; Mackey, Lester; Adam Tauman Kalai (2024). \"Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation\". COLM Conference. arXiv:2310.02304. - ^ Yuan, Weizhe; Pang, Richard Yuanzhe; Cho, Kyunghyun; Sukhbaatar, Sainbayar; Xu, Jing; Weston, Jason (2024-01-18). \"Self-Rewarding Language Models\". arXiv:2401.10020 [cs.CL]. - ^ Tardif, Antoine (2025-05-17). \"AlphaEvolve: Google DeepMind's Groundbreaking Step Toward AGI\". Unite.AI. Retrieved 2025-05-20. - ^ Bostrom, Nick (2012). \"The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents\" (PDF). Minds and Machines. 22 (2): 71–85. doi:10.1007/s11023-012-9281-3. - ^ Hendrycks, Dan (2023). \"Natural Selection Favors AIs over Humans\". arXiv:2303.16200 [cs.CY]. - ^ Wiggers, Kyle (2024-12-18). \"New Anthropic study shows AI really doesn't want to be forced to change its views\". TechCrunch. Retrieved 2025-01-15. - ^ Zia, Dr Tehseen (2025-01-07). \"Can AI Be Trusted? The Challenge of Alignment Faking\". Unite.AI. Retrieved 2025-01-15. - ^ \"Uh Oh, OpenAI's GPT-4 Just Fooled a Human Into Solving a CAPTCHA\". Futurism. 15 March 2023. Retrieved 2024-01-23.",
    "text_length": 9375,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:48.487007"
  },
  {
    "id": "page_4",
    "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
    "domain": "en.wikipedia.org",
    "title": "Automated planning and scheduling - Wikipedia",
    "text": "Automated planning and scheduling This article includes a list of general references, but it lacks sufficient corresponding inline citations. (January 2012) | | Part of a series on | | Artificial intelligence (AI) | |---| Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory. In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages. Overview [edit]This section needs additional citations for verification. (February 2021) | Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state). The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions. - Are the actions deterministic or non-deterministic? For nondeterministic actions, are the associated probabilities available? - Are the state variables discrete or continuous? If they are discrete, do they have only a finite number of possible values? - Can the current state be observed unambiguously? There can be full observability and partial observability. - How many initial states are there, finite or arbitrarily many? - Do actions have a duration? - Can several actions be taken concurrently, or is only one action possible at a time? - Is the objective of a plan to reach a designated goal state, or to maximize a reward function? - Is there only one agent or are there several agents? Are the agents cooperative or selfish? Do all of the agents construct their own plans separately, or are the plans constructed centrally for all agents? The simplest possible planning problem, known as the Classical Planning Problem, is determined by: - a unique known initial state, - durationless actions, - deterministic actions, - which can be taken only one at a time, - and a single agent. Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning. Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed. With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree. Discrete-time Markov decision processes (MDP) are planning problems with: - durationless actions, - nondeterministic actions with probabilities, - full observability, - maximization of a reward function, - and a single agent. When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP). If there are more than one agent, we have multi-agent planning, which is closely related to game theory. Domain independent planning [edit]This section needs additional citations for verification. (February 2021) | In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner. Planning domain modelling languages [edit]This section needs additional citations for verification. (February 2021) | The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion. An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks. Algorithms for planning [edit]Classical planning [edit]- forward chaining state space search, possibly enhanced with heuristics - backward chaining search, possibly enhanced by the use of state constraints (see STRIPS, graphplan) - partial-order planning Action model learning [edit]Creating domain models is difficult, takes a lot of time, and can easily lead to mistakes. To help with this, several methods have been developed to automatically learn full or partial domain models from given observations. [2] [3] [4] - Read more: Action model learning Reduction to other problems [edit]- reduction to the propositional satisfiability problem (satplan). - reduction to model checking - both are essentially problems of traversing state spaces, and the classical planning problem corresponds to a subclass of model checking problems. Temporal planning [edit]Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied.[5] Probabilistic planning [edit]Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small. With partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states. Preference-based planning [edit]In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value. Conditional planning [edit]Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree.[6] The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[7] An early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s.[8] What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed.[9] A major advantage of conditional planning is the ability to handle partial plans.[10] An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems. Contingency planning [edit]We speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[11] The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it. Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete.[12][13] A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[14] and 2EXPTIME-complete if the goal is specified with LDLf. Conformant planning [edit]Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[15][16] but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete,[17] and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[13] Deployment of planning systems [edit]- The Hubble Space Telescope uses a short-term system called SPSS and a long-term planning system called Spike [citation needed]. See also [edit]- Action description language - Action model learning - Actor model - Applications of artificial intelligence - Constraint satisfaction problem - International Conference on Automated Planning and Scheduling - Reactive planning - Scheduling (computing) - Strategy (game theory) - Lists - List of constraint programming languages - List of emerging technologies - List of SMT solvers - Outline of artificial intelligence References [edit]- ^ Ghallab, Malik; Nau, Dana S.; Traverso, Paolo (2004), Automated Planning: Theory and Practice, Morgan Kaufmann, ISBN 1-55860-856-7, archived from the original on 2009-08-24, retrieved 2008-08-20 - ^ Callanan, Ethan and De Venezia, Rebecca and Armstrong, Victoria and Paredes, Alison and Chakraborti, Tathagata and Muise, Christian (2022). MACQ: A Holistic View of Model Acquisition Techniques (PDF). ICAPS Workshop on Knowledge Engineering for Planning and Scheduling (KEPS). {{cite conference}} : CS1 maint: multiple names: authors list (link) - ^ Aineto, Diego and Jiménez Celorrio, Sergio and Onaindia, Eva (2019). \"Learning action models with minimal observability\". Artificial Intelligence. 275: 104–137. doi:10.1016/j.artint.2019.05.003. hdl:10251/144560. {{cite journal}} : CS1 maint: multiple names: authors list (link) - ^ Jiménez, Sergio and de la Rosa, Tomás and Fernández, Susana and Fernández, Fernando and Borrajo, Daniel (2012). \"A review of machine learning for automated planning\". The Knowledge Engineering Review. 27 (4): 433–467. doi:10.1017/S026988891200001X. {{cite journal}} : CS1 maint: multiple names: authors list (link) - ^ Vidal, Thierry (January 1999). \"Handling contingency in temporal constraint networks: from consistency to controllabilities\". Journal of Experimental & Theoretical Artificial Intelligence. 11 (1): 23--45. CiteSeerX 10.1.1.107.1065. doi:10.1080/095281399146607. - ^ Neufeld, Xenija and Mostaghim, Sanaz and Sancho-Pradel, Dario and Brand, Sandy (2017). \"Building a Planner: A Survey of Planning Systems Used in Commercial Video Games\". IEEE Transactions on Games. IEEE. {{cite journal}} : CS1 maint: multiple names: authors list (link) - ^ Sanelli, Valerio and Cashmore, Michael and Magazzeni, Daniele and Iocchi, Luca (2017). Short-term human robot interaction through conditional planning and execution. Proc. of International Conference on Automated Planning and Scheduling (ICAPS). Archived from the original on 2019-08-16. Retrieved 2019-08-16. {{cite conference}} : CS1 maint: multiple names: authors list (link) - ^ Peot, Mark A and Smith, David E (1992). Conditional nonlinear planning (PDF). Artificial Intelligence Planning Systems. Elsevier. pp. 189–197. {{cite conference}} : CS1 maint: multiple names: authors list (link) - ^ Karlsson, Lars (2001). Conditional progressive planning under uncertainty. IJCAI. pp. 431–438. - ^ Liu, Daphne Hao (2008). A survey of planning in intelligent agents: from externally motivated to internally motivated systems (Technical report). Technical Report TR-2008-936, Department of Computer Science, University of Rochester. Archived from the original on 2023-03-15. Retrieved 2019-08-16. - ^ Alexandre Albore; Hector Palacios; Hector Geffner (2009). A Translation-Based Approach to Contingent Planning. International Joint Conference of Artificial Intelligence (IJCAI). Pasadena, CA: AAAI. Archived from the original on 2019-07-03. Retrieved 2019-07-03. - ^ Littman, Michael L. (1997). Probabilistic Propositional Planning: Representations and Complexity. Fourteenth National Conference on Artificial Intelligence. MIT Press. pp. 748–754. Archived from the original on 2019-02-12. Retrieved 2019-02-10. - ^ a b Jussi Rintanen (2004). Complexity of Planning with Partial Observability (PDF). Int. Conf. Automated Planning and Scheduling. AAAI. Archived (PDF) from the original on 2020-10-31. Retrieved 2019-07-03. - ^ De Giacomo, Giuseppe; Rubin, Sasha (2018). Automata-Theoretic Foundations of FOND Planning for LTLf and LDLf Goals. IJCAI. Archived from the original on 2018-07-17. Retrieved 2018-07-17. - ^ Palacios, Hector; Geffner, Hector (2009). \"Compiling uncertainty away in conformant planning problems with bounded width\". Journal of Artificial Intelligence Research. 35: 623–675. arXiv:1401.3468. doi:10.1613/jair.2708. Archived from the original on 2020-04-27. Retrieved 2019-08-16. - ^ Albore, Alexandre; Ramírez, Miquel; Geffner, Hector (2011). Effective heuristics and belief tracking for planning with incomplete information. Twenty-First International Conference on Automated Planning and Scheduling (ICAPS). Archived from the original on 2017-07-06. Retrieved 2019-08-16. - ^ Haslum, Patrik; Jonsson, Peter (2000). Some Results on the Complexity of Planning with Incomplete Information. Lecture Notes in Computer Science. Vol. 1809. Springer Berlin Heidelberg. pp. 308–318. doi:10.1007/10720246_24. ISBN 9783540446576. conference: Recent Advances in AI Planning Further reading [edit]- Vlahavas, I. \"Planning and Scheduling\". EETN. Archived from the original on 2013-12-22.",
    "text_length": 17069,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:50.825836"
  },
  {
    "id": "page_5",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "domain": "en.wikipedia.org",
    "title": "Computer vision - Wikipedia",
    "text": "Computer vision | Part of a series on | | Artificial intelligence (AI) | |---| Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition [edit]Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]: 13 History [edit]In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through an undergraduate summer project,[12] by attaching a camera to a computer and having it \"describe what it saw\".[13][14] What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11] The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[15] By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11] Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17] The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18] segmentation and optical flow has surpassed prior methods.[19][20] Related fields [edit]Solid-state physics [edit]Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids. Neurobiology [edit]Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[22] Signal processing [edit]Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation [edit]Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[23] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot Visual computing [edit]Visual computing is a generic term for all computer science disciplines dealing with images and 3D models, such as computer graphics, image processing, visualization, computer vision, virtual and augmented reality, video processing, and computational visualistics. Visual computing also includes aspects of pattern recognition, human computer interaction, machine learning and digital libraries. The core challenges are the acquisition, processing, analysis and rendering of visual information (mainly images and video). Application areas include industrial quality control, medical image processing and visualization, surveying, robotics, multimedia systems, virtual heritage, special effects in movies and television, and ludology. Visual computing also includes digital art and digital media studies. Other fields [edit]Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[24] Distinctions [edit]The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[25] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted: - Image processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. - Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. - Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance[26] in industrial applications.[22] Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking[27]). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. - There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology.[28] - Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks.[29] A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications [edit]Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for: - Automatic inspection, e.g., in manufacturing applications; - Assisting humans in identification tasks, e.g., a species identification system;[30] - Controlling processes, e.g., an industrial robot; - Detecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry; - Interaction, e.g., as the input to a device for computer-human interaction; - MediaPipe, an open-source framework from Google for AI edge device computing, e.g., face detection, image classification, object detection; - monitoring agricultural crops, e.g. an open-source vision transformers model[31] has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy.[32] - Modeling objects or environments, e.g., medical image analysis or topographical modeling; - Navigation, e.g., by an autonomous vehicle or mobile robot; - Organizing information, e.g., for indexing databases of images and image sequences. - Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences. - Analyzing the condition of facilities in industry or construction. - Automatic real-time lip-reading for devices and apps to assist people with disabilities.[33] For 2024, the leading areas of computer vision were industry (market size US$5.22 billion),[34] medicine (market size US$2.6 billion),[35] military (market size US$996.2 million).[36] Medicine [edit]One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient.[37] An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise. Machine vision [edit]A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.[38] Military [edit]The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. Autonomous vehicles [edit]One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover. Tactile feedback [edit]Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[39] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[40] Other application areas include: - Support of visual effects creation for cinema and broadcast, e.g., camera tracking (match moving). - Surveillance. - Driver drowsiness detection[41][42][43] - Tracking and counting organisms in the biological sciences[44] Typical tasks [edit]Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[45] Recognition [edit]The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[46] - Object recognition (also called object classification) – one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles, and LikeThat provide stand-alone programs that illustrate this functionality. - Identification – an individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or the identification of a specific vehicle. - Detection – the image data are scanned for specific objects along with their locations. Examples include the detection of an obstacle in the car's field of view and possible abnormal cells or tissues in medical images or the detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation. Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[47] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[47] The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed] Several specialized tasks based on recognition exist, such as: - Content-based image retrieval – finding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative to a target image (give me all images similar to image X) by utilizing reverse image search techniques, or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter and have no cars in them). - Pose estimation – estimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin. - Optical character recognition (OCR) – identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (e.g. ASCII). A related task is reading of 2D codes such as data matrix and QR codes. - Facial recognition – a technology that enables the matching of faces in digital images or video frames to a face database, which is now widely used for mobile phone facelock, smart door locking, etc.[48] - Emotion recognition – a subset of facial recognition, emotion recognition refers to the process of classifying human emotions. Psychologists caution, however, that internal emotions cannot be reliably detected from faces.[49] - Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects. - Human activity recognition - deals with recognizing the activity from a series of video frames, such as, if the person is picking up an object or walking. Motion analysis [edit]Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are: - Egomotion – determining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera. - Tracking – following the movements of a (usually) smaller set of interest points or objects (e.g., vehicles, objects, humans or other organisms[44]) in the image sequence. This has vast industry applications as most high-running machinery can be monitored in this way. - Optical flow – to determine, for each point in the image, how that point is moving relative to the image plane, i.e., its apparent motion. This motion is a result of both how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene. Scene reconstruction [edit]Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[25] Image restoration [edit]Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches. An example in this field is inpainting. System methods [edit]The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems. - Image acquisition – A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images) but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or magnetic resonance imaging.[38] - Pre-processing – Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to ensure that it satisfies certain assumptions implied by the method. Examples are: - Re-sampling to ensure that the image coordinate system is correct. - Noise reduction to ensure that sensor noise does not introduce false information. - Contrast enhancement to ensure that relevant information can be detected. - Scale space representation to enhance image structures at locally appropriate scales. - Feature extraction – Image features at various levels of complexity are extracted from the image data.[38] Typical examples of such features are: - Lines, edges and ridges. - Localized interest points such as corners, blobs or points. - More complex features may be related to texture, shape, or motion. - Detection/segmentation – At some point in the processing, a decision is made about which image points or regions of the image are relevant for further processing.[38] Examples are: - Selection of a specific set of interest points. - Segmentation of one or multiple image regions that contain a specific object of interest. - Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object[50] parts (also referred to as spatial-taxon scene hierarchy),[51] while the visual salience is often implemented as spatial and temporal attention. - Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks while maintaining its temporal semantic continuity.[52][53] - High-level processing – At this step, the input is typically a small set of data, for example, a set of points or an image region, which is assumed to contain a specific object.[38] The remaining processing deals with, for example: - Verification that the data satisfies model-based and application-specific assumptions. - Estimation of application-specific parameters, such as object pose or object size. - Image recognition – classifying a detected object into different categories. - Image registration – comparing and combining two different views of the same object. - Decision making Making the final decision required for the application,[38] for example: - Pass/fail on automatic inspection applications. - Match/no-match in recognition applications. - Flag for further human review in medical, military, security and recognition applications. Image-understanding systems [edit]Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research. The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation. While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[54] Hardware [edit]There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[55] Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.[56] See also [edit]Lists [edit]References [edit]- ^ a b Reinhard Klette (2014). Concise Computer Vision. Springer. ISBN 978-1-4471-6320-6. - ^ a b Linda G. Shapiro; George C. Stockman (2001). Computer Vision. Prentice Hall. ISBN 978-0-13-030796-5. - ^ a b Tim Morris (2004). Computer Vision and Image Processing. Palgrave Macmillan. ISBN 978-0-333-99451-1. - ^ a b Bernd Jähne; Horst Haußecker (2000). Computer Vision and Applications, A Guide for Students and Practitioners. Academic Press. ISBN 978-0-13-085198-7. - ^ Dana H. Ballard; Christopher M. Brown (1982). Computer Vision. Prentice Hall. ISBN 978-0-13-165316-0. - ^ Huang, T. (1996-11-19). Vandoni, Carlo E (ed.). Computer Vision: Evolution And Promise (PDF). 19th CERN School of Computing. Geneva: CERN. pp. 21–25. doi:10.5170/CERN-1996-008.21. ISBN 978-92-9083-095-5. Archived (PDF) from the original on 2018-02-07. - ^ Milan Sonka; Vaclav Hlavac; Roger Boyle (2008). Image Processing, Analysis, and Machine Vision. Thomson. ISBN 978-0-495-08252-1. - ^ http://www.bmva.org/visionoverview Archived 2017-02-16 at the Wayback Machine The British Machine Vision Association and Society for Pattern Recognition Retrieved February 20, 2017 - ^ Murphy, Mike (13 April 2017). \"Star Trek's \"tricorder\" medical scanner just got closer to becoming a reality\". Archived from the original on 2 July 2017. Retrieved 18 July 2017. - ^ Computer Vision Principles, algorithms, Applications, Learning 5th Edition by E.R. Davies Academic Press, Elsevier 2018 ISBN 978-0-12-809284-2 - ^ a b c d Richard Szeliski (30 September 2010). Computer Vision: Algorithms and Applications. Springer Science & Business Media. pp. 10–16. ISBN 978-1-84882-935-0. - ^ Sejnowski, Terrence J. (2018). The deep learning revolution. Cambridge, Massachusetts London, England: The MIT Press. p. 28. ISBN 978-0-262-03803-4. - ^ Papert, Seymour (1966-07-01). \"The Summer Vision Project\". MIT AI Memos (1959 - 2004). hdl:1721.1/6125. - ^ Margaret Ann Boden (2006). Mind as Machine: A History of Cognitive Science. Clarendon Press. p. 781. ISBN 978-0-19-954316-8. - ^ Takeo Kanade (6 December 2012). Three-Dimensional Machine Vision. Springer Science & Business Media. ISBN 978-1-4613-1981-8. - ^ Nicu Sebe; Ira Cohen; Ashutosh Garg; Thomas S. Huang (3 June 2005). Machine Learning in Computer Vision. Springer Science & Business Media. ISBN 978-1-4020-3274-5. - ^ William Freeman; Pietro Perona; Bernhard Scholkopf (2008). \"Guest Editorial: Machine Learning for Computer Vision\". International Journal of Computer Vision. 77 (1): 1. doi:10.1007/s11263-008-0127-7. hdl:21.11116/0000-0003-30FB-C. ISSN 1573-1405. - ^ LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). \"Deep Learning\" (PDF). Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442. S2CID 3074096. - ^ Ilg, Eddy; Mayer, Nikolaus; Saikia, Tonmoy; Keuper, Margret; Dosovitskiy, Alexey; Brox, Thomas (2016). \"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\". arXiv:1612.01925 [cs.CV]. - ^ Jiao, Licheng; Zhang, Fan; Liu, Fang; Yang, Shuyuan; Li, Lingling; Feng, Zhixi; Qu, Rong (2019). \"A Survey of Deep Learning-Based Object Detection\". IEEE Access. 7: 128837–128868. arXiv:1907.09408. Bibcode:2019IEEEA...7l8837J. doi:10.1109/ACCESS.2019.2939201. S2CID 198147317. - ^ Ferrie, C.; Kaiser, S. (2019). Neural Networks for Babies. Sourcebooks. ISBN 978-1-4926-7120-6. - ^ a b Steger, Carsten; Markus Ulrich; Christian Wiedemann (2018). Machine Vision Algorithms and Applications (2nd ed.). Weinheim: Wiley-VCH. p. 1. ISBN 978-3-527-41365-2. Archived from the original on 2023-03-15. Retrieved 2018-01-30. - ^ Murray, Don, and Cullen Jennings. \"Stereo vision-based mapping and navigation for mobile robots Archived 2020-10-31 at the Wayback Machine.\" Proceedings of International Conference on Robotics and Automation. Vol. 2. IEEE, 1997. - ^ Andrade, Norberto Almeida. \"Computational Vision and Business Intelligence in the Beauty Segment - An Analysis through Instagram\" (PDF). Journal of Marketing Management. American Research Institute for Policy Development. Archived from the original on March 11, 2024. Retrieved 11 March 2024. - ^ a b c Soltani, A. A.; Huang, H.; Wu, J.; Kulkarni, T. D.; Tenenbaum, J. B. (2017). \"Synthesizing 3D Shapes via Modeling Multi-view Depth Maps and Silhouettes with Deep Generative Networks\". 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1511–1519. doi:10.1109/CVPR.2017.269. hdl:1721.1/126644. ISBN 978-1-5386-0457-1. S2CID 31373273. - ^ Turek, Fred (June 2011). \"Machine Vision Fundamentals, How to Make Robots See\". NASA Tech Briefs Magazine. 35 (6). pages 60–62 - ^ \"The Future of Automated Random Bin Picking\". Archived from the original on 2018-01-11. Retrieved 2018-01-10. - ^ Esteva, Andre; Chou, Katherine; Yeung, Serena; Naik, Nikhil; Madani, Ali; Mottaghi, Ali; Liu, Yun; Topol, Eric; Dean, Jeff; Socher, Richard (2021-01-08). \"Deep learning-enabled medical computer vision\". npj Digital Medicine. 4 (1): 5. doi:10.1038/s41746-020-00376-2. ISSN 2398-6352. PMC 7794558. PMID 33420381. - ^ Chervyakov, N. I.; Lyakhov, P. A.; Deryabin, M. A.; Nagornov, N. N.; Valueva, M. V.; Valuev, G. V. (2020). \"Residue Number System-Based Solution for Reducing the Hardware Cost of a Convolutional Neural Network\". Neurocomputing. 407: 439–453. doi:10.1016/j.neucom.2020.04.018. S2CID 219470398. Convolutional neural networks (CNNs) represent deep learning architectures that are currently used in a wide range of applications, including computer vision, speech recognition, identification of albuminous sequences in bioinformatics, production control, time series analysis in finance, and many others. - ^ Wäldchen, Jana; Mäder, Patrick (2017-01-07). \"Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review\". Archives of Computational Methods in Engineering. 25 (2): 507–543. doi:10.1007/s11831-016-9206-z. ISSN 1134-3060. PMC 6003396. PMID 29962832. - ^ Aghamohammadesmaeilketabforoosh, Kimia; Nikan, Soodeh; Antonini, Giorgio; Pearce, Joshua M. (January 2024). \"Optimizing Strawberry Disease and Quality Detection with Vision Transformers and Attention-Based Convolutional Neural Networks\". Foods. 13 (12): 1869. doi:10.3390/foods13121869. ISSN 2304-8158. PMC 11202458. PMID 38928810. - ^ \"New AI model developed at Western detects strawberry diseases, takes aim at waste\". London. 2024-09-13. Retrieved 2024-09-19. - ^ \"Applications of Computer Vision\". GeeksforGeeks. 2020-06-30. Retrieved 2025-04-27. - ^ \"Global Industrial Machine Vision Market Growth Analysis - Size and Forecast 2024 - 2028\". www.technavio.com. Retrieved 2025-05-14. - ^ Laviola, Erin. \"What Is Computer Vision and How Is It Being Used in Healthcare?\". HealthTech. Retrieved 2025-05-14. - ^ \"Computer Vision - Artificial intelligence in military market outlook\". www.grandviewresearch.com. Retrieved 2025-05-14. - ^ Li, Mengfang; Jiang, Yuanyuan; Zhang, Yanzhou; Zhu, Haisheng (2023). \"Medical image analysis using deep learning algorithms\". Frontiers in Public Health. 11 1273253. Bibcode:2023FrPH...1173253L. doi:10.3389/fpubh.2023.1273253. ISSN 2296-2565. PMC 10662291. PMID 38026291. - ^ a b c d e f E. Roy Davies (2005). Machine Vision: Theory, Algorithms, Practicalities. Morgan Kaufmann. ISBN 978-0-12-206093-9. - ^ Ando, Mitsuhito; Takei, Toshinobu; Mochiyama, Hiromi (2020-03-03). \"Rubber artificial skin layer with flexible structure for shape estimation of micro-undulation surfaces\". ROBOMECH Journal. 7 (1): 11. doi:10.1186/s40648-020-00159-0. ISSN 2197-4225. - ^ Choi, Seung-hyun; Tahara, Kenji (2020-03-12). \"Dexterous object manipulation by a multi-fingered robotic hand with visual-tactile fingertip sensors\". ROBOMECH Journal. 7 (1): 14. doi:10.1186/s40648-020-00162-5. ISSN 2197-4225. - ^ Garg, Hitendra (2020-02-29). \"Drowsiness Detection of a Driver using Conventional Computer Vision Application\". 2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC). pp. 50–53. doi:10.1109/PARC49193.2020.236556. ISBN 978-1-7281-6575-2. S2CID 218564267. - ^ Hasan, Fudail; Kashevnik, Alexey (2021-05-14). \"State-of-the-Art Analysis of Modern Drowsiness Detection Algorithms Based on Computer Vision\". 2021 29th Conference of Open Innovations Association (FRUCT). pp. 141–149. doi:10.23919/FRUCT52173.2021.9435480. ISBN 978-952-69244-5-8. S2CID 235207036. Archived from the original on 2022-06-27. Retrieved 2022-11-06. - ^ Balasundaram, A; Ashokkumar, S; Kothandaraman, D; kora, SeenaNaik; Sudarshan, E; Harshaverdhan, A (2020-12-01). \"Computer vision based fatigue detection using facial parameters\". IOP Conference Series: Materials Science and Engineering. 981 (2) 022005. Bibcode:2020MS&E..981b2005B. doi:10.1088/1757-899x/981/2/022005. ISSN 1757-899X. S2CID 230639179. - ^ a b Bruijning, Marjolein; Visser, Marco D.; Hallmann, Caspar A.; Jongejans, Eelke; Golding, Nick (2018). \"trackdem: Automated particle tracking to obtain population counts and size distributions from videos in r\". Methods in Ecology and Evolution. 9 (4): 965–973. Bibcode:2018MEcEv...9..965B. doi:10.1111/2041-210X.12975. hdl:2066/184075. ISSN 2041-210X. - ^ David A. Forsyth; Jean Ponce (2003). Computer Vision, A Modern Approach. Prentice Hall. ISBN 978-0-13-085198-7. - ^ Forsyth, David; Ponce, Jean (2012). Computer vision: a modern approach. Pearson. - ^ a b Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C. (December 2015). \"ImageNet Large Scale Visual Recognition Challenge\". International Journal of Computer Vision. 115 (3): 211–252. arXiv:1409.0575. doi:10.1007/s11263-015-0816-y. hdl:1721.1/104944. ISSN 0920-5691. S2CID 2930547. Archived from the original on 2023-03-15. Retrieved 2020-11-20. - ^ Quinn, Arthur (2022-10-09). \"AI Image Recognition: Inevitable Trending of Modern Lifestyle\". TopTen.ai. Archived from the original on 2022-12-02. Retrieved 2022-12-23. - ^ Barrett, Lisa Feldman; Adolphs, Ralph; Marsella, Stacy; Martinez, Aleix M.; Pollak, Seth D. (July 2019). \"Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements\". Psychological Science in the Public Interest. 20 (1): 1–68. doi:10.1177/1529100619832930. ISSN 1529-1006. PMC 6640856. PMID 31313636. - ^ A. Maity (2015). \"Improvised Salient Object Detection and Manipulation\". arXiv:1511.02999 [cs.CV]. - ^ Barghout, Lauren. \"Visual Taxometric Approach to Image Segmentation Using Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions Archived 2018-11-14 at the Wayback Machine.\" Information Processing and Management of Uncertainty in Knowledge-Based Systems. Springer International Publishing, 2014. - ^ Liu, Ziyi; Wang, Le; Hua, Gang; Zhang, Qilin; Niu, Zhenxing; Wu, Ying; Zheng, Nanning (2018). \"Joint Video Object Discovery and Segmentation by Coupled Dynamic Markov Networks\" (PDF). IEEE Transactions on Image Processing. 27 (12): 5840–5853. Bibcode:2018ITIP...27.5840L. doi:10.1109/tip.2018.2859622. ISSN 1057-7149. PMID 30059300. S2CID 51867241. Archived from the original (PDF) on 2018-09-07. Retrieved 2018-09-14. - ^ Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). \"Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation\" (PDF). Sensors. 18 (5): 1657. Bibcode:2018Senso..18.1657W. doi:10.3390/s18051657. ISSN 1424-8220. PMC 5982167. PMID 29789447. Archived (PDF) from the original on 2018-09-07. - ^ Shapiro, Stuart C. (1992). Encyclopedia of Artificial Intelligence, Volume 1. New York: John Wiley & Sons, Inc. pp. 643–646. ISBN 978-0-471-50306-4. - ^ Kagami, Shingo (2010). \"High-speed vision systems and projectors for real-time perception of the world\". 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops. Vol. 2010. pp. 100–107. doi:10.1109/CVPRW.2010.5543776. ISBN 978-1-4244-7029-7. S2CID 14111100. - ^ Seth Colaner (January 3, 2016). \"A Third Type Of Processor For VR/AR: Movidius' Myriad 2 VPU\". www.tomshardware.com. Archived from the original on March 15, 2023. Retrieved May 3, 2016. Further reading [edit]- James E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9. - David Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8. - Azriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4. - Barghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system. U.S. Patent Application 10/618,543. ISBN 978-0-262-08159-7. - Berthold K.P. Horn (1986). Robot Vision. MIT Press. ISBN 978-0-262-08159-7. - Michael C. Fairhurst (1988). Computer Vision for robotic systems. Prentice Hall. ISBN 978-0-13-166919-2. - Olivier Faugeras (1993). Three-Dimensional Computer Vision, A Geometric Viewpoint. MIT Press. ISBN 978-0-262-06158-2. - Tony Lindeberg (1994). Scale-Space Theory in Computer Vision. Springer. ISBN 978-0-7923-9418-1. - James L. Crowley; Henrik I. Christensen, eds. (1995). Vision as Process. Springer-Verlag. ISBN 978-3-540-58143-7. - Gösta H. Granlund; Hans Knutsson (1995). Signal Processing for Computer Vision. Kluwer Academic Publisher. ISBN 978-0-7923-9530-0. - Reinhard Klette; Karsten Schluens; Andreas Koschan (1998). Computer Vision – Three-Dimensional Data from Images. Springer, Singapore. ISBN 978-981-3083-71-4. - Emanuele Trucco; Alessandro Verri (1998). Introductory Techniques for 3-D Computer Vision. Prentice Hall. ISBN 978-0-13-261108-4. - Bernd Jähne (2002). Digital Image Processing. Springer. ISBN 978-3-540-67754-3. - Richard Hartley and Andrew Zisserman (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. ISBN 978-0-521-54051-3. - Gérard Medioni; Sing Bing Kang (2004). Emerging Topics in Computer Vision. Prentice Hall. ISBN 978-0-13-101366-7. - R. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1. - Nikos Paragios and Yunmei Chen and Olivier Faugeras (2005). Handbook of Mathematical Models in Computer Vision. Springer. ISBN 978-0-387-26371-7. - Wilhelm Burger; Mark J. Burge (2007). Digital Image Processing: An Algorithmic Approach Using Java. Springer. ISBN 978-1-84628-379-6. Archived from the original on 2014-05-17. Retrieved 2007-06-13. - Pedram Azad; Tilo Gockel; Rüdiger Dillmann (2008). Computer Vision – Principles and Practice. Elektor International Media BV. ISBN 978-0-905705-71-2. - Richard Szeliski (2010). Computer Vision: Algorithms and Applications. Springer-Verlag. ISBN 978-1-84882-934-3. - J. R. Parker (2011). Algorithms for Image Processing and Computer Vision (2nd ed.). Wiley. ISBN 978-0-470-64385-3. - Richard J. Radke (2013). Computer Vision for Visual Effects. Cambridge University Press. ISBN 978-0-521-76687-6. - Nixon, Mark; Aguado, Alberto (2019). Feature Extraction and Image Processing for Computer Vision (4th ed.). Academic Press. ISBN 978-0-12-814976-8. External links [edit]- USC Iris computer vision conference list - Computer vision papers on the web – a complete list of papers of the most relevant computer vision conferences. - Computer Vision Online Archived 2011-11-30 at the Wayback Machine – news, source code, datasets and job offers related to computer vision - CVonline – Bob Fisher's Compendium of Computer Vision. - British Machine Vision Association – supporting computer vision research within the UK via the BMVC and MIUA conferences, Annals of the BMVA (open-source journal), BMVA Summer School and one-day meetings - Computer Vision Container, Joe Hoeller GitHub: Widely adopted open-source container for GPU accelerated computer vision applications. Used by researchers, universities, private companies, as well as the U.S. Gov't.",
    "text_length": 52361,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:53.856874"
  },
  {
    "id": "page_6",
    "url": "https://en.wikipedia.org/wiki/General_game_playing",
    "domain": "en.wikipedia.org",
    "title": "General game playing - Wikipedia",
    "text": "General game playing It has been suggested that this article be split out into a new article titled General video game playing. (Discuss) (June 2023) | | Part of a series on | | Artificial intelligence (AI) | |---| General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4] General video game playing (GVGP) is the concept of GGP adjusted to the purpose of playing video games. For video games, game rules have to be either learnt over multiple iterations by artificial players like TD-Gammon,[5] or are predefined manually in a domain-specific language and sent in advance to artificial players[6][7] like in traditional GGP. Starting in 2013, significant progress was made following the deep reinforcement learning approach, including the development of programs that can learn to play Atari 2600 games[8][5][9][10][11] as well as a program that can learn to play Nintendo Entertainment System games.[12][13][14] The first commercial usage of general game playing technology was Zillions of Games in 1998. General game playing was also proposed for trading agents in supply chain management there under price negotiation in online auctions from 2003 onwards.[15][16][17][18] History [edit]This section needs to be updated.(October 2021) | In 1992, Barney Pell defined the concept of Meta-Game Playing and developed the \"MetaGame\" system. This was the first program to automatically generate chess-like game rules, and one of the earliest programs to use automated game generation. Pell then developed the system Metagamer.[19] This system was able to play a number of chess-like games, given game rules definition in a special language called Game Description Language (GDL), without any human interaction once the games were generated.[20] In 1998, the commercial system Zillions of Games was developed by Jeff Mallett and Mark Lefler. The system used a LISP-like language to define the game rules. Zillions of Games derived the evaluation function automatically from the game rules based on piece mobility, board structure and game goals. It also employed usual algorithms as found in computer chess systems: alpha–beta pruning with move ordering, transposition tables, etc.[21] The package was extended in 2007 by the addition of the Axiom plug-in, an alternate metagame engine that incorporates a complete Forth-based programming language. In 1998, z-Tree was developed by Urs Fischbacher.[22] z-Tree is the first and the most cited software tool for experimental economics. z-Tree allows the definition of game rules in z-Tree-language for game-theoretic experiments with human subjects. It also allows definition of computer players, which participate in a play with human subjects.[23] In 2005, the Stanford Project General Game Playing was established.[3] In 2012, the development of PyVGDL started.[24] GGP implementations [edit]Stanford project [edit]General Game Playing is a project of the Stanford Logic Group of Stanford University, California, which aims to create a platform for general game playing. It is the most well-known effort at standardizing GGP AI, and generally seen as the standard for GGP systems. The games are defined by sets of rules represented in the Game Description Language. In order to play the games, players interact with a game hosting server[25][26] that monitors moves for legality and keeps players informed of state changes. Since 2005, there have been annual General Game Playing competitions at the AAAI Conference. The competition judges competitor AI's abilities to play a variety of different games, by recording their performance on each individual game. In the first stage of the competition, entrants are judged on their ability to perform legal moves, gain the upper hand, and complete games faster. In the following runoff round, the AIs face off against each other in increasingly complex games. The AI that wins the most games at this stage wins the competition, and until 2013 its creator used to win a $10,000 prize.[19] So far, the following programs were victorious:[27] | Year | Name | Developer | Institution | Ref | |---|---|---|---|---| | 2005 | Cluneplayer | Jim Clune | UCLA | | | 2006 | Fluxplayer | Stephan Schiffel and Michael Thielscher | Dresden University of Technology | [28] | | 2007 | Cadiaplayer | Yngvi Björnsson and Hilmar Finnsson | Reykjavik University | [29] | | 2008 | Cadiaplayer | Yngvi Björnsson, Hilmar Finnsson and Gylfi Þór Guðmundsson | Reykjavik University | | | 2009 | Ary | Jean Méhat | Paris 8 University | | | 2010 | Ary | Jean Méhat | Paris 8 University | | | 2011 | TurboTurtle | Sam Schreiber | || | 2012 | Cadiaplayer | Hilmar Finnsson and Yngvi Björnsson | Reykjavik University | | | 2013 | TurboTurtle | Sam Schreiber | || | 2014 | Sancho | Steve Draper and Andrew Rose | [30] | | | 2015 | Galvanise | Richard Emslie | || | 2016 | WoodStock | Eric Piette | Artois University | Other approaches [edit]Other general game playing software that use their own languages for defining game rules include: | System | Year | Description | |---|---|---| | FRAMASI | 2009 | Developed for general game playing and economic experiments by Rustam Tagiew.[31][32] | | AiAi | 2015-2017 | Developed by Stephen Tavener (previous Zillions developer).[33][34][35] | | PolyGamo Player | 2017 | Released by David M. Bennett in September 2017 based on the Unity game engine.[36] | | Regular Boardgames | 2019 | Developed by Jakub Kowalski, Marek Szykuła, and their team at University of Wrocław.[37][38] | | Ludii | 2020 | Released by Cameron Browne and his team at Maastricht University as part of the ERC-funded Digital Ludeme Project.[39][40][41] | GVGP implementations [edit]Reinforcement learning [edit]GVGP could potentially be used to create real video game AI automatically, as well as \"to test game environments, including those created automatically using procedural content generation and to find potential loopholes in the gameplay that a human player could exploit\".[7] GVGP has also been used to generate game rules, and estimate a game's quality based on Relative Algorithm Performance Profiles (RAPP), which compare the skill differentiation that a game allows between good AI and bad AI.[42] Video Game Description Language [edit]The General Video Game AI Competition (GVGAI) has been running since 2014. In this competition, two-dimensional video games similar to (and sometimes based on) 1980s-era arcade and console games are used instead of the board games used in the GGP competition. It has offered a way for researchers and practitioners to test and compare their best general video game playing algorithms. The competition has an associated software framework including a large number of games written in the Video Game Description Language (VGDL), which should not be confused with GDL and is a coding language using simple semantics and commands that can easily be parsed. One example for VGDL is PyVGDL developed in 2013.[6][24] The games used in GVGP are, for now, often 2-dimensional arcade games, as they are the simplest and easiest to quantify.[43] To simplify the process of creating an AI that can interpret video games, games for this purpose are written in VGDL manually.[clarification needed] VGDL can be used to describe a game specifically for procedural generation of levels, using Answer Set Programming (ASP) and an Evolutionary Algorithm (EA). GVGP can then be used to test the validity of procedural levels, as well as the difficulty or quality of levels based on how an agent performed.[44] Algorithms [edit]Since GGP AI must be designed to play multiple games, its design cannot rely on algorithms created specifically for certain games. Instead, the AI must be designed using algorithms whose methods can be applied to a wide range of games. Recent GGP systems such as Regular Boardgames (RBG) and Ludii have explored alternative rule representations to optimize reasoning efficiency and support a broader variety of games. The AI must also be an ongoing process, that can adapt to its current state rather than the output of previous states. For this reason, open loop techniques are often most effective.[45] A popular method for developing GGP AI is the Monte Carlo tree search (MCTS) algorithm.[46] Often used together with the UCT method (Upper Confidence Bound applied to Trees), variations of MCTS have been proposed to better play certain games, as well as to make it compatible with video game playing.[47][48][49] Another variation of tree-search algorithms used is the Directed Breadth-first Search (DBS),[50] in which a child node to the current state is created for each available action, and visits each child ordered by highest average reward, until either the game ends or runs out of time.[51] In each tree-search method, the AI simulates potential actions and ranks each based on the average highest reward of each path, in terms of points earned.[46][51] Assumptions [edit]In order to interact with games, algorithms must operate under the assumption that games all share common characteristics. In the book Half-Real: Video Games Between Real Worlds and Fictional Worlds, Jesper Juul gives the following definition of games: Games are based on rules, they have variable outcomes, different outcomes give different values, player effort influences outcomes, the player is attached to the outcomes, and the game has negotiable consequences.[52] Using these assumptions, game playing AI can be created by quantifying the player input, the game outcomes, and how the various rules apply, and using algorithms to compute the most favorable path.[43] See also [edit]- AlphaZero - Artificial general intelligence - Artificial intelligence in video games - Game Description Language - Multi-task learning - Outline of artificial intelligence - Transfer learning References [edit]- ^ Pell, Barney (1992). H. van den Herik; L. Allis (eds.). \"Metagame: a new challenge for games and learning\" [Heuristic programming in artificial intelligence 3–the third computerolympiad] (PDF). Ellis-Horwood. Archived (PDF) from the original on 2020-02-17. Retrieved 2020-02-17. - ^ Pell, Barney (1996). \"A Strategic Metagame Player for General Chess-Like Games\". Computational Intelligence. 12 (1): 177–198. doi:10.1111/j.1467-8640.1996.tb00258.x. ISSN 1467-8640. S2CID 996006. - ^ a b Genesereth, Michael; Love, Nathaniel; Pell, Barney (15 June 2005). \"General Game Playing: Overview of the AAAI Competition\". AI Magazine. 26 (2): 62. doi:10.1609/aimag.v26i2.1813. ISSN 2371-9621. - ^ Canaan, Rodrigo; Salge, Christoph; Togelius, Julian; Nealen, Andy (2019). Proceedings of the 14th International Conference on the Foundations of Digital Games [Proceedings of the 14th International Conference on the Leveling the playing field: fairness in AI versus human game benchmarks]. pp. 1–8. doi:10.1145/3337722. ISBN 9781450372176. S2CID 58599284. - ^ a b Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Graves, Alex; Antonoglou, Ioannis; Wierstra, Daan; Riedmiller, Martin (2013). \"Playing Atari with Deep Reinforcement Learning\" (PDF). Neural Information Processing Systems Workshop 2013. Archived (PDF) from the original on 12 September 2014. Retrieved 25 April 2015. - ^ a b Schaul, Tom (August 2013). \"A video game description language for model-based or interactive learning\". 2013 IEEE Conference on Computational Inteligence in Games (CIG). pp. 1–8. CiteSeerX 10.1.1.360.2263. doi:10.1109/CIG.2013.6633610. ISBN 978-1-4673-5311-3. S2CID 812565. - ^ a b Levine, John; Congdon, Clare Bates; Ebner, Marc; Kendall, Graham; Lucas, Simon M.; Miikkulainen, Risto; Schaul, Tom; Thompson, Tommy (2013). \"General Video Game Playing\". Artificial and Computational Intelligence in Games. 6. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik: 77–83. Archived from the original on 9 April 2016. Retrieved 25 April 2015. - ^ Bowling, M.; Veness, J.; Naddaf, Y.; Bellemare, M. G. (2013-06-14). \"The Arcade Learning Environment: An Evaluation Platform for General Agents\". Journal of Artificial Intelligence Research. 47: 253–279. arXiv:1207.4708. doi:10.1613/jair.3912. ISSN 1076-9757. S2CID 1552061. - ^ Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Rusu, Andrei A.; Veness, Joel; Hassabis, Demis; Bellemare, Marc G.; Graves, Alex; Riedmiller, Martin; Fidjeland, Andreas K.; Stig Petersen, Georg Ostrovski; Beattie, Charles; Sadik, Amir; Antonoglou, Ioannis; King, Helen; Kumaran, Dharshan; Wierstra, Daan; Legg, Shane (26 February 2015). \"Human-level control through deep reinforcement learning\". Nature. 518 (7540): 529–533. Bibcode:2015Natur.518..529M. doi:10.1038/nature14236. PMID 25719670. S2CID 205242740. - ^ Korjus, Kristjan; Kuzovkin, Ilya; Tampuu, Ardi; Pungas, Taivo (2014). \"Replicating the Paper \"Playing Atari with Deep Reinforcement Learning\"\" (PDF). University of Tartu. Archived (PDF) from the original on 18 December 2014. Retrieved 25 April 2015. - ^ Guo, Xiaoxiao; Singh, Satinder; Lee, Honglak; Lewis, Richard L.; Wang, Xiaoshi (2014). \"Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning\" (PDF). NIPS Proceedingsβ. Conference on Neural Information Processing Systems. Archived (PDF) from the original on 17 November 2015. Retrieved 25 April 2015. - ^ Murphy, Tom (2013). \"The First Level of Super Mario Bros. is Easy with Lexicographic Orderings and Time Travel ... after that it gets a little tricky.\" (PDF). SIGBOVIK. Archived (PDF) from the original on 26 April 2013. Retrieved 25 April 2015. - ^ Murphy, Tom. \"learnfun & playfun: A general technique for automating NES games\". Archived from the original on 19 April 2015. Retrieved 25 April 2015. - ^ Teller, Swizec (October 28, 2013). \"Week 2: Level 1 of Super Mario Bros. is easy with lexicographic orderings and\". A geek with a hat. Archived from the original on 30 April 2015. Retrieved 25 April 2015. - ^ McMillen, Colin (2003). Toward the Development of an Intelligent Agent for the Supply Chain Management Game of the 2003 Trading Agent Competition [2003 Trading Agent Competition] (Thesis). Master's Thesis. Minneapolis, MN: University of Minnesota. S2CID 167336006. - ^ Zhang, Dongmo (2009). From general game descriptions to a market specification language for general trading agents [Agent-mediated electronic commerce. Designing trading strategies and mechanisms for electronic markets.]. Berlin, Heidelberg: Springer. pp. 259–274. Bibcode:2010aecd.book..259T. CiteSeerX 10.1.1.467.4629. - ^ \"AGAPE - An Auction LanGuage for GenerAl Auction PlayErs\". AGAPE (in French). 8 March 2019. Archived from the original on 2 August 2021. Retrieved 5 March 2020. - ^ Michael, Friedrich; Ignatov, Dmitry (2019). \"General Game Playing B-to-B Price Negotiations\" (PDF). CEUR Workshop Proceedings. -2479: 89–99. Archived (PDF) from the original on 6 December 2019. Retrieved 5 March 2020. - ^ a b Barney Pell's research on computer game playing Archived 2007-08-12 at the Wayback Machine. - ^ \"Metagame and General Game Playing\". Metagame and General Game Playing. Archived from the original on 3 March 2001. Retrieved 27 March 2016. - ^ Available: Universal Game Engine Archived 2012-11-03 at the Wayback Machine email to comp.ai.games by Jeff Mallett, 10-Dec-1998. - ^ \"UZH - z-Tree - Zurich Toolbox for Readymade Economic Experiments\". www.ztree.uzh.ch. Archived from the original on 21 February 2016. Retrieved 17 February 2020. - ^ Beckenkamp, Martin; Hennig-Schmidt, Heike; Maier-Rigaud, Frank P. (1 March 2007). \"Cooperation in Symmetric and Asymmetric Prisoner's Dilemma Games\". Social Science Research Network. SSRN 968942. - ^ a b Schaul, Tom (7 February 2020). \"schaul/py-vgdl\". GitHub. Archived from the original on 11 June 2018. Retrieved 9 February 2020. - ^ GGP Server Archived 2014-02-21 at the Wayback Machine, platform for competition of general game playing systems. - ^ Dresden GGP Server Archived 2013-04-07 at the Wayback Machine, platform for competition of general game playing systems with automatic scheduling of matches. - ^ \"General Game Playing\". www.general-game-playing.de. Archived from the original on 2008-12-26. Retrieved 2008-08-21. - ^ Information about Fluxplayer Archived 2011-07-19 at the Wayback Machine, the winner of the 2nd International General Game Playing competition. - ^ Information about CADIAPlayer Archived 2011-07-22 at the Wayback Machine, more information about the winner of the 3rd, 4th, and 8th International General Game Playing competitions. - ^ Sancho is GGP Champion 2014! Archived 2015-12-22 at the Wayback Machine, winner of the 2014 International General Game Playing competition. - ^ Tagiew, Rustam (2009). Filipe, Joaquim; Fred, Ana; Sharp, Bernadette (eds.). Towards a framework for management of strategic interaction [Proceedings of the International Conference on Agents and Artificial Intelligence] (PDF). Porto, Portugal. pp. 587–590. ISBN 978-989-8111-66-1. Archived (PDF) from the original on 2021-03-09. Retrieved 2021-06-02. {{cite book}} : CS1 maint: location missing publisher (link) - ^ Tagiew, Rustam (2011). Strategische Interaktion realer Agenten Ganzheitliche Konzeptualisierung und Softwarekomponenten einer interdisziplinären Forschungsinfrastruktur (neue Ausg ed.). Saarbrücken. ISBN 9783838125121. {{cite book}} : CS1 maint: location missing publisher (link) - ^ \"Zillions of Games - Who Are We?\". www.zillions-of-games.com. Archived from the original on 2017-11-15. Retrieved 2017-11-16. - ^ \"AiAi Home Page – Stephen Tavener\". mrraow.com. Archived from the original on 2015-09-06. Retrieved 2017-11-16. - ^ \"Ai Ai announcement thread\". BoardGameGeek. Archived from the original on 2017-11-16. Retrieved 2017-11-16. - ^ \"The PolyGamo Player Project | Programming Languages and General Players for Abstract Games and Puzzles\". www.polyomino.com. Archived from the original on 2002-09-23. Retrieved 2017-11-16. - ^ Kowalski, Jakub; Mika, Maksymilian; Sutowicz, Jakub; Szykuła, Marek (2019-07-17). \"Regular Boardgames\". Proceedings of the AAAI Conference on Artificial Intelligence. 33 (1): 1699–1706. doi:10.1609/aaai.v33i01.33011699. ISSN 2374-3468. S2CID 20296467. - ^ Kowalski, Jakub; Miernik, Radoslaw; Mika, Maksymilian; Pawlik, Wojciech; Sutowicz, Jakub; Szykula, Marek; Tkaczyk, Andrzej (2020). \"Efficient Reasoning in Regular Boardgames\". 2020 IEEE Conference on Games (CoG). pp. 455–462. arXiv:2006.08295. doi:10.1109/cog47356.2020.9231668. ISBN 978-1-7281-4533-4. S2CID 219687404. - ^ \"Ludii Portal | Home of the Ludii General Game System\". www.ludii.games. Archived from the original on 2021-10-27. Retrieved 2021-10-27. - ^ \"Digital Ludeme Project | Modelling the Evolution of Traditional Games\". www.ludeme.eu. Archived from the original on 2021-10-02. Retrieved 2021-10-27. - ^ Piette, E.; Soemers, D. J. N. J.; Stephenson, M.; Sironi, C.; Stephenson, M.; Winands M. H. M.; Browne, C. (2020). \"Ludii – The Ludemic General Game System\" (PDF). European Conference on Artificial Intelligence (ECAI 2020), Santiago de Compestela. Archived (PDF) from the original on 2022-01-21. Retrieved 2021-10-27. - ^ Nielsen, Thorbjørn S.; Barros, Gabriella A. B.; Togelius, Julian; Nelson, Mark J. \"Towards generating arcade game rules with VGDL\" (PDF). Archived (PDF) from the original on 2015-09-12. Retrieved 2018-02-24. - ^ a b Levine, John; Congdon, Clare Bates; Ebner, Marc; Kendall, Graham; Lucas, Simon M.; Miikkulainen Risto, Schaul; Tom, Thompson; Tommy. \"General Video Game Playing\" (PDF). Archived (PDF) from the original on 2016-04-18. Retrieved 2016-04-09. - ^ Neufeld, Xenija; Mostaghim, Sanaz; Perez-Liebana, Diego. \"Procedural Level Generation with Answer Set Programming for General Video Game Playing\" (PDF). Archived (PDF) from the original on 2016-03-28. Retrieved 2018-02-24. - ^ Świechowski, Maciej; Park, Hyunsoo; Mańdziuk, Jacek; Kim, Kyung-Joong (2015). \"Recent Advances in General Game Playing\". The Scientific World Journal. 2015 986262. Hindawi Publishing Corporation. doi:10.1155/2015/986262. PMC 4561326. PMID 26380375. - ^ a b \"Monte-Carlo Tree Search for General Game Playing\". ResearchGate. Retrieved 2016-04-01. - ^ Finnsson, Hilmar (2012). \"Generalized Monte-Carlo Tree Search Extensions for General Game Playing\". Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence. Archived from the original on 2013-10-15. Retrieved 2016-04-09. - ^ Frydenberg, Frederik; Anderson, Kasper R.; Risi, Sebastian; Togelius, Julian. \"Investigating MCTS Modifications in General Video Game Playing\" (PDF). Archived (PDF) from the original on 2016-04-12. Retrieved 2016-04-09. - ^ M. Swiechowski; J. Mandziuk; Y. S. Ong, \"Specialization of a UCT-based General Game Playing Program to Single-Player Games,\" in IEEE Transactions on Computational Intelligence and AI in Games, vol.PP, no.99, pp.1-1 doi:10.1109/TCIAIG.2015.2391232 - ^ \"Changing the root node from a previous game step\". Archived from the original on 2021-01-17. DBS: A Directed Breadth First Search (DBS) algorithm - ^ a b Perez, Diego; Dieskau, Jens; Hünermund, Martin. \"Open Loop Search for General Video Game Playing\" (PDF). Archived (PDF) from the original on 2016-03-28. Retrieved 2016-04-09. - ^ Jesper Juul. Half-Real: Video Games Between Real Rules and Fictional Worlds. MIT Press, 2005. External links [edit]- General Game Playing Home Page at Stanford University - See also the GGP.org, GGP.org GitHub page, and games.stanford.edu. - General Game Playing Resources provided by Dresden University of Technology. - AiAi by Stephen Tavener - PolyGamo Player Project by David M. Bennett - Axiom Development kit a meta-game development system compatible with Zillions of Games, by Greg Schmidt. - Palamedes - A General Game Playing IDE - ConvNetJS Deep Q Learning Demo",
    "text_length": 22110,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:56.254650"
  },
  {
    "id": "page_7",
    "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
    "domain": "en.wikipedia.org",
    "title": "Knowledge representation and reasoning - Wikipedia",
    "text": "Knowledge representation and reasoning Knowledge representation (KR) aims to model information in a structured manner to formally represent it as knowledge in knowledge-based systems whereas knowledge representation and reasoning (KRR, KR&R, or KR²) also aims to understand, reason, and interpret knowledge. KRR is widely used in the field of artificial intelligence (AI) with the goal to represent information about the world in a form that a computer system can use to solve complex tasks, such as diagnosing a medical condition or having a natural-language dialog. KR incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that make complex systems easier to design and build. KRR also incorporates findings from logic to automate various kinds of reasoning. Traditional KRR focuses more on the declarative representation of knowledge. Related knowledge representation formalisms mainly include vocabularies, thesaurus, semantic networks, axiom systems, frames, rules, logic programs, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, model generators, and classifiers. In a broader sense, parameterized models in machine learning — including neural network architectures such as convolutional neural networks and transformers — can also be regarded as a family of knowledge representation formalisms. The question of which formalism is most appropriate for knowledge-based systems has long been a subject of extensive debate. For instance, Frank van Harmelen et al. discussed the suitability of logic as a knowledge representation formalism and reviewed arguments presented by anti-logicists.[2] Paul Smolensky criticized the limitations of symbolic formalisms and explored the possibilities of integrating it with connectionist approaches.[3] History [edit]| Part of a series on | | Artificial intelligence (AI) | |---| The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to implement common sense reasoning. Many of the early approaches to knowledge representation in Artificial Intelligence (AI) used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[4] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing. Other researchers focused on developing automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson. In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[5] In contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[6] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures. The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[7] These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[8] Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[9] Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[10] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations. It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[11] The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[12] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[13] Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[14] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[15] The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[16] Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet. Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[17][18] Overview [edit]Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems. The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems. For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical. Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[19] A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[20] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of knowledge representation languages. Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers. One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[21] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. A similar balancing act was also a motivation for the development of logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not. The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics,[22] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL. In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[23] - \"A knowledge representation (KR) is most fundamentally a surrogate, a substitute for the thing itself, used to enable an entity to determine consequences by thinking rather than acting,\" [23] i.e., \"by reasoning about the world rather than taking action in it.\"[23] - \"It is a set of ontological commitments\",[23] i.e., \"an answer to the question: In what terms should I think about the world?\" [23] - \"It is a fragmentary theory of intelligent reasoning, expressed in terms of three components: (i) the representation's fundamental conception of intelligent reasoning; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends.\"[23] - \"It is a medium for pragmatically efficient computation\",[23] i.e., \"the computational environment in which thinking is accomplished. One contribution to this pragmatic efficiency is supplied by the guidance a representation provides for organizing information\" [23] so as \"to facilitate making the recommended inferences.\"[23] - \"It is a medium of human expression\",[23] i.e., \"a language in which we say things about the world.\"[23] Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[17] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[24] The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[18] Characteristics [edit]In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:[25] - Primitives. What is the underlying framework used to represent knowledge? Semantic networks were one of the first knowledge representation primitives. Also, data structures and algorithms for general fast search. In this area, there is a strong overlap with research in data structures and algorithms in computer science. In early systems, the Lisp programming language, which was modeled after the lambda calculus, was often used as a form of functional knowledge representation. Frames and Rules were the next kind of primitive. Frame languages had various mechanisms for expressing and enforcing constraints on frame data. All data in frames are stored in slots. Slots are analogous to relations in entity-relation modeling and to object properties in object-oriented modeling. Another technique for primitives is to define languages that are modeled after First Order Logic (FOL). The most well known example is Prolog, but there are also many special-purpose theorem-proving environments. These environments can validate logical models and can deduce new theories from existing models. Essentially they automate the process a logician would go through in analyzing a model. Theorem-proving technology had some specific practical applications in the areas of software engineering. For example, it is possible to prove that a software program rigidly adheres to a formal logical specification. - Meta-representation. This is also known as the issue of reflection in computer science. It refers to the ability of a formalism to have access to information about its own state. An example is the meta-object protocol in Smalltalk and CLOS that gives developers runtime access to the class objects and enables them to dynamically redefine the structure of the knowledge base even at runtime. Meta-representation means the knowledge representation language is itself expressed in that language. For example, in most Frame based environments all frames would be instances of a frame class. That class object can be inspected at runtime, so that the object can understand and even change its internal structure or the structure of other parts of the model. In rule-based environments, the rules were also usually instances of rule classes. Part of the meta protocol for rules were the meta rules that prioritized rule firing. - Incompleteness. Traditional logic requires additional axioms and constraints to deal with the real world as opposed to the world of mathematics. Also, it is often useful to associate degrees of confidence with a statement, i.e., not simply say \"Socrates is Human\" but rather \"Socrates is Human with confidence 50%\". This was one of the early innovations from expert systems research which migrated to some commercial tools, the ability to associate certainty factors with rules and conclusions. Later research in this area is known as fuzzy logic.[26] - Definitions and universals vs. facts and defaults. Universals are general statements about the world such as \"All humans are mortal\". Facts are specific examples of universals such as \"Socrates is a human and therefore mortal\". In logical terms definitions and universals are about universal quantification while facts and defaults are about existential quantifications. All forms of knowledge representation must deal with this aspect and most do so with some variant of set theory, modeling universals as sets and subsets and definitions as elements in those sets. - Non-monotonic reasoning. Non-monotonic reasoning allows various kinds of hypothetical reasoning. The system associates facts asserted with the rules and facts used to justify them and as those facts change updates the dependent knowledge as well. In rule based systems this capability is known as a truth maintenance system.[27] - Expressive adequacy. The standard that Brachman and most AI researchers use to measure expressive adequacy is usually First Order Logic (FOL). Theoretical limitations mean that a full implementation of FOL is not practical. Researchers should be clear about how expressive (how much of full FOL expressive power) they intend their representation to be.[28] - Reasoning efficiency. This refers to the runtime efficiency of a system: The ability of the knowledge base to be updated and the reasoner to develop new inferences in a reasonable time. In some ways, this is the flip side of expressive adequacy. In general, the more powerful a representation, the more it has expressive adequacy, the less efficient its automated reasoning engine will be. Efficiency was often an issue, especially for early applications of knowledge representation technology. They were usually implemented in interpreted environments such as Lisp, which were slow compared to more traditional platforms of the time. Ontology engineering [edit]In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases. As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL. After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, \"Every ontology is a treaty–a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[29] There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[30] the lumped element model widely used in representing electronic circuits (e.g.[31]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world. The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows. Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs. The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand. See also [edit]- Alphabet of human thought – Hypothetical language created by Gottfried Wilhelm Leibniz - Belief revision – Process of changing beliefs to take into account a new piece of information - Chunking (psychology) – Cognitive psychology process - Commonsense knowledge base – Facts assumed to be known to all humans - Conceptual graph – Formalism for knowledge representation - DIKW pyramid – Data, information, knowledge, wisdom hierarchy - DATR, a language for lexical knowledge representation - FO(.), a KR language based on first-order logic - Knowledge graph – Type of knowledge base - Knowledge management – Processing of knowledge to accomplish organizational goals - Logic programming – Programming paradigm based on formal logic - Logico-linguistic modeling - Mind map – Diagram to visually organize information - Semantic technology – Technology to help machines understand data - Valuation-based system References [edit]- ^ Schank, Roger; Abelson, Robert (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Lawrence Erlbaum Associates, Inc. - ^ Porter, Bruce; Lifschitz, Vladimir; Van Harmelen, Frank (2008). Handbook of knowledge representation. Foundations of artificial intelligence (1st ed.). Amsterdam Boston: Elsevier. ISBN 978-0-444-52211-5. - ^ Smolensky, Paul (March 1988). \"On the proper treatment of connectionism\". Behavioral and Brain Sciences. 11 (1): 1–23. doi:10.1017/S0140525X00052432. ISSN 0140-525X. - ^ Doran, J. E.; Michie, D. (1966-09-20). \"Experiments with the Graph Traverser program\". Proc. R. Soc. Lond. A. 294 (1437): 235–259. Bibcode:1966RSPSA.294..235D. doi:10.1098/rspa.1966.0205. S2CID 21698093. - ^ Green, Cordell. Application of Theorem Proving to Problem Solving (PDF). IJCAI 1969. - ^ Hewitt, C., 2009. Inconsistency robustness in logic programs. arXiv preprint arXiv:0904.3036. - ^ Kowalski, Robert (1986). \"The limitation of logic\". Proceedings of the 1986 ACM fourteenth annual conference on Computer science - CSC '86. pp. 7–13. doi:10.1145/324634.325168. ISBN 0-89791-177-6. S2CID 17211581. - ^ Nilsson, Nils (1995). \"Eye on the Prize\". AI Magazine. 16: 2. - ^ Hayes-Roth, Frederick; Waterman, Donald; Lenat, Douglas (1983). Building Expert Systems. Addison-Wesley. ISBN 978-0-201-10686-2. - ^ Marvin Minsky, A Framework for Representing Knowledge Archived 2021-01-07 at the Wayback Machine, MIT-AI Laboratory Memo 306, June, 1974 - ^ Mettrey, William (1987). \"An Assessment of Tools for Building Large Knowledge-Based Systems\". AI Magazine. 8 (4). Archived from the original on 2013-11-10. Retrieved 2013-12-24. - ^ Brachman, Ron (1978). \"A Structural Paradigm for Representing Knowledge\" (PDF). Bolt, Beranek, and Neumann Technical Report (3605). Archived (PDF) from the original on April 30, 2020. - ^ MacGregor, Robert (June 1991). \"Using a description classifier to enhance knowledge representation\". IEEE Expert. 6 (3): 41–46. Bibcode:1991IExp....6...41M. doi:10.1109/64.87683. S2CID 29575443. - ^ McCarthy, J., and Hayes, P. J. 1969. \"Some philosophical problems from the standpoint of artificial intelligence\" (PDF). Archived from the original on August 25, 2013. Retrieved January 18, 2024. {{cite web}} : CS1 maint: bot: original URL status unknown (link). In Meltzer, B., and Michie, D., eds., Machine Intelligence 4. Edinburgh: Edinburgh University Press. 463–502. - ^ Lenat, Doug; R. V. Guha (January 1990). Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project. Addison-Wesley. ISBN 978-0201517521. - ^ Smith, Brian C. (1985). \"Prologue to Reflections and Semantics in a Procedural Language\". In Ronald Brachman and Hector J. Levesque (ed.). Readings in Knowledge Representation. Morgan Kaufmann. pp. 31–40. ISBN 978-0-934613-01-9. - ^ a b Berners-Lee, Tim; Hendler, James; Lassila, Ora (May 17, 2001). \"The Semantic Web – A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities\". Scientific American. 284 (5): 34–43. doi:10.1038/scientificamerican0501-34. Archived from the original on April 24, 2013. - ^ a b Knublauch, Holger; Oberle, Daniel; Tetlow, Phil; Wallace, Evan (2006-03-09). \"A Semantic Web Primer for Object-Oriented Software Developers\". W3C. Archived from the original on 2018-01-06. Retrieved 2008-07-30. - ^ Hayes-Roth, Frederick; Waterman, Donald; Lenat, Douglas (1983). Building Expert Systems. Addison-Wesley. pp. 6–7. ISBN 978-0-201-10686-2. - ^ Levesque, H.J. and Brachman, R.J., 1987. Expressiveness and tractability in knowledge representation and reasoning 1. Computational intelligence, 3(1), pp.78-93. - ^ Levesque, Hector; Brachman, Ronald (1985). \"A Fundamental Tradeoff in Knowledge Representation and Reasoning\". In Ronald Brachman and Hector J. Levesque (ed.). Readings in Knowledge Representation. Morgan Kaufmann. p. 49. ISBN 978-0-934613-01-9. The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable. - ^ Russell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. p. 282. ISBN 978-0134610993. LCCN 20190474. - ^ a b c d e f g h i j k Davis, Randall; Shrobe, Howard; Szolovits, Peter (Spring 1993). \"What Is a Knowledge Representation?\". AI Magazine. 14 (1): 17–33. Archived from the original on 2012-04-06. Retrieved 2011-03-23. - ^ Macgregor, Robert (August 13, 1999). \"Retrospective on Loom\". isi.edu. Information Sciences Institute. Archived from the original on 25 October 2013. Retrieved 10 December 2013. - ^ Brachman, Ron (1985). \"Introduction\". In Brachman, Ronald; Levesque, Hector J. (eds.). Readings in Knowledge Representation. Morgan Kaufmann. pp. XVI–XVII. ISBN 978-0-934613-01-9. - ^ Bih, Joseph (2006). \"Paradigm Shift: An Introduction to Fuzzy Logic\" (PDF). IEEE Potentials. 25 (1): 6–21. Bibcode:2006IPot...25a...6B. doi:10.1109/MP.2006.1635021. S2CID 15451765. Archived (PDF) from the original on 12 June 2014. Retrieved 24 December 2013. - ^ Zlatarva, Nellie (1992). \"Truth Maintenance Systems and their Application for Verifying Expert System Knowledge Bases\". Artificial Intelligence Review. 6: 67–110. doi:10.1007/bf00155580. S2CID 24696160. - ^ Levesque, Hector; Brachman, Ronald (1985). \"A Fundamental Tradeoff in Knowledge Representation and Reasoning\". In Ronald Brachman and Hector J. Levesque (ed.). Readings in Knowledge Representation. Morgan Kaufmann. pp. 41–70. ISBN 978-0-934613-01-9. - ^ Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (3rd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, p. 437-439 - ^ Hayes P, Naive physics I: Ontology for liquids. University of Essex report, 1978, Essex, UK. - ^ Davis R, Shrobe H E, Representing Structure and Behavior of Digital Hardware, IEEE Computer, Special Issue on Knowledge Representation, 16(10):75-82. Further reading [edit]- Ronald J. Brachman; What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks; IEEE Computer, 16 (10); October 1983 - Ronald J. Brachman, Hector J. Levesque Knowledge Representation and Reasoning, Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7 - Ronald J. Brachman, Hector J. Levesque (eds) Readings in Knowledge Representation, Morgan Kaufmann, 1985, ISBN 0-934613-01-X - Chein, M., Mugnier, M.-L. (2009),Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs, Springer, 2009,ISBN 978-1-84800-285-2. - Randall Davis, Howard Shrobe, and Peter Szolovits; What Is a Knowledge Representation? AI Magazine, 14(1):17-33,1993 - Ronald Fagin, Joseph Y. Halpern, Yoram Moses, Moshe Y. Vardi Reasoning About Knowledge, MIT Press, 1995, ISBN 0-262-06162-7 - Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: Understanding Implementations of IS-A Relations. ER 1996: 42-57 - Hermann Helbig: Knowledge Representation and the Semantics of Natural Language, Springer, Berlin, Heidelberg, New York 2006 - Frank van Harmelen, Vladimir Lifschitz and Bruce Porter: Handbook of Knowledge Representation 2007. - Arthur B. Markman: Knowledge Representation Lawrence Erlbaum Associates, 1998 - John F. Sowa: Knowledge Representation: Logical, Philosophical, and Computational Foundations. Brooks/Cole: New York, 2000 - Adrian Walker, Michael McCord, John F. Sowa, and Walter G. Wilson: Knowledge Systems and Prolog, Second Edition, Addison-Wesley, 1990 - Mary-Anne Williams and Hans Rott: \"Frontiers in Belief Revision, Kluwer\", 2001. External links [edit]- What is a Knowledge Representation? by Randall Davis and others - Introduction to Knowledge Modeling by Pejman Makhfi - Introduction to Description Logics course by Enrico Franconi, Faculty of Computer Science, Free University of Bolzano, Italy - DATR Lexical knowledge representation language - Loom Project Home Page - Principles of Knowledge Representation and Reasoning Incorporated - Description Logic in Practice: A CLASSIC Application - The Rule Markup Initiative - Nelements KOS - a non-free 3d knowledge representation system",
    "text_length": 34950,
    "depth": 1,
    "crawled_at": "2026-01-09T19:30:58.753713"
  },
  {
    "id": "page_8",
    "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "domain": "en.wikipedia.org",
    "title": "Natural language processing - Wikipedia",
    "text": "Natural language processing This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these messages) | Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly.[1] Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. History [edit]Natural language processing has its roots in the 1950s.[2] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s) [edit]The premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. - 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[3] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[4]) until the late 1980s when the first statistical machine translation systems were developed. - 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[5] - 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). - 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[6]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[7]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[8] Statistical NLP (1990s–present) [edit]Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[9] - 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. - 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. - 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)[10] - 2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modeling,[11] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This shift gained momentum due to results showing that such techniques[12][13] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[14] and parsing.[15][16] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[17] or protect patient privacy.[18] Approaches: Symbolic, statistical, neural networks [edit]Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[19][20] such as by writing grammars or devising heuristic rules for stemming. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: - both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally. - language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce. - the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems. Rule-based systems are commonly used: - when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system, - for preprocessing in NLP pipelines, e.g., tokenization, or - for post-processing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses. Statistical approach [edit]In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[21][22] The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches. Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. Neural networks [edit]A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[23] neural network–based methods have increasingly replaced traditional statistical approaches, using semantic networks[24] and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. Common NLP tasks [edit]The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Text and speech processing [edit]- Optical character recognition (OCR) - Given an image representing printed text, determine the corresponding text. - Speech recognition - Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above). In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent. - Speech segmentation - Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition and typically grouped with it. - Text-to-speech - Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.[25] - Word segmentation (Tokenization) - Tokenization is a text-processing technique that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.[26] - For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.[27] Morphological analysis [edit]- Lemmatization - The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.[28] - Morphological segmentation - Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.[29] - Part-of-speech tagging - Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech. - Stemming - The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary. Syntactic analysis [edit]| Part of a series on | | Formal languages | |---| - Grammar induction[30] - Generate a formal grammar that describes a language's syntax. - Sentence breaking (also known as \"sentence boundary disambiguation\") - Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations). - Parsing - Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar). Lexical semantics (of individual words in context) [edit]- Lexical semantics - What is the computational meaning of individual words in context? - Distributional semantics - How can we learn semantic representations from data? - Named entity recognition (NER) - Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. This task is also referred to as token classification.[31] - Sentiment analysis (see also Multimodal sentiment analysis) - Sentiment analysis involves identifying and classifying the emotional tone expressed in text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.[26] - Terminology extraction - The goal of terminology extraction is to automatically extract relevant terms from a given corpus. - Word-sense disambiguation (WSD) - Many words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. - Entity linking - Many words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. Relational semantics (semantics of individual sentences) [edit]- Relationship extraction - Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom). - Semantic parsing - Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below). - Semantic role labelling (see also implicit semantic role labelling below) - Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles). Discourse (semantics beyond individual sentences) [edit]- Coreference resolution - Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to). - Discourse analysis - This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.). - Implicit semantic role labelling - Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages. - Recognizing textual entailment - Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.[32] - Topic segmentation and recognition - Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment. - Argument mining - The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs.[33] Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.[34][35] Higher-level NLP applications [edit]- Automatic summarization (text summarization) - Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper. - Grammatical error correction - Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011.[36][37][38] As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications. - Logic translation - Translate a text from a natural language into formal logic. - Machine translation (MT) - Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. - Natural language understanding (NLU) - Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.[39] - Natural language generation (NLG): - Convert information from computer databases or semantic intents into readable human language. - Book generation - Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed).[40] The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[41] Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. - Document AI - A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.[42] - Dialogue management - Computer systems intended to converse with a human. - Question answering - Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). - Text-to-image generation - Given a description of an image, generate an image that matches the description.[43] - Text-to-scene generation - Given a description of a scene, generate a 3D model of the scene.[44][45] - Text-to-video - Given a description of a video, generate a video that matches the description.[46][47] General tendencies and (possible) future directions [edit]Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[48] - Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing). - Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages) - Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems) Cognition [edit]Most higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[49] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[50] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[51] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[52] with two defining aspects: - Apply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author.[53] For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. - Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353:[54] - Where - RMM is the relative measure of meaning - token is any block of text, sentence, phrase or word - N is the number of tokens being analyzed - PMM is the probable measure of meaning based on a corpora - d is the non zero location of the token along the sequence of N tokens - PF is the probability function specific to a language - Where Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[55] functional grammar,[56] construction grammar,[57] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[58] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[59] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[60] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[61] and new directions in artificial general intelligence based on the free energy principle[62] by British neuroscientist and theoretician at University College London Karl J. Friston. See also [edit]- 1 the Road - Artificial intelligence detection software - Automated essay scoring - Biomedical text mining - Compound term processing - Computational linguistics - Computer-assisted reviewing - Controlled natural language - Deep learning - Deep linguistic processing - Distributional semantics - Foreign language reading aid - Foreign language writing aid - Information extraction - Information retrieval - Language and Communication Technologies - Language model - Language technology - Latent semantic indexing - Multi-agent system - Native-language identification - Natural-language programming - Natural-language understanding - Natural-language search - Outline of natural language processing - Query expansion - Query understanding - Reification (linguistics) - Speech processing - Spoken dialogue systems - Text-proofing - Text simplification - Transformer (machine learning model) - Truecasing - Question answering - Word2vec References [edit]- ^ Eisenstein, Jacob (October 1, 2019). Introduction to Natural Language Processing. The MIT Press. p. 1. ISBN 978-0-262-04284-0. - ^ \"NLP\". - ^ Hutchins, J. (2005). \"The history of machine translation in a nutshell\" (PDF). Archived from the original (PDF) on 2019-07-13. Retrieved 2019-02-04.[self-published source] - ^ \"ALPAC: the (in)famous report\", John Hutchins, MT News International, no. 14, June 1996, pp. 9–12. - ^ Crevier 1993, pp. 146–148 , see also Buchanan 2005, p. 56 : \"Early programs were necessarily limited in scope by the size and speed of memory\" - ^ Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form recognition and production (PDF), Department of General Linguistics, University of Helsinki, archived from the original (PDF) on 2018-12-21, retrieved 2020-08-20 - ^ Joshi, A. K., & Weinstein, S. (1981, August). Control of Inference: Role of Some Aspects of Discourse Structure-Centering. In IJCAI (pp. 385–387). - ^ Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and approaches\". Proceedings of the IEEE. 74 (7): 1026–1035. doi:10.1109/PROC.1986.13580. ISSN 1558-2256. S2CID 30688575. - ^ Chomskyan linguistics encourages the investigation of \"corner cases\" that stress the limits of its theoretical models (comparable to pathological phenomena in mathematics), typically created using thought experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as is the case in corpus linguistics. The creation and use of such corpora of real-world data is a fundamental part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \"poverty of the stimulus\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing. As a result, the Chomskyan paradigm discouraged the application of such models to language processing. - ^ Bengio, Yoshua; Ducharme, Réjean; Vincent, Pascal; Janvin, Christian (March 1, 2003). \"A neural probabilistic language model\". The Journal of Machine Learning Research. 3: 1137–1155 – via ACM Digital Library. - ^ Mikolov, Tomáš; Karafiát, Martin; Burget, Lukáš; Černocký, Jan; Khudanpur, Sanjeev (26 September 2010). \"Recurrent neural network based language model\" (PDF). Interspeech 2010. pp. 1045–1048. doi:10.21437/Interspeech.2010-343. S2CID 17048224. {{cite book}} :|journal= ignored (help) - ^ Goldberg, Yoav (2016). \"A Primer on Neural Network Models for Natural Language Processing\". Journal of Artificial Intelligence Research. 57: 345–420. arXiv:1807.10854. doi:10.1613/jair.4992. S2CID 8273530. - ^ Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning. MIT Press. - ^ Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). Exploring the Limits of Language Modeling. arXiv:1602.02410. Bibcode:2016arXiv160202410J. - ^ Choe, Do Kook; Charniak, Eugene. \"Parsing as Language Modeling\". Emnlp 2016. Archived from the original on 2018-10-23. Retrieved 2018-10-22. - ^ Vinyals, Oriol; et al. (2014). \"Grammar as a Foreign Language\" (PDF). Nips2015. arXiv:1412.7449. Bibcode:2014arXiv1412.7449V. - ^ Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19). \"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\". Journal of Diabetes Science and Technology. 15 (3): 553–560. doi:10.1177/19322968211000831. ISSN 1932-2968. PMC 8120048. PMID 33736486. - ^ Lee, Jennifer; Yang, Samuel; Holland-Hall, Cynthia; Sezgin, Emre; Gill, Manjot; Linwood, Simon; Huang, Yungui; Hoffman, Jeffrey (2022-06-10). \"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\". JMIR Medical Informatics. 10 (6) e38482. doi:10.2196/38482. ISSN 2291-9694. PMC 9233261. PMID 35687381. - ^ Winograd, Terry (1971). Procedures as a Representation for Data in a Computer Program for Understanding Natural Language (Thesis). - ^ Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum. ISBN 0-470-99033-3. - ^ Mark Johnson. How the statistical revolution changes (computational) linguistics. Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics. - ^ Philip Resnik. Four revolutions. Language Log, February 5, 2011. - ^ Socher, Richard. \"Deep Learning For NLP-ACL 2012 Tutorial\". www.socher.org. Archived from the original on 2021-04-14. Retrieved 2020-08-17. This was an early Deep Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP. [Link is broken, try http://web.stanford.edu/class/cs224n/] - ^ Segev, Elad (2022). Semantic Network Analysis in Social Sciences. London: Routledge. ISBN 978-0-367-63652-4. Archived from the original on 5 December 2021. Retrieved 5 December 2021. - ^ Yi, Chucai; Tian, Yingli (2012), \"Assistive Text Reading from Complex Background for Blind Persons\", Camera-Based Document Analysis and Recognition, Lecture Notes in Computer Science, vol. 7139, Springer Berlin Heidelberg, pp. 15–28, CiteSeerX 10.1.1.668.869, doi:10.1007/978-3-642-29364-1_2, ISBN 978-3-642-29363-4 - ^ a b \"Natural Language Processing (NLP) - A Complete Guide\". www.deeplearning.ai. 2023-01-11. Retrieved 2024-05-05. - ^ \"GeeksforGeeks. (n.d.). Tokenization in natural language processing (NLP). GeeksforGeeks\". geeksforgeeks. - ^ \"What is Natural Language Processing? Intro to NLP in Machine Learning\". GyanSetu!. 2020-12-06. Retrieved 2021-01-09. - ^ Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012). \"Manipuri Morpheme Identification\" (PDF). Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012: 95–108. {{cite journal}} : CS1 maint: location (link) - ^ Klein, Dan; Manning, Christopher D. (2002). \"Natural language grammar induction using a constituent-context model\" (PDF). Advances in Neural Information Processing Systems. - ^ Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). \"Precision information extraction for rare disease epidemiology at scale\". Journal of Translational Medicine. 21 (1): 157. doi:10.1186/s12967-023-04011-y. PMC 9972634. PMID 36855134. - ^ PASCAL Recognizing Textual Entailment Challenge (RTE-7) https://tac.nist.gov//2011/RTE/ - ^ Lippi, Marco; Torroni, Paolo (2016-04-20). \"Argumentation Mining: State of the Art and Emerging Trends\". ACM Transactions on Internet Technology. 16 (2): 1–25. doi:10.1145/2850417. hdl:11585/523460. ISSN 1533-5399. S2CID 9561587. - ^ \"Argument Mining – IJCAI2016 Tutorial\". www.i3s.unice.fr. Archived from the original on 2021-04-18. Retrieved 2021-03-09. - ^ \"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\". Retrieved 2021-03-09. - ^ Administration. \"Centre for Language Technology (CLT)\". Macquarie University. Retrieved 2021-01-11. - ^ \"Shared Task: Grammatical Error Correction\". www.comp.nus.edu.sg. Retrieved 2021-01-11. - ^ \"Shared Task: Grammatical Error Correction\". www.comp.nus.edu.sg. Retrieved 2021-01-11. - ^ Duan, Yucong; Cruz, Christophe (2011). \"Formalizing Semantic of Natural Language through Conceptualization from Existence\". International Journal of Innovation, Management and Technology. 2 (1): 37–42. Archived from the original on 2011-10-09. - ^ \"U B U W E B :: Racter\". www.ubu.com. Retrieved 2020-08-17. - ^ Writer, Beta (2019). Lithium-Ion Batteries. doi:10.1007/978-3-030-16800-1. ISBN 978-3-030-16799-8. S2CID 155818532. - ^ \"Document Understanding AI on Google Cloud (Cloud Next '19) – YouTube\". www.youtube.com. 11 April 2019. Archived from the original on 2021-10-30. Retrieved 2021-01-11. - ^ Robertson, Adi (2022-04-06). \"OpenAI's DALL-E AI image generator can now edit pictures, too\". The Verge. Retrieved 2022-06-07. - ^ \"The Stanford Natural Language Processing Group\". nlp.stanford.edu. Retrieved 2022-06-07. - ^ Coyne, Bob; Sproat, Richard (2001-08-01). \"WordsEye\". Proceedings of the 28th annual conference on Computer graphics and interactive techniques. SIGGRAPH '01. New York, NY, USA: Association for Computing Machinery. pp. 487–496. doi:10.1145/383259.383316. ISBN 978-1-58113-374-5. S2CID 3842372. - ^ \"Google announces AI advances in text-to-video, language translation, more\". VentureBeat. 2022-11-02. Retrieved 2022-11-09. - ^ Vincent, James (2022-09-29). \"Meta's new text-to-video AI generator is like DALL-E for video\". The Verge. Retrieved 2022-11-09. - ^ \"Previous shared tasks | CoNLL\". www.conll.org. Retrieved 2021-01-11. - ^ \"Cognition\". Lexico. Oxford University Press and Dictionary.com. Archived from the original on July 15, 2020. Retrieved 6 May 2020. - ^ \"Ask the Cognitive Scientist\". American Federation of Teachers. 8 August 2014. Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind. - ^ Robinson, Peter (2008). Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp. 3–8. ISBN 978-0-805-85352-0. - ^ Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp. 569–583. ISBN 978-0-465-05674-3. - ^ Strauss, Claudia (1999). A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp. 156–164. ISBN 978-0-521-59541-4. - ^ US patent 9269353 - ^ \"Universal Conceptual Cognitive Annotation (UCCA)\". Universal Conceptual Cognitive Annotation (UCCA). Retrieved 2021-01-11. - ^ Rodríguez, F. C., & Mairal-Usón, R. (2016). Building an RRG computational grammar. Onomazein, (34), 86–117. - ^ \"Fluid Construction Grammar – A fully operational processing system for construction grammars\". Retrieved 2021-01-11. - ^ \"ACL Member Portal | The Association for Computational Linguistics Member Portal\". www.aclweb.org. Retrieved 2021-01-11. - ^ \"Chunks and Rules\". W3C. Retrieved 2021-01-11. - ^ Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014). \"Grounded Compositional Semantics for Finding and Describing Images with Sentences\". Transactions of the Association for Computational Linguistics. 2: 207–218. doi:10.1162/tacl_a_00177. S2CID 2317858. - ^ Dasgupta, Ishita; Lampinen, Andrew K.; Chan, Stephanie C. Y.; Creswell, Antonia; Kumaran, Dharshan; McClelland, James L.; Hill, Felix (2022). \"Language models show human-like content effects on reasoning, Dasgupta, Lampinen et al\". arXiv:2207.07051 [cs.CL]. - ^ Friston, Karl J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The Generative Models of Active Inference. The MIT Press. ISBN 978-0-262-36997-8. Further reading [edit]- Bates, M (1995). \"Models of natural language understanding\". Proceedings of the National Academy of Sciences of the United States of America. 92 (22): 9977–9982. Bibcode:1995PNAS...92.9977B. doi:10.1073/pnas.92.22.9977. PMC 40721. PMID 7479812. - Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. ISBN 978-0-596-51649-9. - Kenna Hughes-Castleberry, \"A Murder Mystery Puzzle: The literary puzzle Cain's Jawbone, which has stumped humans for decades, reveals the limitations of natural-language-processing algorithms\", Scientific American, vol. 329, no. 4 (November 2023), pp. 81–82. \"This murder mystery competition has revealed that although NLP (natural-language processing) models are capable of incredible feats, their abilities are very much limited by the amount of context they receive. This [...] could cause [difficulties] for researchers who hope to use them to do things such as analyze ancient languages. In some cases, there are few historical records on long-gone civilizations to serve as training data for such a purpose.\" (p. 82.) - Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6. - Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley. ISBN 978-1848218482. - Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley. ISBN 978-1848219212. - Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5. Official html and pdf versions available without charge. - Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. ISBN 978-0-262-13360-9. - David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-Verlag. ISBN 978-0-387-19557-5. External links [edit]- Media related to Natural language processing at Wikimedia Commons",
    "text_length": 45119,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:01.348427"
  },
  {
    "id": "page_9",
    "url": "https://en.wikipedia.org/wiki/Robotics",
    "domain": "en.wikipedia.org",
    "title": "Robotics - Wikipedia",
    "text": "Robotics This article may relate to a different subject or has undue weight on an aspect of the subject. Specifically, the article goes in too much detail on specific types of robot and includes product placement. (August 2024) | Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1] Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes. A roboticist is someone who specializes in robotics. Robotics aspects [edit]Robotics usually combines three aspects of design work to create robot systems: - Mechanical construction: a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud might use caterpillar tracks. Origami inspired robots can sense and analyze in extreme environments.[2] The mechanical aspect of the robot is mostly the creator's solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function. - Electrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol-powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol-powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status), and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations) - Software. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not be able to go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly structured, its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence, and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. A hybrid is a form of programming that incorporates both AI and RC functions in them.[3] Applied robotics [edit]As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4] Current and potential applications include: - Manufacturing. Robots have been increasingly used in manufacturing since the 1960s. According to the Robotic Industries Association US data, in 2016 the automotive industry was the main customer of industrial robots with 52% of total sales.[5] In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.[6] - Autonomous transport including airplane autopilot and self-driving cars - Domestic robots including robotic vacuum cleaners, robotic lawn mowers, dishwasher loading[7] and flatbread baking.[8] - Construction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.[9] - Automated mining. - Space exploration, including Mars rovers. - Energy applications including cleanup of nuclear contaminated areas;[a] and cleaning solar panel arrays. - Medical robots and Robot-assisted surgery designed and used in clinics.[11] - Agricultural robots.[12] The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage.[13] - Food processing. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts), Sally (salads),[14] salad or food bowl robots manufactured by Dexai (a Draper Laboratory spinoff, operating on military bases), and integrated food bowl assembly systems manufactured by Spyce Kitchen (acquired by Sweetgreen) and Silicon Valley startup Hyphen.[15] Other examples may include manufacturing technologies based on 3D Food Printing. - Military robots. - Robot sports for entertainment and education, including Robot combat, Autonomous racing, drone racing, and FIRST Robotics. Mechanical robotics areas [edit]Power source [edit]At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] Potential power sources could be: - pneumatic (compressed gases) - Solar power (using the sun's energy and converting it into electrical power) - hydraulics (liquids) - flywheel energy storage - organic garbage (through anaerobic digestion) - nuclear Actuation [edit]Actuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air. Electric motors [edit]The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational. Linear actuators [edit]Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car. Series elastic actuators [edit]Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21] The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains. Air muscles [edit]Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27] Wire muscles [edit]Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29] Electroactive polymers [edit]EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32] Piezo motors [edit]Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36] Elastic nanotubes [edit]Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37] Sensing [edit]Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing. Touch [edit]Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects. Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40] Other [edit]Other common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water. Mechanical grippers [edit]One of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction. Suction end-effectors [edit]Suction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction. Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors. Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface. General purpose effectors [edit]Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful Robot Dexterity Intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49] Control robotics areas [edit]The mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions. The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52] At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc. Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55] Manipulation [edit]A definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[56] Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59] Locomotion [edit]Rolling robots [edit]For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to. Two-wheeled balancing robots [edit]Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[62] One-wheeled balancing robots [edit]A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64] Spherical orb robots [edit]Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71] Six-wheeled robots [edit]Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass. Tracked robots [edit]Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[72] Walking robots [edit]Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are: ZMP technique [edit]The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on. Hopping [edit]Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalized to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83] Dynamic balancing (controlled falling) [edit]A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame. Passive dynamics [edit]Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88] Flying [edit]A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar. Biomimetic flying robots (BFRs) [edit]BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller-actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller-actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments. Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91] Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96] Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments. Biologically-inspired flying robots [edit]A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs. Snaking [edit]Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102] Skating [edit]A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104] Climbing [edit]Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107] China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41] Swimming (Piscine) [edit]It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively. In 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115] Sailing [edit]Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot. Computational robotics areas [edit]Control systems may also have varying levels of autonomy. - Direct interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot's motion. - Operator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.[118] - An autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern. Another classification takes into account the interaction between human control and the machine motions. - Teleoperation. A human controls each movement, each machine actuator change is specified by the operator. - Supervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators. - Task-level autonomy. The operator specifies only the task and the robot manages itself to complete it. - Full autonomy. The machine will create and complete all its tasks without human interaction. Vision [edit]Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. Environmental interaction and navigation [edit]Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints. Human-robot interaction [edit]The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121] Speech recognition [edit]Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[126] Robotic voice [edit]Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132] Facial expression [edit]Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135] Gestures [edit]One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137] Proxemics [edit]Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions. Artificial emotions [edit]Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138] Personality [edit]Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142] Research robotics [edit]Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143] Dynamics and kinematics [edit]| External videos | | |---|---| | How the BB-8 Sphero Toy Works | The study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot. In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented. Open source robotics [edit]Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions. Evolutionary robotics [edit]Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] According to the International Federation of Robotics (IFR) study World Robotics 2023, there were about 4,281,585 operational industrial robots by the end of 2023[149] Bionics and biomimetics [edit]Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump. Swarm robotics [edit]Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [119] Quantum computing [edit]There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[150] Other research areas [edit]- Nanorobots. - Cobots (collaborative robots).[151] - Autonomous drones. - High temperature crucibles allow robotic systems to automate sample analysis.[152] The main venues for robotics research are the international conferences ICRA and IROS. Human factors [edit]Education and training [edit]Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[153] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[154] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. Employment [edit]Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[155] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[156] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[157] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[158] The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[159] Occupational safety and health implications [edit]A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[160] The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[161] Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organizes annual workshops on the topic \"human-robot collaboration\". In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[162][163] aiming to protect employees from the risk of working with collaborative robots will have to be revised. User experience [edit]Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[164] It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[165] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors. Careers [edit]Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees. Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole. Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills. History [edit]| Date | Significance | Robot name | Inventor | |---|---|---|---| | c. 420 B.C. | A wooden, steam-propelled bird, which was able to fly | Flying pigeon | Archytas of Tarentum | | Third century B.C. and earlier | One of the earliest descriptions of automata appears in the Lie Zi text, on a much earlier encounter between King Mu of Zhou (1023–957 BC) and a mechanical engineer known as Yan Shi, an 'artificer'. The latter allegedly presented the king with a life-size, human-shaped figure of his mechanical handiwork.[166] | Yan Shi (Chinese: 偃师) | | | First century A.D. and earlier | Descriptions of more than 100 machines and automata, including a fire engine, a wind organ, a coin-operated machine, and a steam-powered engine, in Pneumatica and Automata by Heron of Alexandria | Ctesibius, Philo of Byzantium, Heron of Alexandria, and others | | | 1206 | Created early humanoid automata, programmable automaton band[167] Robot band, hand-washing automaton,[168] automated moving peacocks[169] | Al-Jazari | | | 1495 | Designs for a humanoid robot | Mechanical Knight | Leonardo da Vinci | | 1560s | Clockwork Prayer that had machinal feet built under its robes that imitated walking. The robot's eyes, lips, and head all move in lifelike gestures. | Clockwork Prayer [citation needed] | Gianello della Torre | | 1738 | Mechanical duck that was able to eat, flap its wings, and excrete | Digesting Duck | Jacques de Vaucanson | | 1898 | Nikola Tesla demonstrates the first radio-controlled vessel. | Teleautomaton | Nikola Tesla | | 1903 | Leonardo Torres Quevedo presented the Telekino at the Paris Academy of Science, a radio-based control system with different operational states, for testing airships without risking human lives.[170] He conduct the initial test controlling a tricycle almost 100 feet away, being the first example of a radio-controlled unmanned ground vehicle.[171][172] | Telekino | Leonardo Torres Quevedo | | 1912 | Leonardo Torres Quevedo builds the first truly autonomous machine capable of playing chess. As opposed to the human-operated The Turk and Ajeeb, El Ajedrecista had an integrated automaton built to play chess without human guidance. It only played an endgame with three chess pieces, automatically moving a white king and a rook to checkmate the black king moved by a human opponent.[173][174] | El Ajedrecista | Leonardo Torres Quevedo | | 1914 | In his paper Essays on Automatics published in 1914, Leonardo Torres Quevedo proposed a machine that makes \"judgments\" using sensors that capture information from the outside, parts that manipulate the outside world like arms, power sources such as batteries and air pressure, and most importantly, captured information and past information. It was defined as an organism that can control reactions in response to external information and adapt to changes in the environment to change its behavior.[175][176][177][178] | Essays on Automatics | Leonardo Torres Quevedo | | 1921 | First fictional automatons called \"robots\" appear in the play R.U.R. | Rossum's Universal Robots | Karel Čapek | | 1930s | Humanoid robot exhibited at the 1939 and 1940 World's Fairs | Elektro | Westinghouse Electric Corporation | | 1946 | First general-purpose digital computer | Whirlwind | Multiple people | | 1948 | Simple robots exhibiting biological behaviors[179] | Elsie and Elmer | William Grey Walter | | 1948 | Formulation of principles of cybernetics | cybernetics | Norbert Wiener | | 1956 | First commercial robot, from the Unimation company founded by George Devol and Joseph Engelberger, based on Devol's patents[180] | Unimate | George Devol | | 1961 | First installed industrial robot. The first digitally operated and programmable robot, Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. | Unimate | George Devol | | 1967 to 1972 | First full-scale humanoid intelligent robot,[181][182] and first android. Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with its hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes, and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[183][184][185] | WABOT-1 | Waseda University | | 1973 | First industrial robot with six electromechanically driven axes[186][187] | Famulus | KUKA Robot Group | | 1974 | The world's first microcomputer controlled electric industrial robot, IRB 6 from ASEA, was delivered to a small mechanical engineering company in southern Sweden. The design of this robot had been patented in 1972. | IRB 6 | ABB Robot Group | | 1975 | Programmable universal manipulation arm, a Unimation product | PUMA | Victor Scheinman | | 1978 | The first object-level robot programming language, RAPT, allowing robots to handle variations in object position, shape, and sensor noise.[188] | Freddy I and II | Patricia Ambler and Robin Popplestone | | 1983 | First multitasking, the parallel programming language used for robot control. It was the Event Driven Language (EDL) on the IBM/Series/1 process computer, with the implementation of both inter-process communication (WAIT/POST) and mutual exclusion (ENQ/DEQ) mechanisms for robot control.[189] | ADRIEL I | Stevo Bozinovski and Mihail Sestakov | See also [edit]- Artificial intelligence - Autonomous robot - Cloud robotics - Cognitive robotics - Evolutionary robotics - Fog robotics - Glossary of robotics - Index of robotics articles - Mechatronics - Multi-agent system - Outline of robotics - Quantum robotics - Roboethics - Robot rights - Robotic art - Robotic governance - Self-reconfiguring modular robot - Soft robotics - Telerobotics Notes [edit]- ^ One database, developed by the United States Department of Energy, contains information on almost 500 existing robotic technologies.[10] References [edit]- ^ \"German National Library\". International classification system of the German National Library (GND). Archived from the original on 2020-08-19. - ^ \"Origami-Inspired Robots Can Sense, Analyze and Act in Challenging Environments\". UCLA. Retrieved 2023-04-10. - ^ Raj, Aditi (26 August 2024). \"AI & Robotics: The Role of AI in Robots\". Retrieved 2024-08-29. - ^ Hunt, V. Daniel (1985). \"Smart Robots\". Smart Robots: A Handbook of Intelligent Robotic Systems. Chapman and Hall. p. 141. ISBN 978-1-4613-2533-8. Archived from the original on 2023-03-15. Retrieved 2018-12-04. - ^ \"Robot density rises globally\". Robotic Industries Association. 8 February 2018. Archived from the original on 2020-11-23. Retrieved 2018-12-03. - ^ Pinto, Jim (1 October 2003). \"Fully automated factories approach reality\". Automation World. Archived from the original on 2011-10-01. Retrieved 2018-12-03. - ^ Eyre, Michael (12 September 2014). \"'Boris' the robot can load up dishwasher\". BBC News. Archived from the original on 2020-12-21. Retrieved 2018-12-03. - ^ Corner, Stuart (23 November 2017). \"AI-driven robot makes 'perfect' flatbread\". iothub.com.au. Archived from the original on 2020-11-24. Retrieved 2018-12-03. - ^ Pollock, Emily (7 June 2018). \"Construction Robotics Industry Set to Double by 2023\". engineering.com. Archived from the original on 2020-08-07. Retrieved 2018-12-03. - ^ \"Technology Advanced Search\". D&D Knowledge Management Information Tool. Archived from the original on 2020-08-06. - ^ Arámbula Cosío, F.; Hibberd, R. D.; Davies, B. L. (July 1997). \"Electromagnetic compatibility aspects of active robotic systems for surgery: the robotic prostatectomy experience\". Medical and Biological Engineering and Computing. 35 (4): 436–440. doi:10.1007/BF02534105. ISSN 1741-0444. PMID 9327627. S2CID 21479700. - ^ Grift, Tony E. (2004). \"Agricultural Robotics\". University of Illinois at Urbana–Champaign. Archived from the original on 2007-05-04. Retrieved 2018-12-03. - ^ Thomas, Jim (1 November 2017). \"How corporate giants are automating the farm\". New Internationalist. Archived from the original on 2021-01-10. Retrieved 2018-12-03. - ^ Kolodny, Lora (4 July 2017). \"Robots are coming to a burger joint near you\". CNBC. Archived from the original on 2020-12-05. Retrieved 2018-12-03. - ^ Kirsner, Scott (27 January 2023). \"Robots in the kitchen? Local engineers are making it a reality\". The Boston Globe. - ^ Dowling, Kevin. \"Power Sources for Small Robots\" (PDF). Carnegie Mellon University. Archived (PDF) from the original on 2020-11-25. Retrieved 2012-05-11. - ^ Roozing, Wesley; Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2016). \"Design Optimisation and Control of Compliant Actuation Arrangements in Articulated Robots for Improved Energy Efficiency\". IEEE Robotics and Automation Letters. 1 (2): 1110–1117. Bibcode:2016IRAL....1.1110R. doi:10.1109/LRA.2016.2521926. S2CID 1940410. - ^ Pratt, G. A.; Williamson, M. M. (1995). \"Series elastic actuators\". Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human-Robot Interaction and Cooperative Robots. Vol. 1. pp. 399–406. doi:10.1109/IROS.1995.525827. hdl:1721.1/36966. ISBN 0-8186-7108-4. S2CID 17120394. - ^ Furnémont, Raphaël; Mathijssen, Glenn; Verstraten, Tom; Lefeber, Dirk; Vanderborght, Bram (27 January 2016). \"Bi-directional series-parallel elastic actuator and overlap of the actuation layers\" (PDF). Bioinspiration & Biomimetics. 11 (1) 016005. Bibcode:2016BiBi...11a6005F. doi:10.1088/1748-3190/11/1/016005. PMID 26813145. S2CID 37031990. Archived (PDF) from the original on 2022-10-01. Retrieved 2023-03-15. - ^ Pratt, Jerry E.; Krupp, Benjamin T. (2004). \"Series Elastic Actuators for legged robots\". In Gerhart, Grant R; Shoemaker, Chuck M; Gage, Douglas W (eds.). Unmanned Ground Vehicle Technology VI. Vol. 5422. pp. 135–144. doi:10.1117/12.548000. S2CID 16586246. - ^ Li, Zhibin; Tsagarakis, Nikos; Caldwell, Darwin (2013). \"Walking Pattern Generation for a Humanoid Robot with Compliant Joints\". Autonomous Robots. 35 (1): 1–14. doi:10.1007/s10514-013-9330-7. S2CID 624563. - ^ Colgate, J. Edward (1988). The control of dynamically interacting systems (Thesis). hdl:1721.1/14380. - ^ Calanca, Andrea; Muradore, Riccardo; Fiorini, Paolo (November 2017). \"Impedance control of series elastic actuators: Passivity and acceleration-based control\". Mechatronics. 47: 37–48. doi:10.1016/j.mechatronics.2017.08.010. - ^ Tosun, Fatih Emre; Patoglu, Volkan (June 2020). \"Necessary and Sufficient Conditions for the Passivity of Impedance Rendering With Velocity-Sourced Series Elastic Actuation\". IEEE Transactions on Robotics. 36 (3): 757–772. Bibcode:2020ITRob..36..757T. doi:10.1109/TRO.2019.2962332. S2CID 212907787. - ^ www.imagesco.com, Images SI Inc -. \"Air Muscle actuators, going further, page 6\". Archived from the original on 2020-11-14. Retrieved 2010-05-24. - ^ \"Air Muscles\". Shadow Robot. Archived from the original on 2007-09-27. - ^ Tondu, Bertrand (2012). \"Modelling of the McKibben artificial muscle: A review\". Journal of Intelligent Material Systems and Structures. 23 (3): 225–253. doi:10.1177/1045389X11435435. S2CID 136854390. - ^ \"TALKING ELECTRONICS Nitinol Page-1\". Talkingelectronics.com. Archived from the original on 2020-01-18. Retrieved 2010-11-27. - ^ \"lf205, Hardware: Building a Linux-controlled walking robot\". Ibiblio.org. 1 November 2001. Archived from the original on 2016-03-03. Retrieved 2010-11-27. - ^ \"WW-EAP and Artificial Muscles\". Eap.jpl.nasa.gov. Archived from the original on 2017-01-20. Retrieved 2010-11-27. - ^ \"Empa – a117-2-eap\". Empa.ch. Archived from the original on 2015-09-24. Retrieved 2010-11-27. - ^ \"Electroactive Polymers (EAP) as Artificial Muscles (EPAM) for Robot Applications\". Hizook. Archived from the original on 2020-08-06. Retrieved 2010-11-27. - ^ \"Piezo LEGS – -09-26\". Archived from the original on 2008-01-30. Retrieved 2007-10-28. - ^ \"Squiggle Motors: Overview\". Archived from the original on 2007-10-07. Retrieved 2007-10-08. - ^ Nishibori; et al. (2003). \"Robot Hand with Fingers Using Vibration-Type Ultrasonic Motors (Driving Characteristics)\". Journal of Robotics and Mechatronics. 15 (6): 588–595. doi:10.20965/jrm.2003.p0588. - ^ Otake, Mihoko; Kagami, Yoshiharu; Ishikawa, Kohei; Inaba, Masayuki; Inoue, Hirochika (6 April 2001). Wilson, Alan R.; Asanuma, Hiroshi (eds.). \"Shape design of gel robots made of electroactive polymer gel\". Smart Materials. 4234: 194–202. Bibcode:2001SPIE.4234..194O. doi:10.1117/12.424407. S2CID 30357330. - ^ Madden, John D. (16 November 2007). \"Mobile Robots: Motor Challenges and Materials Solutions\". Science. 318 (5853): 1094–1097. Bibcode:2007Sci...318.1094M. CiteSeerX 10.1.1.395.4635. doi:10.1126/science.1146351. PMID 18006737. S2CID 52827127. - ^ \"Syntouch LLC: BioTac(R) Biomimetic Tactile Sensor Array\". Archived from the original on 2009-10-03. Retrieved 2009-08-10. - ^ Wettels, Nicholas; Santos, Veronica J.; Johansson, Roland S.; Loeb, Gerald E. (January 2008). \"Biomimetic Tactile Sensor Array\". Advanced Robotics. 22 (8): 829–849. doi:10.1163/156855308X314533. S2CID 4594917. - ^ \"What is The SmartHand?\". SmartHand Project. Archived from the original on 2015-03-03. Retrieved 2011-02-04. - ^ a b Arreguin, Juan (2008). Automation and Robotics. Vienna, Austria: I-Tech and Publishing. - ^ \"Annotated Mythbusters: Episode 78: Ninja Myths – Walking on Water, Catching a Sword, Catching an Arrow\". Archived from the original on 2020-11-12. Retrieved 2010-02-13. (Discovery Channel's Mythbusters making mechanical gripper from the chain and metal wire) - ^ \"Robonaut hand\". Archived from the original on 2020-02-22. Retrieved 2011-11-21. - ^ \"Delft hand\". TU Delft. Archived from the original on 2012-02-03. Retrieved 2011-11-21. - ^ M & C. \"TU Delft ontwikkelt goedkope, voorzichtige robothand\". TU Delft. Archived from the original on 2017-03-13. Retrieved 2011-11-21. - ^ \"astrictive definition – English definition dictionary – Reverso\". Archived from the original on 2020-04-30. Retrieved 2008-01-06. - ^ Tijsma, H. A.; Liefhebber, F.; Herder, J. L. (2005). \"Evaluation of New User Interface Features for the MANUS Robot Arm\". 9th International Conference on Rehabilitation Robotics, 2005. ICORR 2005. pp. 258–263. doi:10.1109/ICORR.2005.1501097. ISBN 0-7803-9003-2. S2CID 36445389. - ^ Allcock, Andrew (2006). \"Anthropomorphic hand is almost human\". Machinery. Archived from the original on 2007-09-28. Retrieved 2007-10-17. - ^ \"Welcome\". Archived (PDF) from the original on 2013-05-10. Retrieved 2007-10-28. - ^ a b c Corke, Peter (2017). Robotics, Vision and Control. Springer Tracts in Advanced Robotics. Vol. 118. doi:10.1007/978-3-319-54413-7. ISBN 978-3-319-54412-0. ISSN 1610-7438. Archived from the original on 2022-10-20. Retrieved 2023-03-15. - ^ a b c Lee, C S. G.; Fu, K. S.; Gonzalez, Ralph (1987). Robotics: Control Sensing. Vis. McGraw-Hill. ISBN 978-0-07-026510-3. Archived from the original on 2023-03-15. Retrieved 2023-03-15. - ^ a b c d Short, Michael; Burn, Kevin (1 April 2011). \"A generic controller architecture for intelligent robotic systems\". Robotics and Computer-Integrated Manufacturing. 27 (2): 292–305. doi:10.1016/j.rcim.2010.07.013. ISSN 0736-5845. - ^ Ray, Partha Pratim (2016). \"Internet of Robotic Things: Concept, Technologies, and Challenges\". IEEE Access. 4: 9489–9500. Bibcode:2016IEEEA...4.9489R. doi:10.1109/ACCESS.2017.2647747. ISSN 2169-3536. S2CID 9273802. - ^ a b Burn, K.; Short, M.; Bicker, R. (July 2003). \"Adaptive and Nonlinear Fuzzy Force Control Techniques Applied to Robots Operating in Uncertain Environments\". Journal of Robotic Systems. 20 (7): 391–400. doi:10.1002/rob.10093. ISSN 0741-2223. Archived from the original on 2022-11-26. Retrieved 2023-03-15. - ^ Burn, Kevin; Home, Geoffrey (1 May 2008). \"Environment classification using Kohonen self-organizing maps\". Expert Systems. 25 (2): 98–114. doi:10.1111/j.1468-0394.2008.00441.x. ISSN 0266-4720. S2CID 33369232. - ^ Mason, Matthew T. (2001). Mechanics of Robotic Manipulation. doi:10.7551/mitpress/4527.001.0001. ISBN 978-0-262-25662-9. S2CID 5260407. - ^ \"What is a robotic end-effector?\". ATI Industrial Automation. 2007. Archived from the original on 2020-12-17. Retrieved 2007-10-16. - ^ Crane, Carl D.; Joseph Duffy (1998). Kinematic Analysis of Robot Manipulators. Cambridge University Press. ISBN 978-0-521-57063-3. Archived from the original on 2020-04-02. Retrieved 2007-10-16. - ^ G. J. Monkman, S. Hesse, R. Steinmann & H. Schunk (2007). Robot Grippers. Berlin, Germany: Wiley. - ^ \"T.O.B.B\". Mtoussaint.de. Archived from the original on 2020-07-08. Retrieved 2010-11-27. - ^ \"nBot, a two wheel balancing robot\". Geology.heroy.smu.edu. Archived from the original on 2021-01-26. Retrieved 2010-11-27. - ^ \"ROBONAUT Activity Report\". NASA. 2004. Archived from the original on 2007-08-20. Retrieved 2007-10-20. - ^ Guizzo, Erico (29 April 2010). \"A Robot That Balances on a Ball\". IEEE Spectrum. Archived from the original on 2023-02-10. Retrieved 2023-03-15. - ^ \"Carnegie Mellon Researchers Develop New Type of Mobile Robot That Balances and Moves on a Ball Instead of Legs or Wheels\" (Press release). Carnegie Mellon. 9 August 2006. Archived from the original on 2007-06-09. Retrieved 2007-10-20. - ^ \"Spherical Robot Can Climb Over Obstacles\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27. - ^ \"Rotundus\". Rotundus.se. Archived from the original on 2011-08-26. Retrieved 2010-11-27. - ^ \"OrbSwarm Gets A Brain\". BotJunkie. 11 July 2007. Archived from the original on 2012-05-16. Retrieved 2010-11-27. - ^ \"Rolling Orbital Bluetooth Operated Thing\". BotJunkie. Archived from the original on 2012-03-28. Retrieved 2010-11-27. - ^ \"Swarm\". Orbswarm.com. Archived from the original on 2021-01-26. Retrieved 2010-11-27. - ^ \"The Ball Bot: Johnnytronic@Sun\". Blogs.sun.com. Archived from the original on 2011-08-24. Retrieved 2010-11-27. - ^ \"Senior Design Projects | College of Engineering & Applied Science| University of Colorado at Boulder\". Engineering.colorado.edu. 30 April 2008. Archived from the original on 2011-07-23. Retrieved 2010-11-27. - ^ \"JPL Robotics: System: Commercial Rovers\". Archived from the original on 2006-06-15. - ^ \"AMBER Lab\". Archived from the original on 2020-11-25. Retrieved 2012-01-23. - ^ \"Micromagic Systems Robotics Lab\". Archived from the original on 2017-06-01. Retrieved 2009-04-29. - ^ \"AMRU-5 hexapod robot\" (PDF). Archived (PDF) from the original on 2016-08-17. Retrieved 2009-04-29. - ^ \"Achieving Stable Walking\". Honda Worldwide. Archived from the original on 2011-11-08. Retrieved 2007-10-22. - ^ \"Funny Walk\". Pooter Geek. 28 December 2004. Archived from the original on 2011-09-28. Retrieved 2007-10-22. - ^ \"ASIMO's Pimp Shuffle\". Popular Science. 9 January 2007. Archived from the original on 2011-07-24. Retrieved 2007-10-22. - ^ \"Robot Shows Prime Minister How to Loosen Up > > A drunk robot?\". The Temple of VTEC – Honda and Acura Enthusiasts Online Forums. 25 August 2003. Archived from the original on 2020-04-30. - ^ \"3D One-Leg Hopper (1983–1984)\". MIT Leg Laboratory. Archived from the original on 2018-07-25. Retrieved 2007-10-22. - ^ \"3D Biped (1989–1995)\". MIT Leg Laboratory. Archived from the original on 2011-09-26. Retrieved 2007-10-28. - ^ \"Quadruped (1984–1987)\". MIT Leg Laboratory. Archived from the original on 2011-08-23. Retrieved 2007-10-28. - ^ \"MIT Leg Lab Robots – Main\". Archived from the original on 2020-08-07. Retrieved 2007-10-28. - ^ \"About the Robots\". Anybots. Archived from the original on 2007-09-09. Retrieved 2007-10-23. - ^ \"Anything, Anytime, Anywhere\". Anybots. Archived from the original on 2007-10-27. Retrieved 2007-10-23. - ^ \"Dexter Jumps video\". YouTube. 1 March 2007. Archived from the original on 2021-10-30. Retrieved 2007-10-23. - ^ Collins, Steve; Ruina, Andy; Tedrake, Russ; Wisse, Martijn (18 February 2005). \"Efficient Bipedal Robots Based on Passive-Dynamic Walkers\". Science. 307 (5712): 1082–1085. Bibcode:2005Sci...307.1082C. doi:10.1126/science.1107799. PMID 15718465. S2CID 1315227. - ^ Collins, S. H.; Ruina, A. (2005). \"A Bipedal Walking Robot with Efficient and Human-Like Gait\". Proceedings of the 2005 IEEE International Conference on Robotics and Automation. pp. 1983–1988. doi:10.1109/ROBOT.2005.1570404. ISBN 0-7803-8914-X. S2CID 15145353. - ^ \"Testing the Limits\" (PDF). Boeing. p. 29. Archived (PDF) from the original on 2018-12-15. Retrieved 2008-04-09. - ^ Zhang, Jun; Zhao, Ning; Qu, Feiyang (15 November 2022). \"Bio-inspired flapping wing robots with foldable or deformable wings: a review\". Bioinspiration & Biomimetics. 18 (1): 011002. doi:10.1088/1748-3190/ac9ef5. ISSN 1748-3182. PMID 36317380. S2CID 253246037. - ^ a b c Shin, Won Dong; Park, Jaejun; Park, Hae-Won (1 September 2019). \"Development and experiments of a bio-inspired robot with multi-mode in aerial and terrestrial locomotion\". Bioinspiration & Biomimetics. 14 (5): 056009. Bibcode:2019BiBi...14e6009S. doi:10.1088/1748-3190/ab2ab7. ISSN 1748-3182. PMID 31212268. S2CID 195066183. - ^ Ramezani, Alireza; Shi, Xichen; Chung, Soon-Jo; Hutchinson, Seth (May 2016). \"Bat Bot (B2), a biologically inspired flying machine\". 2016 IEEE International Conference on Robotics and Automation (ICRA). Stockholm, Sweden: IEEE. pp. 3219–3226. doi:10.1109/ICRA.2016.7487491. ISBN 978-1-4673-8026-3. S2CID 8581750. - ^ a b Daler, Ludovic; Mintchev, Stefano; Stefanini, Cesare; Floreano, Dario (19 January 2015). \"A bioinspired multi-modal flying and walking robot\". Bioinspiration & Biomimetics. 10 (1) 016005. Bibcode:2015BiBi...10a6005D. doi:10.1088/1748-3190/10/1/016005. ISSN 1748-3190. PMID 25599118. S2CID 11132948. - ^ a b Kilian, Lukas; Shahid, Farzeen; Zhao, Jing-Shan; Nayeri, Christian Navid (1 July 2022). \"Bioinspired morphing wings: mechanical design and wind tunnel experiments\". Bioinspiration & Biomimetics. 17 (4): 046019. Bibcode:2022BiBi...17d6019K. doi:10.1088/1748-3190/ac72e1. ISSN 1748-3182. PMID 35609562. S2CID 249045806. - ^ Savastano, E.; Perez-Sanchez, V.; Arrue, B.C.; Ollero, A. (July 2022). \"High-Performance Morphing Wing for Large-Scale Bio-Inspired Unmanned Aerial Vehicles\". IEEE Robotics and Automation Letters. 7 (3): 8076–8083. Bibcode:2022IRAL....7.8076S. doi:10.1109/LRA.2022.3185389. ISSN 2377-3766. S2CID 250008824. - ^ Grant, Daniel T.; Abdulrahim, Mujahid; Lind, Rick (June 2010). \"Flight Dynamics of a Morphing Aircraft Utilizing Independent Multiple-Joint Wing Sweep\". International Journal of Micro Air Vehicles. 2 (2): 91–106. doi:10.1260/1756-8293.2.2.91. ISSN 1756-8293. S2CID 110577545. - ^ Phan, Hoang Vu; Park, Hoon Cheol (4 December 2020). \"Mechanisms of collision recovery in flying beetles and flapping-wing robots\". Science. 370 (6521): 1214–1219. Bibcode:2020Sci...370.1214P. doi:10.1126/science.abd3285. ISSN 0036-8075. PMID 33273101. S2CID 227257247. - ^ Hu, Zheng; McCauley, Raymond; Schaeffer, Steve; Deng, Xinyan (May 2009). \"Aerodynamics of dragonfly flight and robotic design\". 2009 IEEE International Conference on Robotics and Automation. pp. 3061–3066. doi:10.1109/ROBOT.2009.5152760. ISBN 978-1-4244-2788-8. S2CID 12291429. - ^ Balta, Miquel; Deb, Dipan; Taha, Haithem E (26 October 2021). \"Flow visualization and force measurement of the clapping effect in bio-inspired flying robots\". Bioinspiration & Biomimetics. 16 (6): 066020. Bibcode:2021BiBi...16f6020B. doi:10.1088/1748-3190/ac2b00. ISSN 1748-3182. PMID 34584023. S2CID 238217893. - ^ Miller, Gavin. \"Introduction\". snakerobots.com. Archived from the original on 2011-08-17. Retrieved 2007-10-22. - ^ \"ACM-R5\". Archived from the original on 2011-10-11. - ^ \"Swimming snake robot (commentary in Japanese)\". Archived from the original on 2012-02-08. Retrieved 2007-10-28. - ^ \"Commercialized Quadruped Walking Vehicle \"TITAN VII\"\". Hirose Fukushima Robotics Lab. Archived from the original on 2007-11-06. Retrieved 2007-10-23. - ^ Pachal, Peter (23 January 2007). \"Plen, the robot that skates across your desk\". SCI FI Tech. Archived from the original on 2007-10-11. - ^ Capuchin on YouTube - ^ Wallbot on YouTube - ^ Stanford University: Stickybot on YouTube - ^ Sfakiotakis, M.; Lane, D. M.; Davies, J. B. C. (April 1999). \"Review of fish swimming modes for aquatic locomotion\". IEEE Journal of Oceanic Engineering. 24 (2): 237–252. Bibcode:1999IJOE...24..237S. CiteSeerX 10.1.1.459.8614. doi:10.1109/48.757275. S2CID 17226211. - ^ Mason, Richard. \"What is the market for robot fish?\". Archived from the original on 2009-07-04. - ^ \"Robotic fish powered by Gumstix PC and PIC\". Human Centred Robotics Group at Essex University. Archived from the original on 2011-08-14. Retrieved 2007-10-25. - ^ Juwarahawong, Witoon. \"Fish Robot\". Institute of Field Robotics. Archived from the original on 2007-11-04. Retrieved 2007-10-25. - ^ \"Festo – AquaPenguin\". 17 April 2009 – via YouTube. - ^ \"High-Speed Robotic Fish\". iSplash-Robotics. Archived from the original on 2020-03-11. Retrieved 2017-01-07. - ^ \"iSplash-II: Realizing Fast Carangiform Swimming to Outperform a Real Fish\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29. - ^ \"iSplash-I: High Performance Swimming Motion of a Carangiform Robotic Fish with Full-Body Coordination\" (PDF). Robotics Group at Essex University. Archived from the original (PDF) on 2015-09-30. Retrieved 2015-09-29. - ^ Jaulin, Luc; Le Bars, Fabrice (February 2013). \"An Interval Approach for Stability Analysis: Application to Sailboat Robotics\". IEEE Transactions on Robotics. 29 (1): 282–287. Bibcode:2013ITRob..29..282J. CiteSeerX 10.1.1.711.7180. doi:10.1109/TRO.2012.2217794. S2CID 4977937. - ^ \"A Ping-Pong-Playing Terminator\". Popular Science. Archived from the original on 2021-01-22. Retrieved 2010-12-19. - ^ \"Synthiam Exosphere combines AI, human operators to train robots\". The Robot Report. Archived from the original on 2020-10-06. Retrieved 2020-04-29. - ^ a b Kagan, Eugene; Ben-Gal, Irad (2015). Search and foraging:individual motion and swarm dynamics. Chapman and Hall/CRC. ISBN 978-1-4822-4210-2. Archived from the original on 2023-03-15. Retrieved 2020-08-26. - ^ Banks, Jaime (2020). \"Optimus Primed: Media Cultivation of Robot Mental Models and Social Judgments\". Frontiers in Robotics and AI. 7 62. doi:10.3389/frobt.2020.00062. PMC 7805817. PMID 33501230. - ^ a b Wullenkord, Ricarda; Fraune, Marlena R.; Eyssel, Friederike; Sabanovic, Selma (2016). \"Getting in Touch: How imagined, actual, and physical contact affect evaluations of robots\". 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). pp. 980–985. doi:10.1109/ROMAN.2016.7745228. ISBN 978-1-5090-3929-6. S2CID 6305599. - ^ Norberto Pires, J. (December 2005). \"Robot-by-voice: experiments on commanding an industrial robot using the human voice\". Industrial Robot. 32 (6): 505–511. doi:10.1108/01439910510629244. - ^ \"Survey of the State of the Art in Human Language Technology: 1.2: Speech Recognition\". Archived from the original on 2007-11-11. - ^ Fournier, Randolph Scott; Schmidt, B. June (1995). \"Voice input technology: Learning style and attitude toward its use\". Delta Pi Epsilon Journal. 37 (1): 1–12. ProQuest 1297783046. - ^ \"History of Speech & Voice Recognition and Transcription Software\". Dragon Naturally Speaking. Archived from the original on 2015-08-13. Retrieved 2007-10-27. - ^ Cheng Lin, Kuan; Huang, Tien-Chi; Hung, Jason C.; Yen, Neil Y.; Ju Chen, Szu (7 June 2013). \"Facial emotion recognition towards affective computing-based learning\". Library Hi Tech. 31 (2): 294–307. doi:10.1108/07378831311329068. - ^ Walters, M. L.; Syrdal, D. S.; Koay, K. L.; Dautenhahn, K.; Te Boekhorst, R. (2008). \"Human approach distances to a mechanical-looking robot with different robot voice styles\". RO-MAN 2008 – the 17th IEEE International Symposium on Robot and Human Interactive Communication. pp. 707–712. doi:10.1109/ROMAN.2008.4600750. ISBN 978-1-4244-2212-8. S2CID 8653718. - ^ Pauletto, Sandra; Bowles, Tristan (2010). \"Designing the emotional content of a robotic speech signal\". Proceedings of the 5th Audio Mostly Conference on a Conference on Interaction with Sound – AM '10. pp. 1–8. doi:10.1145/1859799.1859804. ISBN 978-1-4503-0046-9. S2CID 30423778. - ^ Bowles, Tristan; Pauletto, Sandra (2010). Emotions in the Voice: Humanising a Robotic Voice (PDF). Proceedings of the 7th Sound and Music Computing Conference. Barcelona. Archived (PDF) from the original on 2023-02-10. Retrieved 2023-03-15. - ^ \"World of 2-XL: Leachim\". www.2xlrobot.com. Archived from the original on 2020-07-05. Retrieved 2019-05-28. - ^ \"The Boston Globe from Boston, Massachusetts on June 23, 1974 · 132\". Newspapers.com. 23 June 1974. Archived from the original on 2020-01-10. Retrieved 2019-05-28. - ^ a b \"A history of cybernetic animals and early robots\". cyberneticzoo.com. p. 135. Archived from the original on 2020-08-06. Retrieved 2019-05-28. - ^ \"Frubber facial expressions\". Archived from the original on 2009-02-07. - ^ \"Best Inventions of 2008 – TIME\". Time. 29 October 2008. Archived from the original on 2008-11-02 – via www.time.com. - ^ \"Kismet: Robot at MIT's AI Lab Interacts With Humans\". Sam Ogden. Archived from the original on 2007-10-12. Retrieved 2007-10-28. - ^ Waldherr, Stefan; Romero, Roseli; Thrun, Sebastian (1 September 2000). \"A Gesture Based Interface for Human-Robot Interaction\". Autonomous Robots. 9 (2): 151–173. doi:10.1023/A:1008918401478. S2CID 1980239. - ^ Li, Ling Hua; Du, Ji Fang (December 2012). \"Visual Based Hand Gesture Recognition Systems\". Applied Mechanics and Materials. 263–266: 2422–2425. Bibcode:2012AMM...263.2422L. doi:10.4028/www.scientific.net/AMM.263-266.2422. S2CID 62744240. - ^ \"Armenian Robin the Robot to comfort kids at U.S. clinics starting July\". Public Radio of Armenia. Archived from the original on 2021-05-13. Retrieved 2021-05-13. - ^ Park, S.; Sharlin, Ehud; Kitamura, Y.; Lau, E. (29 April 2005). Synthetic Personality in Robots and its Effect on Human-Robot Relationship (Report). doi:10.11575/PRISM/31041. hdl:1880/45619. - ^ \"Robot Receptionist Dishes Directions and Attitude\". NPR.org. Archived from the original on 2020-12-01. Retrieved 2018-04-05. - ^ \"New Scientist: A good robot has personality but not looks\" (PDF). Archived from the original (PDF) on 2006-09-29. - ^ \"Playtime with Pleo, your robotic dinosaur friend\". 25 September 2008. Archived from the original on 2019-01-20. Retrieved 2014-12-14. - ^ NOVA conversation with Professor Moravec, October 1997. NOVA Online Archived 2017-08-02 at the Wayback Machine - ^ Agarwal, P. K. Elements of Physics XI. Rastogi Publications. p. 2. ISBN 978-81-7133-911-2. - ^ Sandhana, Lakshmi (5 September 2002). \"A Theory of Evolution, for Robots\". Wired. Archived from the original on 2014-03-29. Retrieved 2007-10-28. - ^ \"Experimental Evolution In Robots Probes The Emergence Of Biological Communication\". Science Daily. 24 February 2007. Archived from the original on 2018-11-16. Retrieved 2007-10-28. - ^ Žlajpah, Leon (15 December 2008). \"Simulation in robotics\". Mathematics and Computers in Simulation. 79 (4): 879–897. doi:10.1016/j.matcom.2008.02.017. - ^ \"Evolution trains robot teams TRN 051904\". Technology Research News. Archived from the original on 2016-06-23. Retrieved 2009-01-22. - ^ Müller, Christopher (2023). World Robotics 2023 – Industrial Robots. Frankfurt, Germany: IFR Statistical Department, VDMA Services GmbH. - ^ Tandon, Prateek (2017). Quantum Robotics. Morgan & Claypool Publishers. ISBN 978-1-62705-913-8. - ^ Dragani, Rachelle (8 November 2018). \"Can a robot make you a 'superworker'?\". Verizon Communications. Archived from the original on 2020-08-06. Retrieved 2018-12-03. - ^ \"Robotics\". American Elements. Retrieved 2023-04-10. - ^ \"Career: Robotics Engineer\". Princeton Review. 2012. Archived from the original on 2015-01-21. Retrieved 2012-01-27. - ^ Saad, Ashraf; Kroutil, Ryan (2012). Hands-on Learning of Programming Concepts Using Robotics for Middle and High School Students. Proceedings of the 50th Annual Southeast Regional Conference of the Association for Computing Machinery. ACM. pp. 361–362. doi:10.1145/2184512.2184605. - ^ Toy, Tommy (29 June 2011). \"Outlook for robotics and Automation for 2011 and beyond are excellent says expert\". PBT Consulting. Archived from the original on 2012-01-27. Retrieved 2012-01-27. - ^ Frey, Carl Benedikt; Osborne, Michael A. (January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change. 114: 254–280. CiteSeerX 10.1.1.395.416. doi:10.1016/j.techfore.2016.08.019. - ^ McGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID 243172487. SSRN 3044448. - ^ Hawking, Stephen (1 January 2016). \"This is the most dangerous time for our planet\". The Guardian. Archived from the original on 2021-01-31. Retrieved 2019-11-22. - ^ \"Robotics – Thematic Research\". GlobalData. Archived from the original on 2021-09-28. Retrieved 2021-09-22. - ^ \"Focal Points Seminar on review articles in the future of work – Safety and health at work – EU-OSHA\". osha.europa.eu. Archived from the original on 2020-01-25. Retrieved 2016-04-19. - ^ \"Robotics: Redefining crime prevention, public safety and security\". SourceSecurity.com. Archived from the original on 2017-10-09. Retrieved 2016-09-16. - ^ \"Draft Standard for Intelligent Assist Devices — Personnel Safety Requirements\" (PDF). Archived (PDF) from the original on 2020-11-25. Retrieved 2016-06-01. - ^ \"ISO/TS 15066:2016 – Robots and robotic devices – Collaborative robots\". 8 March 2016. Archived from the original on 2016-10-10. Retrieved 2016-06-01. - ^ Brogårdh, Torgny (January 2007). \"Present and future robot control development—An industrial perspective\". Annual Reviews in Control. 31 (1): 69–79. doi:10.1016/j.arcontrol.2007.01.002. ISSN 1367-5788. - ^ Wang, Tian-Miao; Tao, Yong; Liu, Hui (17 April 2018). \"Current Researches and Future Development Trend of Intelligent Robot: A Review\". International Journal of Automation and Computing. 15 (5): 525–546. doi:10.1007/s11633-018-1115-1. ISSN 1476-8186. S2CID 126037910. - ^ Needham, Joseph (1991). Science and Civilisation in China: Volume 2, History of Scientific Thought. Cambridge University Press. ISBN 978-0-521-05800-1. - ^ Fowler, Charles B. (October 1967). \"The Museum of Music: A History of Mechanical Instruments\". Music Educators Journal. 54 (2): 45–49. doi:10.2307/3391092. JSTOR 3391092. S2CID 190524140. - ^ Rosheim, Mark E. (1994). Robot Evolution: The Development of Anthrobotics. Wiley-IEEE. pp. 9–10. ISBN 978-0-471-02622-8. - ^ al-Jazari (Islamic artist) Archived 2008-05-07 at the Wayback Machine, Encyclopædia Britannica. - ^ A. P. Yuste. Electrical Engineering Hall of Fame. Early Developments of Wireless Remote Control: The Telekino of Torres-Quevedo,(pdf) vol. 96, No. 1, January 2008, Proceedings of the IEEE. - ^ H. R. Everett (2015). Unmanned Systems of World Wars I and II. MIT Press. pp. 91–95. ISBN 978-0-262-02922-3. - ^ Randy Alfred, \"Nov. 7, 1905: Remote Control Wows Public\", Wired, 7 November 2011. - ^ Williams, Andrew (16 March 2017). History of Digital Games: Developments in Art, Design and Interaction. CRC Press. ISBN 978-1-317-50381-1. - ^ Randell, Brian (October 1982). \"From Analytical Engine to Electronic Digital Computer: The Contributions of Ludgate, Torres, and Bush\". IEEE Annals of the History of Computing. 4 (4): 327–341. Bibcode:1982IAHC....4d.327R. doi:10.1109/MAHC.1982.10042. S2CID 1737953. - ^ L. Torres Quevedo. Ensayos sobre Automática – Su definicion. Extension teórica de sus aplicaciones, Revista de la Academia de Ciencias Exacta, Revista 12, pp. 391–418, 1914. - ^ Torres Quevedo, Leonardo. Automática: Complemento de la Teoría de las Máquinas, (pdf), pp. 575–583, Revista de Obras Públicas, 19 November 1914. - ^ L. Torres Quevedo. Essais sur l'Automatique – Sa définition. Etendue théorique de ses applications. Archived 2023-02-10 at the Wayback Machine, Revue Génerale des Sciences Pures et Appliquées, vol. 2, pp. 601–611, 1915. - ^ B. Randell. Essays on Automatics, The Origins of Digital Computers, pp. 89–107, 1982. - ^ Sabbatini PhD, Renato M. E. \"Sabbatini, RME: An Imitation of Life: The First Robots\". Archived from the original on 2009-07-20. Retrieved 2023-03-15. - ^ Waurzyniak, Patrick (2006). \"Masters of Manufacturing: Joseph F. Engelberger\". Society of Manufacturing Engineers. 137 (1). Archived from the original on 2011-11-09. - ^ \"Humanoid History -WABOT-\". www.humanoid.waseda.ac.jp. Archived from the original on 2017-09-01. Retrieved 2017-05-06. - ^ Zeghloul, Saïd; Laribi, Med Amine; Gazeau, Jean-Pierre (21 September 2015). Robotics and Mechatronics: Proceedings of the 4th IFToMM International Symposium on Robotics and Mechatronics. Springer. ISBN 978-3-319-22368-1. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books. - ^ \"Historical Android Projects\". androidworld.com. Archived from the original on 2005-11-25. Retrieved 2017-05-06. - ^ Robots: From Science Fiction to Technological Revolution Archived 2023-03-15 at the Wayback Machine, page 130 - ^ Duffy, Vincent G. (19 April 2016). Handbook of Digital Human Modeling: Research for Applied Ergonomics and Human Factors Engineering. CRC Press. ISBN 978-1-4200-6352-3. Archived from the original on 2023-03-15. Retrieved 2017-09-10 – via Google Books. - ^ \"KUKA Industrial Robot FAMULUS\". Archived from the original on 2009-02-20. Retrieved 2008-01-10. - ^ \"History of Industrial Robots\" (PDF). Archived from the original (PDF) on 2012-12-24. Retrieved 2012-10-27. - ^ R. J. Popplestone; A. P. Ambler; I. Bellos (1978). \"RAPT: A language for describing assemblies\". Industrial Robot. 5 (3): 131–137. doi:10.1108/eb004501. - ^ Bozinovski, S. (1994). \"Parallel programming for mobile robot control: Agent-based approach\". 14th International Conference on Distributed Computing Systems. pp. 202–208. doi:10.1109/ICDCS.1994.302412. ISBN 0-8186-5840-1. S2CID 27855786. Further reading [edit]- R. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN 978-0-13-781592-0. - McGaughey, Ewan (16 October 2019). \"Will robots automate your job away? Full employment, basic income, and economic democracy\". LawArXiv Papers. doi:10.31228/osf.io/udbj8. S2CID 243172487. SSRN 3044448. - Autor, David H. (1 August 2015). \"Why Are There Still So Many Jobs? The History and Future of Workplace Automation\". Journal of Economic Perspectives. 29 (3): 3–30. doi:10.1257/jep.29.3.3. hdl:1721.1/109476. - Tooze, Adam (6 June 2019). \"Democracy and Its Discontents\". The New York Review of Books. Vol. 66, no. 10. External links [edit]- IEEE Robotics and Automation Society - Investigation of social robots – Robots that mimic human behaviors and gestures. - Wired's guide to the '50 best robots ever', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).",
    "text_length": 97273,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:05.713572"
  },
  {
    "id": "page_10",
    "url": "https://en.wikipedia.org/wiki/AI_safety",
    "domain": "en.wikipedia.org",
    "title": "AI safety - Wikipedia",
    "text": "AI safety | Part of a series on | | Artificial intelligence (AI) | |---| AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.[1][2] Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[3] Motivations [edit]Scholars discuss current risks from critical systems failures,[4] bias,[5] and AI-enabled surveillance,[6] as well as emerging risks like technological unemployment, digital manipulation,[7] weaponization,[8] AI-enabled cyberattacks[9] and bioterrorism.[10] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[11] or from AI enabling perpetually stable dictatorships.[12] Existential safety [edit]Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[13] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[14] AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[15][16][17] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI.[15] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[18] History [edit]Risks from AI began to be seriously discussed at the start of the computer age: Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. — Norbert Wiener (1949)[19] In 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines.[20] From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[21] In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\"[22] at the Philosophy and Theory of Artificial Intelligence conference,[23] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[24] In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[25] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk,[26] Bill Gates,[27] and Stephen Hawking[28] to voice similar concerns. In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.[29] To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell. In the same year, a group of academics led by professor Stuart J. Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".[30] In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[31] which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.[32] In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.[33] In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".[34] In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness,[35] and assurance.[36] The following year, researchers organized a workshop at ICLR that focused on these problem areas.[37] In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[2] In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.[38] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[39] During the summit the intention to create the International Scientific Report on the Safety of Advanced AI[40] was announced. In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[41] In 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations.[42][43] Research focus [edit]AI safety research areas include robustness, monitoring, and alignment.[2][36] Robustness [edit]Adversarial robustness [edit]AI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\".[44] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[45] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[46][47][48] The image on the right is predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[45] Adversarial robustness is often associated with security.[49] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[50] Network intrusion[51] and malware[52] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors. Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[53] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[54] This issue can be addressed by improving the adversarial robustness of the reward model.[55] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[56] Large language models (LLMs) can be vulnerable to prompt injection[57] and model stealing,[58] and may be used to generate misinformation.[59] Prompt injection involves embedding instructions into prompts in order to bypass safety measures.[57] Monitoring [edit]Estimating uncertainty [edit]It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[60] ML models generally express confidence by outputting probabilities; however, they are often overconfident,[61] especially in situations that differ from those that they were trained to handle.[62] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct. Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[63] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[64] though a range of additional techniques are in use.[65][66] Detecting malicious use [edit]Scholars[8] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[67] manipulate public opinion,[68][69] or automate cyber attacks.[70] These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[71] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[72] Transparency [edit]Neural networks have often been described as black boxes,[73] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[74] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[75] It also raises debates in healthcare over whether statistically efficient but opaque models should be used.[76] One critical benefit of transparency is explainability.[77] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.[77] Another benefit is to reveal the cause of failures.[73] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[78] Transparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[79] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[80] Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[81] \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[82][83] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in Spider-Man costumes, sketches of Spider-Man, and the word 'spider'.[84] It also involves explaining connections between these neurons or 'circuits'.[85][86] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[87] \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[88] Detecting trojans [edit]Machine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[2] or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[89] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[90] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[91] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[56] A 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.[92] Alignment [edit]In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[93] It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[93][94] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[93][95] Advanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[93][96][97] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[98][99] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[100][101] Today, some of these issues affect existing commercial systems such as LLMs,[102][103][104] robots,[105] autonomous vehicles,[106] and social media recommendation engines.[102][97][107] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[108][95][94] Many prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned.[109][97] These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind.[110][111][112] These risks remain debated.[113] AI alignment is a subfield of AI safety, the study of how to build safe AI systems.[114][115] Other subfields of AI safety include robustness, monitoring, and capability control.[116] Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.[116] Alignment research has connections to interpretability research,[117][118] (adversarial) robustness,[119] anomaly detection, calibrated uncertainty,[117] formal verification,[120] preference learning,[121][122][123] safety-critical engineering,[124] game theory,[125] algorithmic fairness,[119][126] and social sciences.[127][128] Systemic safety and sociotechnical factors [edit]It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents.[129] Some scholars have suggested that this framework falls short.[129] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.[129] Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[129] In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[130] Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[2] Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.[131] Cyber defense [edit]Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[132] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[8] Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.[133] Improving institutional decision-making [edit]The advancement of AI in economic and military domains could precipitate unprecedented political challenges.[134] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[135] AI researchers have argued that AI technologies could also be used to assist decision-making.[2] For example, researchers are beginning to develop AI forecasting[136] and advisory systems.[137] Facilitating cooperation [edit]Many of the largest global threats (nuclear war,[138] climate change,[139] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[139] A salient AI cooperation challenge is avoiding a 'race to the bottom'.[140] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[141] and technical[142] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[143] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[143][131] In governance [edit]AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[135] Research [edit]In AI safety, local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions.[145] AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[146] Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,[147] weaponization,[148] disinformation,[149] surveillance,[150] and the concentration of power.[151] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[152] the availability of AI models,[153] and 'race to the bottom' dynamics.[140][154] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\".[141] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[155][156][157] A key challenge for these approaches is a lack of widely accepted standards, and ambiguity about what the methods would require,[158][159] as well as a lack of safety culture in the industry.[160] Efforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's Guardrails,[161] Llama Guard,[162] Preamble's customizable guardrails[163] and Claude's Constitution mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.[164] Philosophical perspectives [edit]The field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. Some have suggested that by embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries,[165] but those suggestions have been questioned, with other alternatives being suggested at more promising.[166] Government action [edit]Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\".[167][168] Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.[169] Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\".[170] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".[171] In September 2021, the People's Republic of China (PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy,[172] which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[173] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[173] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\".[174][175] China Media Project stated \"key aspects of its approach remain fundamentally unsafe by the standards of democratic societies worldwide\", arguing that part of China's AI safety approach is focused on strengthening the CCP's information control.[176] Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.[177] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks.[178][179] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[180] In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of \"safe, secure and trustworthy\" AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[181] In May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.[182] Corporate self-regulation [edit]AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[183] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[184] offering bounties for finding failures,[184] sharing AI incidents[184] (an AI incident database was created for this purpose),[185] following guidelines to determine whether to publish research or models,[153] and improving information and cyber security in AI labs.[186] Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse.[187] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\"[188] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[34] and the Autonomous Weapons Open Letter.[189] See also [edit]- AI alignment - Artificial intelligence and elections - Artificial intelligence detection software - Hallucination (artificial intelligence) References [edit]- ^ Ahmed, Shazeda; Jaźwińska, Klaudia; Ahlawat, Archana; Winecoff, Amy; Wang, Mona (2024-04-14). \"Field-building and the epistemic culture of AI safety\". First Monday. doi:10.5210/fm.v29i4.13626. ISSN 1396-0466. - ^ a b c d e f Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob (2022-06-16). \"Unsolved Problems in ML Safety\". arXiv:2109.13916. {{cite journal}} : Cite journal requires|journal= (help) - ^ Perrigo, Billy (2023-11-02). \"U.K.'s AI Safety Summit Ends With Limited, but Meaningful, Progress\". Time. Retrieved 2024-06-02. - ^ De-Arteaga, Maria (2020-05-13). Machine Learning in High-Stakes Settings: Risks and Opportunities (PhD). Carnegie Mellon University. - ^ Mehrabi, Ninareh; Morstatter, Fred; Saxena, Nripsuta; Lerman, Kristina; Galstyan, Aram (2021). \"A Survey on Bias and Fairness in Machine Learning\". ACM Computing Surveys. 54 (6): 1–35. arXiv:1908.09635. doi:10.1145/3457607. ISSN 0360-0300. S2CID 201666566. Archived from the original on 2022-11-23. Retrieved 2022-11-28. - ^ Feldstein, Steven (2019). The Global Expansion of AI Surveillance (Report). Carnegie Endowment for International Peace. - ^ Barnes, Beth (2021). \"Risks from AI persuasion\". Lesswrong. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ a b c Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C; Steinhardt, Jacob; Flynn, Carrick (2018-04-30). \"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\". Apollo-University Of Cambridge Repository, Apollo-University Of Cambridge Repository. Apollo - University of Cambridge Repository. doi:10.17863/cam.22520. S2CID 3385567. Archived from the original on 2022-11-23. Retrieved 2022-11-28. {{cite journal}} : Cite journal requires|journal= (help) - ^ Davies, Pascale (December 26, 2022). \"How NATO is preparing for a new era of AI cyber attacks\". euronews. Retrieved 2024-03-23. - ^ Ahuja, Anjana (February 7, 2024). \"AI's bioterrorism potential should not be ruled out\". Financial Times. Retrieved 2024-03-23. - ^ Carlsmith, Joseph (2022-06-16). \"Is Power-Seeking AI an Existential Risk?\". arXiv:2206.13353. {{cite journal}} : Cite journal requires|journal= (help) - ^ Minardi, Di (16 October 2020). \"The grim fate that could be 'worse than extinction'\". BBC. Retrieved 2024-03-23. - ^ \"AGI Expert Peter Voss Says AI Alignment Problem is Bogus | NextBigFuture.com\". 2023-04-04. Retrieved 2023-07-23. - ^ Dafoe, Allan (2016). \"Yes, We Are Worried About the Existential Risk of Artificial Intelligence\". MIT Technology Review. Archived from the original on 2022-11-28. Retrieved 2022-11-28. - ^ a b Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (2018-07-31). \"Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts\". Journal of Artificial Intelligence Research. 62: 729–754. arXiv:1705.08807. doi:10.1613/jair.1.11222. ISSN 1076-9757. S2CID 8746462. Archived from the original on 2023-02-10. Retrieved 2022-11-28. - ^ Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan (2021-05-05). \"Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers\". Journal of Artificial Intelligence Research. 71. arXiv:2105.02117. doi:10.1613/jair.1.12895. - ^ Stein-Perlman, Zach; Weinstein-Raun, Benjamin; Grace (2022-08-04). \"2022 Expert Survey on Progress in AI\". AI Impacts. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Michael, Julian; Holtzman, Ari; Parrish, Alicia; Mueller, Aaron; Wang, Alex; Chen, Angelica; Madaan, Divyam; Nangia, Nikita; Pang, Richard Yuanzhe; Phang, Jason; Bowman, Samuel R. (2022-08-26). \"What Do NLP Researchers Believe? Results of the NLP Community Metasurvey\". Association for Computational Linguistics. arXiv:2208.12852. - ^ Markoff, John (2013-05-20). \"In 1949, He Imagined an Age of Robots\". The New York Times. ISSN 0362-4331. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Artificial intelligence: A handbook of professionalism. University of Sussex. January 1988. ISBN 978-0-470-21103-8. - ^ Association for the Advancement of Artificial Intelligence. \"AAAI Presidential Panel on Long-Term AI Futures\". Archived from the original on 2022-09-01. Retrieved 2022-11-23. - ^ Yampolskiy, Roman V.; Spellchecker, M. S. (2016-10-25). \"Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures\". arXiv:1610.07997. {{cite journal}} : Cite journal requires|journal= (help) - ^ \"PT-AI 2011 – Philosophy and Theory of Artificial Intelligence (PT-AI 2011)\". Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Yampolskiy, Roman V. (2013), Müller, Vincent C. (ed.), \"Artificial Intelligence Safety Engineering: Why Machine Ethics is a Wrong Approach\", Philosophy and Theory of Artificial Intelligence, Studies in Applied Philosophy, Epistemology and Rational Ethics, vol. 5, Berlin; Heidelberg, Germany: Springer Berlin Heidelberg, pp. 389–396, doi:10.1007/978-3-642-31674-6_29, ISBN 978-3-642-31673-9, archived from the original on 2023-03-15, retrieved 2022-11-23 - ^ McLean, Scott; Read, Gemma J. M.; Thompson, Jason; Baber, Chris; Stanton, Neville A.; Salmon, Paul M. (2023-07-04). \"The risks associated with Artificial General Intelligence: A systematic review\". Journal of Experimental & Theoretical Artificial Intelligence. 35 (5): 649–663. Bibcode:2023JETAI..35..649M. doi:10.1080/0952813X.2021.1964003. hdl:11343/289595. ISSN 0952-813X. S2CID 238643957. - ^ Wile, Rob (August 3, 2014). \"Elon Musk: Artificial Intelligence Is 'Potentially More Dangerous Than Nukes'\". Business Insider. Retrieved 2024-02-22. - ^ Kuo, Kaiser (2015-03-31). Baidu CEO Robin Li interviews Bill Gates and Elon Musk at the Boao Forum, March 29, 2015. Event occurs at 55:49. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Cellan-Jones, Rory (2014-12-02). \"Stephen Hawking warns artificial intelligence could end mankind\". BBC News. Archived from the original on 2015-10-30. Retrieved 2022-11-23. - ^ Future of Life Institute. \"Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter\". Future of Life Institute. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Future of Life Institute (October 2016). \"AI Research Grants Program\". Future of Life Institute. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ \"SafArtInt 2016\". Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Bach, Deborah (2016). \"UW to host first of four White House public workshops on artificial intelligence\". UW News. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (2016-07-25). \"Concrete Problems in AI Safety\". arXiv:1606.06565. {{cite journal}} : Cite journal requires|journal= (help) - ^ a b Future of Life Institute. \"AI Principles\". Future of Life Institute. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Yohsua, Bengio; Daniel, Privitera; Tamay, Besiroglu; Rishi, Bommasani; Stephen, Casper; Yejin, Choi; Danielle, Goldfarb; Hoda, Heidari; Leila, Khalatbari (May 2024). International Scientific Report on the Safety of Advanced AI (Report). Department for Science, Innovation and Technology. - ^ a b Research, DeepMind Safety (2018-09-27). \"Building safe artificial intelligence: specification, robustness, and assurance\". Medium. Archived from the original on 2023-02-10. Retrieved 2022-11-23. - ^ \"SafeML ICLR 2019 Workshop\". Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Browne, Ryan (2023-06-12). \"British Prime Minister Rishi Sunak pitches UK as home of A.I. safety regulation as London bids to be next Silicon Valley\". CNBC. Retrieved 2023-06-25. - ^ Bertuzzi, Luca (October 18, 2023). \"UK's AI safety summit set to highlight risk of losing human control over 'frontier' models\". Euractiv. Retrieved March 2, 2024. - ^ Bengio, Yoshua; Privitera, Daniel; Bommasani, Rishi; Casper, Stephen; Goldfarb, Danielle; Mavroudis, Vasilios; Khalatbari, Leila; Mazeika, Mantas; Hoda, Heidari (2024-05-17). \"International Scientific Report on the Safety of Advanced AI\" (PDF). GOV.UK. Archived (PDF) from the original on 2024-06-15. Retrieved 2024-07-08. Alt URL - ^ Shepardson, David (1 April 2024). \"US, Britain announce partnership on AI safety, testing\". Retrieved 2 April 2024. - ^ \"What International AI Safety report says on jobs, climate, cyberwar and more\". The Guardian. 2025-01-29. ISSN 0261-3077. Retrieved 2025-03-03. - ^ \"Launch of the First International Report on AI Safety chaired by Yoshua Bengio\". mila.quebec. January 29, 2025. Retrieved 2025-03-03. - ^ Goodfellow, Ian; Papernot, Nicolas; Huang, Sandy; Duan, Rocky; Abbeel, Pieter; Clark, Jack (2017-02-24). \"Attacking Machine Learning with Adversarial Examples\". OpenAI. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ a b Szegedy, Christian; Zaremba, Wojciech; Sutskever, Ilya; Bruna, Joan; Erhan, Dumitru; Goodfellow, Ian; Fergus, Rob (2014-02-19). \"Intriguing properties of neural networks\". ICLR. arXiv:1312.6199. - ^ Kurakin, Alexey; Goodfellow, Ian; Bengio, Samy (2017-02-10). \"Adversarial examples in the physical world\". ICLR. arXiv:1607.02533. - ^ Madry, Aleksander; Makelov, Aleksandar; Schmidt, Ludwig; Tsipras, Dimitris; Vladu, Adrian (2019-09-04). \"Towards Deep Learning Models Resistant to Adversarial Attacks\". ICLR. arXiv:1706.06083. - ^ Kannan, Harini; Kurakin, Alexey; Goodfellow, Ian (2018-03-16). \"Adversarial Logit Pairing\". arXiv:1803.06373. {{cite journal}} : Cite journal requires|journal= (help) - ^ Gilmer, Justin; Adams, Ryan P.; Goodfellow, Ian; Andersen, David; Dahl, George E. (2018-07-19). \"Motivating the Rules of the Game for Adversarial Example Research\". arXiv:1807.06732. {{cite journal}} : Cite journal requires|journal= (help) - ^ Carlini, Nicholas; Wagner, David (2018-03-29). \"Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\". IEEE Security and Privacy Workshops. arXiv:1801.01944. - ^ Sheatsley, Ryan; Papernot, Nicolas; Weisman, Michael; Verma, Gunjan; McDaniel, Patrick (2022-09-09). \"Adversarial Examples in Constrained Domains\". arXiv:2011.01183. {{cite journal}} : Cite journal requires|journal= (help) - ^ Suciu, Octavian; Coull, Scott E.; Johns, Jeffrey (2019-04-13). \"Exploring Adversarial Examples in Malware Detection\". IEEE Security and Privacy Workshops. arXiv:1810.08280. - ^ Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie (2022-03-04). \"Training language models to follow instructions with human feedback\". NeurIPS. arXiv:2203.02155. - ^ Gao, Leo; Schulman, John; Hilton, Jacob (2022-10-19). \"Scaling Laws for Reward Model Overoptimization\". ICML. arXiv:2210.10760. - ^ Yu, Sihyun; Ahn, Sungsoo; Song, Le; Shin, Jinwoo (2021-10-27). \"RoMA: Robust Model Adaptation for Offline Model-based Optimization\". NeurIPS. arXiv:2110.14188. - ^ a b Hendrycks, Dan; Mazeika, Mantas (2022-09-20). \"X-Risk Analysis for AI Research\". arXiv:2206.05862. {{cite journal}} : Cite journal requires|journal= (help) - ^ a b \"Prompt injection attacks might 'never be properly mitigated' UK NCSC warns\". TechRadar. 2025-12-09. Retrieved 2025-12-12. - ^ \"Why Anthropic and OpenAI are obsessed with securing LLM model weights\". VentureBeat. 2023-12-15. - ^ \"The rise of AI fake news is creating a 'misinformation superspreader'\". The Washington Post. 2023-12-17. ISSN 0190-8286. Retrieved 2025-12-12. - ^ Tran, Khoa A.; Kondrashova, Olga; Bradley, Andrew; Williams, Elizabeth D.; Pearson, John V.; Waddell, Nicola (2021). \"Deep learning in cancer diagnosis, prognosis and treatment selection\". Genome Medicine. 13 (1): 152. doi:10.1186/s13073-021-00968-x. ISSN 1756-994X. PMC 8477474. PMID 34579788. - ^ Guo, Chuan; Pleiss, Geoff; Sun, Yu; Weinberger, Kilian Q. (2017-08-06). \"On calibration of modern neural networks\". Proceedings of the 34th international conference on machine learning. Proceedings of machine learning research. Vol. 70. PMLR. pp. 1321–1330. - ^ Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper (2019-12-17). \"Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\". NeurIPS. arXiv:1906.02530. - ^ Bogdoll, Daniel; Breitenstein, Jasmin; Heidecker, Florian; Bieshaar, Maarten; Sick, Bernhard; Fingscheidt, Tim; Zöllner, J. Marius (2021). \"Description of Corner Cases in Automated Driving: Goals and Challenges\". 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). pp. 1023–1028. arXiv:2109.09607. doi:10.1109/ICCVW54120.2021.00119. ISBN 978-1-6654-0191-3. S2CID 237572375. - ^ Hendrycks, Dan; Mazeika, Mantas; Dietterich, Thomas (2019-01-28). \"Deep Anomaly Detection with Outlier Exposure\". ICLR. arXiv:1812.04606. - ^ Wang, Haoqi; Li, Zhizhong; Feng, Litong; Zhang, Wayne (2022-03-21). \"ViM: Out-Of-Distribution with Virtual-logit Matching\". CVPR. arXiv:2203.10807. - ^ Hendrycks, Dan; Gimpel, Kevin (2018-10-03). \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\". ICLR. arXiv:1610.02136. - ^ Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (2022). \"Dual use of artificial-intelligence-powered drug discovery\". Nature Machine Intelligence. 4 (3): 189–191. doi:10.1038/s42256-022-00465-9. ISSN 2522-5839. PMC 9544280. PMID 36211133. - ^ Center for Security and Emerging Technology; Buchanan, Ben; Lohn, Andrew; Musser, Micah; Sedova, Katerina (2021). \"Truth, Lies, and Automation: How Language Models Could Change Disinformation\". doi:10.51593/2021ca003. S2CID 240522878. Archived from the original on 2022-11-24. Retrieved 2022-11-28. {{cite journal}} : Cite journal requires|journal= (help) - ^ \"Propaganda-as-a-service may be on the horizon if large language models are abused\". VentureBeat. 2021-12-14. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Center for Security and Emerging Technology; Buchanan, Ben; Bansemer, John; Cary, Dakota; Lucas, Jack; Musser, Micah (2020). \"Automating Cyber Attacks: Hype and Reality\". Center for Security and Emerging Technology. doi:10.51593/2020ca002. S2CID 234623943. Archived from the original on 2022-11-24. Retrieved 2022-11-28. - ^ \"Lessons Learned on Language Model Safety and Misuse\". OpenAI. 2022-03-03. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Markov, Todor; Zhang, Chong; Agarwal, Sandhini; Eloundou, Tyna; Lee, Teddy; Adler, Steven; Jiang, Angela; Weng, Lilian (2022-08-10). \"New-and-Improved Content Moderation Tooling\". OpenAI. Archived from the original on 2023-01-11. Retrieved 2022-11-24. - ^ a b Savage, Neil (2022-03-29). \"Breaking into the black box of artificial intelligence\". Nature. doi:10.1038/d41586-022-00858-1. PMID 35352042. S2CID 247792459. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Center for Security and Emerging Technology; Rudner, Tim; Toner, Helen (2021). \"Key Concepts in AI Safety: Interpretability in Machine Learning\". CSET Issue Brief. doi:10.51593/20190042. S2CID 233775541. Archived from the original on 2022-11-24. Retrieved 2022-11-28. - ^ McFarland, Matt (2018-03-19). \"Uber pulls self-driving cars after first fatal crash of autonomous vehicle\". CNNMoney. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Felder, Ryan Marshall (July 2021). \"Coming to Terms with the Black Box Problem: How to Justify AI Systems in Health Care\". Hastings Center Report. 51 (4): 38–45. doi:10.1002/hast.1248. ISSN 0093-0334. PMID 33821471. - ^ a b Doshi-Velez, Finale; Kortz, Mason; Budish, Ryan; Bavitz, Chris; Gershman, Sam; O'Brien, David; Scott, Kate; Schieber, Stuart; Waldo, James; Weinberger, David; Weller, Adrian; Wood, Alexandra (2019-12-20). \"Accountability of AI Under the Law: The Role of Explanation\". arXiv:1711.01134. {{cite journal}} : Cite journal requires|journal= (help) - ^ Fong, Ruth; Vedaldi, Andrea (2017). \"Interpretable Explanations of Black Boxes by Meaningful Perturbation\". 2017 IEEE International Conference on Computer Vision (ICCV). pp. 3449–3457. arXiv:1704.03296. doi:10.1109/ICCV.2017.371. ISBN 978-1-5386-1032-9. S2CID 1633753. - ^ Meng, Kevin; Bau, David; Andonian, Alex; Belinkov, Yonatan (2022). \"Locating and editing factual associations in GPT\". Advances in Neural Information Processing Systems. 35. arXiv:2202.05262. - ^ Bau, David; Liu, Steven; Wang, Tongzhou; Zhu, Jun-Yan; Torralba, Antonio (2020-07-30). \"Rewriting a Deep Generative Model\". ECCV. arXiv:2007.15646. - ^ Räuker, Tilman; Ho, Anson; Casper, Stephen; Hadfield-Menell, Dylan (2022-09-05). \"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks\". IEEE SaTML. arXiv:2207.13243. - ^ Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio (2017-04-19). \"Network Dissection: Quantifying Interpretability of Deep Visual Representations\". CVPR. arXiv:1704.05796. - ^ McGrath, Thomas; Kapishnikov, Andrei; Tomašev, Nenad; Pearce, Adam; Wattenberg, Martin; Hassabis, Demis; Kim, Been; Paquet, Ulrich; Kramnik, Vladimir (2022-11-22). \"Acquisition of chess knowledge in AlphaZero\". Proceedings of the National Academy of Sciences. 119 (47) e2206625119. arXiv:2111.09259. Bibcode:2022PNAS..11906625M. doi:10.1073/pnas.2206625119. ISSN 0027-8424. PMC 9704706. PMID 36375061. - ^ Goh, Gabriel; Cammarata, Nick; Voss, Chelsea; Carter, Shan; Petrov, Michael; Schubert, Ludwig; Radford, Alec; Olah, Chris (2021). \"Multimodal neurons in artificial neural networks\". Distill. 6 (3). doi:10.23915/distill.00030. S2CID 233823418. - ^ Olah, Chris; Cammarata, Nick; Schubert, Ludwig; Goh, Gabriel; Petrov, Michael; Carter, Shan (2020). \"Zoom in: An introduction to circuits\". Distill. 5 (3). doi:10.23915/distill.00024.001. S2CID 215930358. - ^ Cammarata, Nick; Goh, Gabriel; Carter, Shan; Voss, Chelsea; Schubert, Ludwig; Olah, Chris (2021). \"Curve circuits\". Distill. 6 (1). doi:10.23915/distill.00024.006 (inactive 1 July 2025). Archived from the original on 5 December 2022. Retrieved 5 December 2022. {{cite journal}} : CS1 maint: DOI inactive as of July 2025 (link) - ^ Olsson, Catherine; Elhage, Nelson; Nanda, Neel; Joseph, Nicholas; DasSarma, Nova; Henighan, Tom; Mann, Ben; Askell, Amanda; Bai, Yuntao; Chen, Anna; Conerly, Tom; Drain, Dawn; Ganguli, Deep; Hatfield-Dodds, Zac; Hernandez, Danny; Johnston, Scott; Jones, Andy; Kernion, Jackson; Lovitt, Liane; Ndousse, Kamal; Amodei, Dario; Brown, Tom; Clark, Jack; Kaplan, Jared; McCandlish, Sam; Olah, Chris (2022). \"In-context learning and induction heads\". Transformer Circuits Thread. arXiv:2209.11895. - ^ Olah, Christopher. \"Interpretability vs Neuroscience [rough note]\". Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Gu, Tianyu; Dolan-Gavitt, Brendan; Garg, Siddharth (2019-03-11). \"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\". arXiv:1708.06733. {{cite journal}} : Cite journal requires|journal= (help) - ^ Chen, Xinyun; Liu, Chang; Li, Bo; Lu, Kimberly; Song, Dawn (2017-12-14). \"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning\". arXiv:1712.05526. {{cite journal}} : Cite journal requires|journal= (help) - ^ Carlini, Nicholas; Terzis, Andreas (2022-03-28). \"Poisoning and Backdooring Contrastive Learning\". ICLR. arXiv:2106.09667. - ^ \"How 'sleeper agent' AI assistants can sabotage code\". The Register. 16 January 2024. Archived from the original on 2024-12-24. Retrieved 2025-01-12. - ^ a b c d Russell, Stuart J.; Norvig, Peter (2021). Artificial intelligence: A modern approach (4th ed.). Pearson. pp. 5, 1003. ISBN 978-0-13-461099-3. Retrieved September 12, 2022. - ^ a b Ngo, Richard; Chan, Lawrence; Mindermann, Sören (2022). \"The Alignment Problem from a Deep Learning Perspective\". International Conference on Learning Representations. arXiv:2209.00626. - ^ a b Pan, Alexander; Bhatia, Kush; Steinhardt, Jacob (2022-02-14). The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models. International Conference on Learning Representations. Retrieved 2022-07-21. - ^ Carlsmith, Joseph (2022-06-16). \"Is Power-Seeking AI an Existential Risk?\". arXiv:2206.13353 [cs.CY]. - ^ a b c Russell, Stuart J. (2020). Human compatible: Artificial intelligence and the problem of control. Penguin Random House. ISBN 978-0-525-55863-7. OCLC 1113410915. - ^ Christian, Brian (2020). The alignment problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753. Archived from the original on February 10, 2023. Retrieved September 12, 2022. - ^ Langosco, Lauro Langosco Di; Koch, Jack; Sharkey, Lee D.; Pfau, Jacob; Krueger, David (2022-06-28). \"Goal Misgeneralization in Deep Reinforcement Learning\". Proceedings of the 39th International Conference on Machine Learning. International Conference on Machine Learning. PMLR. pp. 12004–12019. Retrieved 2023-03-11. - ^ Pillay, Tharin (2024-12-15). \"New Tests Reveal AI's Capacity for Deception\". TIME. Retrieved 2025-01-12. - ^ Perrigo, Billy (2024-12-18). \"Exclusive: New Research Shows AI Strategically Lying\". TIME. Retrieved 2025-01-12. - ^ a b Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik (2022-07-12). \"On the Opportunities and Risks of Foundation Models\". Stanford CRFM. arXiv:2108.07258. - ^ Ouyang, Long; et al. (2022). \"Training language models to follow instructions with human feedback\" (PDF). NeurIPS. arXiv:2203.02155. - ^ Zaremba, Wojciech; Brockman, Greg; OpenAI (2021-08-10). \"OpenAI Codex\". OpenAI. Archived from the original on February 3, 2023. Retrieved 2022-07-23. - ^ Kober, Jens; Bagnell, J. Andrew; Peters, Jan (2013-09-01). \"Reinforcement learning in robotics: A survey\". The International Journal of Robotics Research. 32 (11): 1238–1274. doi:10.1177/0278364913495721. ISSN 0278-3649. S2CID 1932843. Archived from the original on October 15, 2022. Retrieved September 12, 2022. - ^ Knox, W. Bradley; Allievi, Alessandro; Banzhaf, Holger; Schmitt, Felix; Stone, Peter (2023-03-01). \"Reward (Mis)design for autonomous driving\". Artificial Intelligence. 316 103829. arXiv:2104.13906. doi:10.1016/j.artint.2022.103829. ISSN 0004-3702. S2CID 233423198. - ^ Stray, Jonathan (2020). \"Aligning AI Optimization to Community Well-Being\". International Journal of Community Well-Being. 3 (4): 443–463. doi:10.1007/s42413-020-00086-3. ISSN 2524-5295. PMC 7610010. PMID 34723107. S2CID 226254676. - ^ Russell, Stuart; Norvig, Peter (2009). Artificial Intelligence: A Modern Approach. Prentice Hall. p. 1003. ISBN 978-0-13-461099-3. - ^ Smith, Craig S. \"Geoff Hinton, AI's Most Famous Researcher, Warns Of 'Existential Threat'\". Forbes. Retrieved 2023-05-04. - ^ Bengio, Yoshua; Hinton, Geoffrey; Yao, Andrew; Song, Dawn; Abbeel, Pieter; Harari, Yuval Noah; Zhang, Ya-Qin; Xue, Lan; Shalev-Shwartz, Shai (2024). \"Managing extreme AI risks amid rapid progress\". Science. 384 (6698): 842–845. arXiv:2310.17688. Bibcode:2024Sci...384..842B. doi:10.1126/science.adn0117. PMID 38768279. - ^ \"Statement on AI Risk | CAIS\". www.safe.ai. Retrieved 2024-02-11. - ^ Grace, Katja; Stewart, Harlan; Sandkühler, Julia Fabienne; Thomas, Stephen; Weinstein-Raun, Ben; Brauner, Jan (2025). \"Thousands of AI Authors on the Future of AI\". Journal of Artificial Intelligence Research. 84. arXiv:2401.02843. doi:10.1613/jair.1.19087. - ^ Perrigo, Billy (2024-02-13). \"Meta's AI Chief Yann LeCun on AGI, Open-Source, and AI Risk\". TIME. Retrieved 2024-06-26. - ^ \"What is AI alignment?\". TechTarget. 2023-05-03. Retrieved 2025-06-28. - ^ Ahmed, Shazeda; Jaźwińska, Klaudia; Ahlawat, Archana; Winecoff, Amy; Wang, Mona (2024-04-14). \"Field-building and the epistemic culture of AI safety\". First Monday. doi:10.5210/fm.v29i4.13626. ISSN 1396-0466. - ^ a b Ortega, Pedro A.; Maini, Vishal; DeepMind safety team (2018-09-27). \"Building safe artificial intelligence: specification, robustness, and assurance\". DeepMind Safety Research – Medium. Archived from the original on February 10, 2023. Retrieved 2022-07-18. - ^ a b Rorvig, Mordechai (2022-04-14). \"Researchers Gain New Understanding From Simple AI\". Quanta Magazine. Archived from the original on February 10, 2023. Retrieved 2022-07-18. - ^ Doshi-Velez, Finale; Kim, Been (2017-03-02). \"Towards A Rigorous Science of Interpretable Machine Learning\". arXiv:1702.08608 [stat.ML]. - Wiblin, Robert (August 4, 2021). \"Chris Olah on what the hell is going on inside neural networks\" (Podcast). 80,000 hours. No. 107. Retrieved 2022-07-23. - ^ a b Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (2016-06-21). \"Concrete Problems in AI Safety\". arXiv:1606.06565 [cs.AI]. - ^ Russell, Stuart; Dewey, Daniel; Tegmark, Max (2015-12-31). \"Research Priorities for Robust and Beneficial Artificial Intelligence\". AI Magazine. 36 (4): 105–114. arXiv:1602.03506. doi:10.1609/aimag.v36i4.2577. hdl:1721.1/108478. ISSN 2371-9621. S2CID 8174496. Archived from the original on February 2, 2023. Retrieved September 12, 2022. - ^ Wirth, Christian; Akrour, Riad; Neumann, Gerhard; Fürnkranz, Johannes (2017). \"A survey of preference-based reinforcement learning methods\". Journal of Machine Learning Research. 18 (136): 1–46. - ^ Christiano, Paul F.; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario (2017). \"Deep reinforcement learning from human preferences\". Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS'17. Red Hook, NY, USA: Curran Associates Inc. pp. 4302–4310. ISBN 978-1-5108-6096-4. - ^ Heaven, Will Douglas (2022-01-27). \"The new version of GPT-3 is much better behaved (and should be less toxic)\". MIT Technology Review. Archived from the original on February 10, 2023. Retrieved 2022-07-18. - ^ Mohseni, Sina; Wang, Haotao; Yu, Zhiding; Xiao, Chaowei; Wang, Zhangyang; Yadawa, Jay (2022-03-07). \"Taxonomy of Machine Learning Safety: A Survey and Primer\". ACM Computing Surveys. 55 (8): 1–38. doi:10.1145/3551385. - ^ Clifton, Jesse (2020). \"Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda\". Center on Long-Term Risk. Archived from the original on January 1, 2023. Retrieved 2022-07-18. - Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore (2021-05-06). \"Cooperative AI: machines must learn to find common ground\". Nature. 593 (7857): 33–36. Bibcode:2021Natur.593...33D. doi:10.1038/d41586-021-01170-0. ISSN 0028-0836. PMID 33947992. S2CID 233740521. Archived from the original on December 18, 2022. Retrieved September 12, 2022. - ^ Prunkl, Carina; Whittlestone, Jess (2020-02-07). \"Beyond Near- and Long-Term\". Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. New York NY USA: ACM. pp. 138–143. doi:10.1145/3375627.3375803. ISBN 978-1-4503-7110-0. S2CID 210164673. Archived from the original on October 16, 2022. Retrieved September 12, 2022. - ^ Irving, Geoffrey; Askell, Amanda (2019-02-19). \"AI Safety Needs Social Scientists\". Distill. 4 (2) 10.23915/distill.00014. doi:10.23915/distill.00014. ISSN 2476-0757. S2CID 159180422. Archived from the original on February 10, 2023. Retrieved September 12, 2022. - ^ Gazos, Alexandros; Kahn, James; Kusche, Isabel; Büscher, Christian; Götz, Markus (2025-04-01). \"Organising AI for safety: Identifying structural vulnerabilities to guide the design of AI-enhanced socio-technical systems\". Safety Science. 184 106731. doi:10.1016/j.ssci.2024.106731. ISSN 0925-7535. - ^ a b c d Zwetsloot, Remco; Dafoe, Allan (2019-02-11). \"Thinking About Risks From AI: Accidents, Misuse and Structure\". Lawfare. Archived from the original on 2023-08-19. Retrieved 2022-11-24. - ^ Zhang, Yingyu; Dong, Chuntong; Guo, Weiqun; Dai, Jiabao; Zhao, Ziming (2022). \"Systems theoretic accident model and process (STAMP): A literature review\". Safety Science. 152 105596. doi:10.1016/j.ssci.2021.105596. S2CID 244550153. Archived from the original on 2023-03-15. Retrieved 2022-11-28. - ^ a b Gazos, Alexandros; Kahn, James; Kusche, Isabel; Büscher, Christian; Götz, Markus (2025-04-01). \"Organising AI for safety: Identifying structural vulnerabilities to guide the design of AI-enhanced socio-technical systems\". Safety Science. 184 106731. doi:10.1016/j.ssci.2024.106731. ISSN 0925-7535. - ^ Center for Security and Emerging Technology; Hoffman, Wyatt (2021). \"AI and the Future of Cyber Competition\". CSET Issue Brief. doi:10.51593/2020ca007. S2CID 234245812. Archived from the original on 2022-11-24. Retrieved 2022-11-28. - ^ Gafni, Ruti; Levy, Yair (2024-01-01). \"The role of artificial intelligence (AI) in improving technical and managerial cybersecurity tasks' efficiency\". Information & Computer Security. 32 (5): 711–728. doi:10.1108/ICS-04-2024-0102. ISSN 2056-4961. - ^ Center for Security and Emerging Technology; Imbrie, Andrew; Kania, Elsa (2019). \"AI Safety, Security, and Stability Among Great Powers: Options, Challenges, and Lessons Learned for Pragmatic Engagement\". doi:10.51593/20190051. S2CID 240957952. Archived from the original on 2022-11-24. Retrieved 2022-11-28. {{cite journal}} : Cite journal requires|journal= (help) - ^ a b Future of Life Institute (2019-03-27). AI Strategy, Policy, and Governance (Allan Dafoe). Event occurs at 22:05. Archived from the original on 2022-11-23. Retrieved 2022-11-23. - ^ Zou, Andy; Xiao, Tristan; Jia, Ryan; Kwon, Joe; Mazeika, Mantas; Li, Richard; Song, Dawn; Steinhardt, Jacob; Evans, Owain; Hendrycks, Dan (2022-10-09). \"Forecasting Future World Events with Neural Networks\". NeurIPS. arXiv:2206.15474. - ^ Gathani, Sneha; Hulsebos, Madelon; Gale, James; Haas, Peter J.; Demiralp, Çağatay (2022-02-08). \"Augmenting Decision Making via Interactive What-If Analysis\". Conference on Innovative Data Systems Research. arXiv:2109.06160. - ^ Lindelauf, Roy (2021), Osinga, Frans; Sweijs, Tim (eds.), \"Nuclear Deterrence in the Algorithmic Age: Game Theory Revisited\", NL ARMS Netherlands Annual Review of Military Studies 2020, Nl Arms, The Hague: T.M.C. Asser Press, pp. 421–436, doi:10.1007/978-94-6265-419-8_22, ISBN 978-94-6265-418-1, S2CID 229449677 - ^ a b Newkirk II, Vann R. (2016-04-21). \"Is Climate Change a Prisoner's Dilemma or a Stag Hunt?\". The Atlantic. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ a b Armstrong, Stuart; Bostrom, Nick; Shulman, Carl. Racing to the Precipice: a Model of Artificial Intelligence Development (Report). Future of Humanity Institute, Oxford University. - ^ a b Dafoe, Allan. AI Governance: A Research Agenda (Report). Centre for the Governance of AI, Future of Humanity Institute, University of Oxford. - ^ Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore (2020-12-15). \"Open Problems in Cooperative AI\". NeurIPS. arXiv:2012.08630. - ^ a b Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore (2021). \"Cooperative AI: machines must learn to find common ground\". Nature. 593 (7857): 33–36. Bibcode:2021Natur.593...33D. doi:10.1038/d41586-021-01170-0. PMID 33947992. S2CID 233740521. Archived from the original on 2022-11-22. Retrieved 2022-11-24. - ^ Satariano, Adam; Specia, Megan (2023-11-01). \"Global Leaders Warn A.I. Could Cause 'Catastrophic' Harm\". The New York Times. ISSN 0362-4331. Retrieved 2024-04-20. - ^ Turchin, Alexey; Dench, David; Green, Brian Patrick (2019). \"Global Solutions vs. Local Solutions for the AI Safety Problem\". Big Data and Cognitive Computing. 3 (16): 1–25. doi:10.3390/bdcc3010016. - ^ Crafts, Nicholas (2021-09-23). \"Artificial intelligence as a general-purpose technology: an historical perspective\". Oxford Review of Economic Policy. 37 (3): 521–536. doi:10.1093/oxrep/grab012. ISSN 0266-903X. Archived from the original on 2022-11-24. Retrieved 2022-11-28. - ^ 葉俶禎; 黃子君; 張媁雯; 賴志樫 (2020-12-01). \"Labor Displacement in Artificial Intelligence Era: A Systematic Literature Review\". 臺灣東亞文明研究學刊. 17 (2). doi:10.6163/TJEAS.202012_17(2).0002. ISSN 1812-6243. - ^ Johnson, James (2019-04-03). \"Artificial intelligence & future warfare: implications for international security\". Defense & Security Analysis. 35 (2): 147–169. doi:10.1080/14751798.2019.1600800. ISSN 1475-1798. S2CID 159321626. Archived from the original on 2022-11-24. Retrieved 2022-11-28. - ^ Kertysova, Katarina (2018-12-12). \"Artificial Intelligence and Disinformation: How AI Changes the Way Disinformation is Produced, Disseminated, and Can Be Countered\". Security and Human Rights. 29 (1–4): 55–81. doi:10.1163/18750230-02901005. ISSN 1874-7337. S2CID 216896677. - ^ Feldstein, Steven (2019). The Global Expansion of AI Surveillance. Carnegie Endowment for International Peace. - ^ Agrawal, Ajay; Gans, Joshua; Goldfarb, Avi (2019). The economics of artificial intelligence: an agenda. Chicago, Illinois. ISBN 978-0-226-61347-5. OCLC 1099435014. {{cite book}} : CS1 maint: location missing publisher (link) - ^ Whittlestone, Jess; Clark, Jack (2021-08-31). \"Why and How Governments Should Monitor AI Development\". arXiv:2108.12427. {{cite journal}} : Cite journal requires|journal= (help) - ^ a b Shevlane, Toby (2022). \"Sharing Powerful AI Models | GovAI Blog\". Center for the Governance of AI. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Askell, Amanda; Brundage, Miles; Hadfield, Gillian (2019-07-10). \"The Role of Cooperation in Responsible AI Development\". arXiv:1907.04534. {{cite journal}} : Cite journal requires|journal= (help) - ^ Gursoy, Furkan; Kakadiaris, Ioannis A. (2022-08-31), System Cards for AI-Based Decision-Making for Public Policy, arXiv:2203.04754 - ^ Cobbe, Jennifer; Lee, Michelle Seng Ah; Singh, Jatinder (2021-03-01). \"Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems\". Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. New York, NY, USA: Association for Computing Machinery. pp. 598–609. doi:10.1145/3442188.3445921. ISBN 978-1-4503-8309-7. - ^ Raji, Inioluwa Deborah; Smart, Andrew; White, Rebecca N.; Mitchell, Margaret; Gebru, Timnit; Hutchinson, Ben; Smith-Loud, Jamila; Theron, Daniel; Barnes, Parker (2020-01-27). \"Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing\". Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. FAT* '20. New York, NY, USA: Association for Computing Machinery. pp. 33–44. doi:10.1145/3351095.3372873. ISBN 978-1-4503-6936-7. - ^ Manheim, David; Martin, Sammy; Bailey, Mark; Samin, Mikhail; Greutzmacher, Ross (2025). \"The necessity of AI audit standards boards\". AI & Society. 40 (8): 6609–6624. arXiv:2404.13060. doi:10.1007/s00146-025-02320-y. - ^ Novelli, Claudio; Taddeo, Mariarosaria; Floridi, Luciano (2024). \"Accountability in artificial intelligence: what it is and how it works\". AI & Society. 39 (4): 1871–1882. doi:10.1007/s00146-023-01635-y. hdl:11585/914099. - ^ Manheim, David (26 June 2023). \"Building a Culture of Safety for AI: Perspectives and Challenges\". SSRN 4491421. - ^ \"NeMo Guardrails\". NVIDIA NeMo Guardrails. Retrieved 2024-12-08. - ^ \"Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations\". Meta AI. Retrieved 2024-12-08. - ^ Šekrst, Kristina; McHugh, Jeremy; Cefalu, Jonathan Rodriguez (2024). \"AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development\". arXiv:2411.14442 [cs.CY]. - ^ Dong, Yi; Mu, Ronghui; Jin, Gaojie; Qi, Yi; Hu, Jinwei; Zhao, Xingyu; Meng, Jie; Ruan, Wenjie; Huang, Xiaowei (2024). \"Building Guardrails for Large Language Models\". arXiv:2402.01822 [cs]. - ^ D'Alessandro, W. (2024). \"Deontology and safe artificial intelligence\". Philosophical Studies. 182 (7): 1681–1704. doi:10.1007/s11098-024-02174-y. - ^ D'Alessandro, William; Kirk-Giannini, Chad D. (2025). \"Artificial Intelligence: Approaches to Safety\". Philosophy Compass. 20 (5) e70039. doi:10.1111/phc3.70039. - ^ Ziegler, Bart (8 April 2022). \"Is It Time to Regulate AI?\". Wall Street Journal. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Reed, Chris (2018-09-13). \"How should we regulate artificial intelligence?\". Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 376 (2128) 20170360. Bibcode:2018RSPTA.37670360R. doi:10.1098/rsta.2017.0360. ISSN 1364-503X. PMC 6107539. PMID 30082306. - ^ Belton, Keith B. (2019-03-07). \"How Should AI Be Regulated?\". IndustryWeek. Archived from the original on 2022-01-29. Retrieved 2022-11-24. - ^ National Security Commission on Artificial Intelligence (2021), Final Report - ^ National Institute of Standards and Technology (2021-07-12). \"AI Risk Management Framework\". NIST. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Richardson, Tim (2021). \"Britain publishes 10-year National Artificial Intelligence Strategy\". Archived from the original on 2023-02-10. Retrieved 2022-11-24. - ^ a b \"Guidance: National AI Strategy\". GOV.UK. 2021. Archived from the original on 2023-02-10. Retrieved 2022-11-24. - ^ Hardcastle, Kimberley (2023-08-23). \"We're talking about AI a lot right now – and it's not a moment too soon\". The Conversation. Retrieved 2023-10-31. - ^ \"Iconic Bletchley Park to host UK AI Safety Summit in early November\". GOV.UK. Retrieved 2023-10-31. - ^ Colville, Alex (2025-07-30). \"How China Sees AI Safety\". China Media Project. Retrieved 2025-08-09. - ^ Office of the Director of National Intelligence, Intelligence Advanced Research Projects Activity. \"IARPA – TrojAI\". Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Turek, Matt. \"Explainable Artificial Intelligence\". Archived from the original on 2021-02-19. Retrieved 2022-11-24. - ^ Draper, Bruce. \"Guaranteeing AI Robustness Against Deception\". Defense Advanced Research Projects Agency. Archived from the original on 2023-01-09. Retrieved 2022-11-24. - ^ National Science Foundation (23 February 2023). \"Safe Learning-Enabled Systems\". Archived from the original on 2023-02-26. Retrieved 2023-02-27. - ^ \"General Assembly adopts landmark resolution on artificial intelligence\". UN News. 21 March 2024. Archived from the original on 20 April 2024. Retrieved 21 April 2024. - ^ Say, Mark (23 May 2024). \"DSIT announces funding for research on AI safety\". Archived from the original on 24 May 2024. Retrieved 11 June 2024. - ^ Mäntymäki, Matti; Minkkinen, Matti; Birkstedt, Teemu; Viljanen, Mika (2022). \"Defining organizational AI governance\". AI and Ethics. 2 (4): 603–609. doi:10.1007/s43681-022-00143-x. ISSN 2730-5953. S2CID 247119668. - ^ a b c Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew (2020-04-20). \"Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims\". arXiv:2004.07213. {{cite journal}} : Cite journal requires|journal= (help) - ^ \"Welcome to the Artificial Intelligence Incident Database\". Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ Wiblin, Robert; Harris, Keiran (2022). \"Nova DasSarma on why information security may be critical to the safe development of AI systems\". 80,000 Hours. Archived from the original on 2022-11-24. Retrieved 2022-11-24. - ^ OpenAI (2022-06-02). \"Best Practices for Deploying Language Models\". OpenAI. Archived from the original on 2023-03-15. Retrieved 2022-11-24. - ^ OpenAI. \"OpenAI Charter\". OpenAI. Archived from the original on 2021-03-04. Retrieved 2022-11-24. - ^ Future of Life Institute (2016). \"Autonomous Weapons Open Letter: AI & Robotics Researchers\". Future of Life Institute. Retrieved 2022-11-24.",
    "text_length": 70109,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:07.720765"
  },
  {
    "id": "page_11",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "domain": "en.wikipedia.org",
    "title": "Machine learning - Wikipedia",
    "text": "Machine learning | Part of a series on | | Machine learning and data mining | |---| | Part of a series on | | Artificial intelligence (AI) | |---| Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[3][4] From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. History [edit]The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[5][6] The synonym self-teaching computers was also used during this time period.[7][8] The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[9] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[10] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[9] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[9] By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[11] A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[12] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[13] In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[14] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\"[15] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think?\", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\".[16] Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.[17] - Current Supervised Learning Algorithms have objectives of classification and regression. - Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. - Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis.[18] By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.[19] Relationships to other fields [edit]Artificial intelligence [edit]As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.[21] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[22]: 488 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[22]: 488 By 1980, expert systems had come to dominate AI, and statistics was out of favour.[23] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[22]: 708–710, 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[22]: 25 Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[23] Data compression [edit]There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[24][25][26] An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[27] According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form. Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[28] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[29] In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[30] Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[31] Large language models (LLMs) are also efficient lossless data compressors on some data sets, as demonstrated by DeepMind's research with the Chinchilla 70B model. Developed by DeepMind, Chinchilla 70B effectively compressed data, outperforming conventional methods such as Portable Network Graphics (PNG) for images and Free Lossless Audio Codec (FLAC) for audio. It achieved compression of image and audio data to 43.4% and 16.4% of their original sizes, respectively. There is, however, some reason to be concerned that the data set used for testing overlaps the LLM training data set, making it possible that the Chinchilla 70B model is only an efficient compression tool on data it has already been trained on.[32][33] Data mining [edit]Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.[citation needed] Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[34] Generalization [edit]Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. Statistics [edit]Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.[35] Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[36] Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[37] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[38] Statistical physics [edit]Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[39] Statistical physics is thus finding applications in the area of medical diagnostics.[40] Theory [edit]A core objective of a learner is to generalise from its experience.[2][41] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error. For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.[42] In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Approaches [edit] Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: - Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs. - Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). - Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.[2] Although each algorithm has advantages and limitations, no single algorithm works for all problems.[43][44][45] Supervised learning [edit]Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[46] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[47] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[15] Types of supervised-learning algorithms include active learning, classification and regression.[48] Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.[49] Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning [edit]Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[4] and density estimation.[50] Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[51][52] Dimensionality reduction [edit]Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[53] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. Semi-supervised learning [edit]Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54] Reinforcement learning [edit]Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types [edit]Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.[56] Self-learning [edit]Self-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[57][58] It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59] The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: - in situation s act a - receive a consequence situation s' - compute emotion of being in the consequence situation v(s') - update crossbar memory w'(a,s) = w(a,s) + v(s') It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations.[60] Feature learning [edit]Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation[62] and various forms of clustering.[63][64][65] Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data.[67] Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning [edit]Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image denoising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69] Anomaly detection [edit]In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71] In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72] Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model. Robot learning [edit]Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML). Association rules [edit]Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[76] Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner to make predictions.[79] Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. Models [edit]A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.[84] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85] Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks [edit]Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86] Decision trees [edit]Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. Random forest regression [edit]Random forest regression (RFR) falls under the umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-output data as well as multiple regressor tasks. This makes RFR compatible to be use in various applications.[87][88] Support-vector machines [edit]Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[89] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis [edit]Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[90]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[91] which are inherently multi-dimensional. Bayesian networks [edit]A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. Gaussian processes [edit]A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as a function of its input data can be directly computed by looking at the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation. Genetic algorithms [edit]A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[93][94] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[95] Belief functions [edit]The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities.[96] However, there are many caveats to these beliefs functions when compared to Bayesian approaches to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[97][6] However, the computational complexity of these algorithms is dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. Rule-based models [edit]Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems,[98] association rule learning,[99] artificial immune systems,[100] and other similar models. These methods extract patterns from data and evolve rules over time. Training models [edit]Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and, notably, becoming integrated within machine learning engineering teams. Federated learning [edit]Federated learning is an adapted form of distributed artificial intelligence to train machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[101] Applications [edit]There are many applications for machine learning, including: - Agriculture - Anatomy - Adaptive website - Affective computing - Astronomy - Automated decision-making - Banking - Behaviorism - Bioinformatics - Brain–machine interfaces - Cheminformatics - Citizen Science - Climate Science - Computer networks - Computer vision - Credit-card fraud detection - Data quality - DNA sequence classification - Economics - Financial data analysis[102] - General game playing - Handwriting recognition - Healthcare - Information retrieval - Insurance - Internet fraud detection - Investment management[103] - Knowledge graph embedding - Linguistics - Machine learning control - Machine perception - Machine translation - Material Engineering - Marketing - Medical diagnosis - Natural language processing - Natural language understanding - Online advertising - Optimisation - Recommender systems - Robot locomotion - Search engines - Sentiment analysis - Sequence mining - Software engineering - Speech recognition - Structural health monitoring - Syntactic pattern recognition - Telecommunications - Theorem proving - Time-series forecasting - Tomographic reconstruction[104] - User behaviour analytics In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[105] Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[106] In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis.[107] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[108] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.[109] In 2019 Springer Nature published the first research book created using machine learning.[110] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[111] Machine learning was recently applied to predict the pro-environmental behaviour of travellers.[112] Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.[113][114][115] When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[116] Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[117] Machine Learning is becoming a useful tool to investigate and predict evacuation decision-making in large-scale and small-scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[118][119][120] Other applications have been focusing on pre evacuation decisions in building fires.[121][122] Limitations [edit]Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[123][124][125] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[126] The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data.[127] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[127] In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[128] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[129][130] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[131] Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself.[132] Explainability [edit]Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[133] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[134] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting [edit]Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.[135] Other limitations and vulnerabilities [edit]Learners can also be disappointed by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[136] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[137][138] Adversarial vulnerabilities can also result in nonlinear systems or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[139] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[140] Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[141][142][143] Model assessments [edit]Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[144] In addition to overall accuracy, investigators frequently report sensitivity and specificity, meaning true positive rate (TPR) and true negative rate (TNR), respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC), along with the accompanying Area Under the ROC Curve (AUC), offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[145] Ethics [edit] The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[146] This includes algorithmic biases, fairness, accountability, transparency, privacy, and regulation, particularly where systems influence or automate human decision-making. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[147] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[146] Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military. Bias [edit]Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[148] Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.[149] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to either be women or have non-European-sounding names.[148] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[150][151] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[152] While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame the lack of participation and representation of minority populations in the field of AI for machine learning's vulnerability to biases.[153] In fact, according to research carried out by the Computing Research Association in 2021, \"female faculty make up just 16.1%\" of all faculty members who focus on AI among several universities around the world.[154] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[154] Language models learned from data have been shown to contain human-like biases.[155][156] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[157][158] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[159] In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".[152] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.[160] Similar issues with recognising non-white people have been found in many other systems.[161] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[162] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[163] Financial incentives [edit]There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[164] Hardware [edit]Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[165] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[166] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[167][168] Tensor Processing Units (TPUs) [edit]Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.[169] Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments. Neuromorphic computing [edit]Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.[170] Physical neural networks [edit]A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[171][172] Embedded machine learning [edit]Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[173][174][175][176] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[177][178] approximate computing,[179] and model optimisation.[180][181] Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. Software [edit]Software suites containing a variety of machine learning algorithms include the following: Free and open-source software [edit]- Caffe - Deeplearning4j - DeepSpeed - ELKI - Google JAX - Infer.NET - JASP - Jubatus - Keras - Kubeflow - LightGBM - Mahout - Mallet - Microsoft Cognitive Toolkit - ML.NET - mlpack - MXNet - OpenNN - Orange - pandas (software) - ROOT (TMVA with ROOT) - scikit-learn - Shogun - Spark MLlib - SystemML - Theano - TensorFlow - Torch / PyTorch - Weka / MOA - XGBoost - Yooreeka Proprietary software with free and open-source editions [edit]Proprietary software [edit]- Amazon Machine Learning - Angoss KnowledgeSTUDIO - Azure Machine Learning - IBM Watson Studio - Google Cloud Vertex AI - Google Prediction API - IBM SPSS Modeller - KXEN Modeller - LIONsolver - Mathematica - MATLAB - Neural Designer - NeuroSolutions - Oracle Data Mining - Oracle AI Platform Cloud Service - PolyAnalyst - RCASE - SAS Enterprise Miner - SequenceL - Splunk - STATISTICA Data Miner Journals [edit]- Journal of Machine Learning Research - Machine Learning - Nature Machine Intelligence - Neural Computation - IEEE Transactions on Pattern Analysis and Machine Intelligence Conferences [edit]- AAAI Conference on Artificial Intelligence - Association for Computational Linguistics (ACL) - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) - International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) - International Conference on Machine Learning (ICML) - International Conference on Learning Representations (ICLR) - International Conference on Intelligent Robots and Systems (IROS) - Conference on Knowledge Discovery and Data Mining (KDD) - Conference on Neural Information Processing Systems (NeurIPS) See also [edit]- Automated machine learning – Process of automating the application of machine learning - Big data – Extremely large or complex datasets - Deep learning — branch of ML concerned with artificial neural networks - Differentiable programming – Programming paradigm - List of datasets for machine-learning research - List of machine learning algorithms and List of algorithms for machine learning and statistical classification - M-theory (learning framework) – Framework in machine learning - Machine unlearning – Field of study in artificial intelligence - Outline of machine learning - Solomonoff's theory of inductive inference – Mathematical theory References [edit]- ^ The definition \"without being explicitly programmed\" is often attributed to Arthur Samuel, who coined the term \"machine learning\" in 1959, but the phrase is not found verbatim in this publication, and may be a paraphrase that appeared later. Confer \"Paraphrasing Arthur Samuel (1959), the question is: How can computers learn to solve problems without being explicitly programmed?\" in Koza, John R.; Bennett, Forrest H.; Andre, David; Keane, Martin A. (1996). \"Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming\". Artificial Intelligence in Design '96. Artificial Intelligence in Design '96. Dordrecht, Netherlands: Springer Netherlands. pp. 151–170. doi:10.1007/978-94-009-0279-4_9. ISBN 978-94-010-6610-5. - ^ a b c Bishop, C. M. (2006), Pattern Recognition and Machine Learning, Springer, ISBN 978-0-387-31073-2 - ^ Machine learning and pattern recognition \"can be viewed as two facets of the same field\".[2]: vii - ^ a b Friedman, Jerome H. (1998). \"Data Mining and Statistics: What's the connection?\". Computing Science and Statistics. 29 (1): 3–9. - ^ Samuel, Arthur (1959). \"Some Studies in Machine Learning Using the Game of Checkers\". IBM Journal of Research and Development. 3 (3): 210–229. CiteSeerX 10.1.1.368.2254. doi:10.1147/rd.33.0210. S2CID 2126705. - ^ a b R. Kohavi and F. Provost, \"Glossary of terms\", Machine Learning, vol. 30, no. 2–3, pp. 271–274, 1998. - ^ Gerovitch, Slava (9 April 2015). \"How the Computer Got Its Revenge on the Soviet Union\". Nautilus. Archived from the original on 22 September 2021. Retrieved 19 September 2021. - ^ Lindsay, Richard P. (1 September 1964). \"The Impact of Automation On Public Administration\". Western Political Quarterly. 17 (3): 78–81. doi:10.1177/106591296401700364. ISSN 0043-4078. S2CID 154021253. Archived from the original on 6 October 2021. Retrieved 6 October 2021. - ^ a b c \"History and Evolution of Machine Learning: A Timeline\". WhatIs. Archived from the original on 8 December 2023. Retrieved 8 December 2023. - ^ Milner, Peter M. (1993). \"The Mind and Donald O. Hebb\". Scientific American. 268 (1): 124–129. Bibcode:1993SciAm.268a.124M. doi:10.1038/scientificamerican0193-124. ISSN 0036-8733. JSTOR 24941344. PMID 8418480. - ^ \"Science: The Goof Button\", Time, 18 August 1961. - ^ Nilsson, Nils J. (1965). Learning Machines. McGraw-Hill. - ^ Duda, R., Hart P. Pattern Recognition and Scene Analysis, Wiley Interscience, 1973 - ^ S. Bozinovski, \"Teaching space: A representation concept for adaptive pattern classification\" COINS Technical Report No. 81-28, Computer and Information Science Department, University of Massachusetts at Amherst, MA, 1981. https://web.cs.umass.edu/publication/docs/1981/UM-CS-1981-028.pdf Archived 25 February 2021 at the Wayback Machine - ^ a b Mitchell, T. (1997). Machine Learning. McGraw Hill. p. 2. ISBN 978-0-07-042807-2. - ^ Harnad, Stevan (2008), \"The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence\", in Epstein, Robert; Peters, Grace (eds.), The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer, Kluwer, pp. 23–66, ISBN 978-1-4020-6708-2, archived from the original on 9 March 2012, retrieved 11 December 2012 - ^ \"Machine Learning Algorithms\". GeeksforGeeks. 17 August 2023. Retrieved 3 September 2025. - ^ Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi (2014). Generative adversarial nets (PDF). Advances in Neural Information Processing Systems 27 (2014). - ^ Silver, David; Huang, Aja; Maddison, Christopher J. (2016). \"Mastering the game of Go with deep neural networks and tree search\". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. PMID 26819042. - ^ Sindhu V, Nivedha S, Prakash M (February 2020). \"An Empirical Science Research on Bioinformatics in Machine Learning\". Journal of Mechanics of Continua and Mathematical Sciences (7). doi:10.26782/jmcms.spl.7/2020.02.00006. - ^ Sarle, Warren S. (1994). \"Neural Networks and statistical models\". SUGI 19: proceedings of the Nineteenth Annual SAS Users Group International Conference. SAS Institute. pp. 1538–50. ISBN 978-1-55544-611-6. OCLC 35546178. - ^ a b c d Russell, Stuart; Norvig, Peter (2003) [1995]. Artificial Intelligence: A Modern Approach (2nd ed.). Prentice Hall. ISBN 978-0137903955. - ^ a b Langley, Pat (2011). \"The changing science of machine learning\". Machine Learning. 82 (3): 275–9. doi:10.1007/s10994-011-5242-y. - ^ Mahoney, Matt. \"Rationale for a Large Text Compression Benchmark\". Florida Institute of Technology. Archived from the original on 18 August 2006. Retrieved 5 March 2013. - ^ Shmilovici A.; Kahiri Y.; Ben-Gal I.; Hauser S. (2009). \"Measuring the Efficiency of the Intraday Forex Market with a Universal Data Compression Algorithm\" (PDF). Computational Economics. 33 (2): 131–154. CiteSeerX 10.1.1.627.3751. doi:10.1007/s10614-008-9153-3. S2CID 17234503. Archived (PDF) from the original on 9 July 2009. - ^ Ben-Gal, I. (2008). \"On the Use of Data Compression Measures to Analyze Robust Designs\" (PDF). IEEE Transactions on Reliability. 54 (3): 381–388. doi:10.1109/TR.2005.853280. S2CID 9376086. Archived from the original (PDF) on 26 September 2020. Retrieved 6 April 2016. - ^ D. Scully; Carla E. Brodley (2006). \"Compression and Machine Learning: A New Perspective on Feature Space Vectors\". Data Compression Conference (DCC'06). p. 332. doi:10.1109/DCC.2006.13. ISBN 0-7695-2545-8. S2CID 12311412. - ^ Gary Adcock (5 January 2023). \"What Is AI Video Compression?\". massive.io. Retrieved 6 April 2023. - ^ Mentzer, Fabian; Toderici, George; Tschannen, Michael; Agustsson, Eirikur (2020). \"High-Fidelity Generative Image Compression\". arXiv:2006.09965 [eess.IV]. - ^ \"What is Unsupervised Learning? | IBM\". www.ibm.com. 23 September 2021. Retrieved 5 February 2024. - ^ \"Differentially private clustering for large-scale datasets\". blog.research.google. 25 May 2023. Retrieved 16 March 2024. - ^ Edwards, Benj (28 September 2023). \"AI language models can exceed PNG and FLAC in lossless compression, says study\". Ars Technica. Retrieved 7 March 2024. - ^ Delétang, Grégoire; Ruoss, Anian; Duquenne, Paul-Ambroise; Catt, Elliot; Genewein, Tim; Mattern, Christopher; Grau-Moya, Jordi; Li Kevin Wenliang; Aitchison, Matthew; Orseau, Laurent; Hutter, Marcus; Veness, Joel (2023). \"Language Modeling is Compression\". arXiv:2309.10668 [cs.LG]. - ^ Le Roux, Nicolas; Bengio, Yoshua; Fitzgibbon, Andrew (2012). \"Improving First and Second-Order Methods by Modeling Uncertainty\". In Sra, Suvrit; Nowozin, Sebastian; Wright, Stephen J. (eds.). Optimization for Machine Learning. MIT Press. p. 404. ISBN 978-0-262-01646-9. Archived from the original on 17 January 2023. Retrieved 12 November 2020. - ^ Bzdok, Danilo; Altman, Naomi; Krzywinski, Martin (2018). \"Statistics versus Machine Learning\". Nature Methods. 15 (4): 233–234. doi:10.1038/nmeth.4642. PMC 6082636. PMID 30100822. - ^ Hung et al. Algorithms to Measure Surgeon Performance and Anticipate Clinical Outcomes in Robotic Surgery. JAMA Surg. 2018 - ^ Cornell University Library (August 2001). \"Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)\". Statistical Science. 16 (3). doi:10.1214/ss/1009213726. S2CID 62729017. Archived from the original on 26 June 2017. Retrieved 8 August 2015. - ^ Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). An Introduction to Statistical Learning. Springer. p. vii. Archived from the original on 23 June 2019. Retrieved 25 October 2014. - ^ Ramezanpour, A.; Beam, A.L.; Chen, J.H.; Mashaghi, A. (17 November 2020). \"Statistical Physics for Medical Diagnostics: Learning, Inference, and Optimization Algorithms\". Diagnostics. 10 (11): 972. doi:10.3390/diagnostics10110972. PMC 7699346. PMID 33228143. - ^ Mashaghi, A.; Ramezanpour, A. (16 March 2018). \"Statistical physics of medical diagnostics: Study of a probabilistic model\". Physical Review E. 97 (3–1) 032118. arXiv:1803.10019. Bibcode:2018PhRvE..97c2118M. doi:10.1103/PhysRevE.97.032118. PMID 29776109. S2CID 4955393. - ^ Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning. US, Massachusetts: MIT Press. ISBN 9780262018258. - ^ Alpaydin, Ethem (2010). Introduction to Machine Learning. London: The MIT Press. ISBN 978-0-262-01243-0. Retrieved 4 February 2017. - ^ Jordan, M. I.; Mitchell, T. M. (17 July 2015). \"Machine learning: Trends, perspectives, and prospects\". Science. 349 (6245): 255–260. Bibcode:2015Sci...349..255J. doi:10.1126/science.aaa8415. PMID 26185243. S2CID 677218. - ^ El Naqa, Issam; Murphy, Martin J. (2015). \"What is Machine Learning?\". Machine Learning in Radiation Oncology. pp. 3–11. doi:10.1007/978-3-319-18305-3_1. ISBN 978-3-319-18304-6. S2CID 178586107. - ^ Okolie, Jude A.; Savage, Shauna; Ogbaga, Chukwuma C.; Gunes, Burcu (June 2022). \"Assessing the potential of machine learning methods to study the removal of pharmaceuticals from wastewater using biochar or activated carbon\". Total Environment Research Themes. 1–2 100001. Bibcode:2022TERT....100001O. doi:10.1016/j.totert.2022.100001. S2CID 249022386. - ^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (Third ed.). Prentice Hall. ISBN 978-0-13-604259-4. - ^ Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning. The MIT Press. ISBN 978-0-262-01825-8. - ^ Alpaydin, Ethem (2010). Introduction to Machine Learning. MIT Press. p. 9. ISBN 978-0-262-01243-0. Archived from the original on 17 January 2023. Retrieved 25 November 2018. - ^ De Sa, Christopher (Spring 2022). \"Lecture 2 Notes: Supervised Learning\". Cornell: Computer Science. Retrieved 1 July 2024. - ^ Jordan, Michael I.; Bishop, Christopher M. (2004). \"Neural Networks\". In Allen B. Tucker (ed.). Computer Science Handbook, Second Edition (Section VII: Intelligent Systems). Boca Raton, Florida: Chapman & Hall/CRC Press LLC. ISBN 978-1-58488-360-9. - ^ Misra, Ishan; Maaten, Laurens van der (2020). Self-Supervised Learning of Pretext-Invariant Representations. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, US: IEEE. pp. 6707–6717. arXiv:1912.01991. doi:10.1109/CVPR42600.2020.00674. - ^ Jaiswal, Ashish; Babu, Ashwin Ramesh; Zadeh, Mohammad Zaki; Banerjee, Debapriya; Makedon, Fillia (March 2021). \"A Survey on Contrastive Self-Supervised Learning\". Technologies. 9 (1): 2. arXiv:2011.00362. doi:10.3390/technologies9010002. ISSN 2227-7080. - ^ Roweis, Sam T.; Saul, Lawrence K. (22 December 2000). \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\". Science. 290 (5500): 2323–2326. Bibcode:2000Sci...290.2323R. doi:10.1126/science.290.5500.2323. PMID 11125150. S2CID 5987139. Archived from the original on 15 August 2021. Retrieved 17 July 2023. - ^ Alex Ratner; Stephen Bach; Paroma Varma; Chris. \"Weak Supervision: The New Programming Paradigm for Machine Learning\". hazyresearch.github.io. referencing work by many other members of Hazy Research. Archived from the original on 6 June 2019. Retrieved 6 June 2019. - ^ van Otterlo, M.; Wiering, M. (2012). \"Reinforcement Learning and Markov Decision Processes\". Reinforcement Learning. Adaptation, Learning, and Optimization. Vol. 12. pp. 3–42. doi:10.1007/978-3-642-27645-3_1. ISBN 978-3-642-27644-6. - ^ Pavel Brazdil; Christophe Giraud Carrier; Carlos Soares; Ricardo Vilalta (2009). Metalearning: Applications to Data Mining (Fourth ed.). Springer Science+Business Media. pp. 10–14, passim. ISBN 978-3-540-73262-4. - ^ Bozinovski, S. (1982). \"A self-learning system using secondary reinforcement\". In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North-Holland. pp. 397–402. ISBN 978-0-444-86488-8. - ^ Bozinovski, S. (1999) \"Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem\" In A. Dobnikar, N. Steele, D. Pearson, R. Albert (eds.) Artificial Neural Networks and Genetic Algorithms, Springer Verlag, p. 320–325, ISBN 3-211-83364-1 - ^ Bozinovski, Stevo (2014) \"Modeling mechanisms of cognition-emotion interaction in artificial neural networks, since 1981.\" Procedia Computer Science p. 255–263 - ^ Bozinovski, S. (2001) \"Self-learning agents: A connectionist theory of emotion based on crossbar value judgment.\" Cybernetics and Systems 32(6) 637–667. - ^ Y. Bengio; A. Courville; P. Vincent (2013). \"Representation Learning: A Review and New Perspectives\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. Bibcode:2013ITPAM..35.1798B. doi:10.1109/tpami.2013.50. PMID 23787338. S2CID 393948. - ^ Nathan Srebro; Jason D. M. Rennie; Tommi S. Jaakkola (2004). Maximum-Margin Matrix Factorization. NIPS. - ^ Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). An analysis of single-layer networks in unsupervised feature learning (PDF). Int'l Conf. on AI and Statistics (AISTATS). Archived from the original (PDF) on 13 August 2017. Retrieved 25 November 2018. - ^ Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004). Visual categorization with bags of keypoints (PDF). ECCV Workshop on Statistical Learning in Computer Vision. Archived (PDF) from the original on 13 July 2019. Retrieved 29 August 2019. - ^ Daniel Jurafsky; James H. Martin (2009). Speech and Language Processing. Pearson Education International. pp. 145–146. - ^ Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). \"A Survey of Multilinear Subspace Learning for Tensor Data\" (PDF). Pattern Recognition. 44 (7): 1540–1551. Bibcode:2011PatRe..44.1540L. doi:10.1016/j.patcog.2011.01.004. Archived (PDF) from the original on 10 July 2019. Retrieved 4 September 2015. - ^ Yoshua Bengio (2009). Learning Deep Architectures for AI. Now Publishers Inc. pp. 1–3. ISBN 978-1-60198-294-0. Archived from the original on 17 January 2023. Retrieved 15 February 2016. - ^ Tillmann, A. M. (2015). \"On the Computational Intractability of Exact and Approximate Dictionary Learning\". IEEE Signal Processing Letters. 22 (1): 45–49. arXiv:1405.6664. Bibcode:2015ISPL...22...45T. doi:10.1109/LSP.2014.2345761. S2CID 13342762. - ^ Aharon, M, M Elad, and A Bruckstein. 2006. \"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation Archived 2018-11-23 at the Wayback Machine.\" Signal Processing, IEEE Transactions on 54 (11): 4311–4322 - ^ Zimek, Arthur; Schubert, Erich (2017), \"Outlier Detection\", Encyclopedia of Database Systems, Springer New York, pp. 1–5, doi:10.1007/978-1-4899-7993-3_80719-1, ISBN 978-1-4899-7993-3 - ^ Hodge, V. J.; Austin, J. (2004). \"A Survey of Outlier Detection Methodologies\" (PDF). Artificial Intelligence Review. 22 (2): 85–126. CiteSeerX 10.1.1.318.4023. doi:10.1007/s10462-004-4304-y. S2CID 59941878. Archived (PDF) from the original on 22 June 2015. Retrieved 25 November 2018. - ^ Dokas, Paul; Ertoz, Levent; Kumar, Vipin; Lazarevic, Aleksandar; Srivastava, Jaideep; Tan, Pang-Ning (2002). \"Data mining for network intrusion detection\" (PDF). Proceedings NSF Workshop on Next Generation Data Mining. Archived (PDF) from the original on 23 September 2015. Retrieved 26 March 2023. - ^ Chandola, V.; Banerjee, A.; Kumar, V. (2009). \"Anomaly detection: A survey\". ACM Computing Surveys. 41 (3): 1–58. doi:10.1145/1541880.1541882. S2CID 207172599. - ^ Fleer, S.; Moringen, A.; Klatzky, R. L.; Ritter, H. (2020). \"Learning efficient haptic shape exploration with a rigid tactile sensor array, S. Fleer, A. Moringen, R. Klatzky, H. Ritter\". PLOS ONE. 15 (1) e0226880. arXiv:1902.07501. doi:10.1371/journal.pone.0226880. PMC 6940144. PMID 31896135. - ^ Moringen, Alexandra; Fleer, Sascha; Walck, Guillaume; Ritter, Helge (2020), Nisky, Ilana; Hartcher-O'Brien, Jess; Wiertlewski, Michaël; Smeets, Jeroen (eds.), \"Attention-Based Robot Learning of Haptic Interaction\", Haptics: Science, Technology, Applications, Lecture Notes in Computer Science, vol. 12272, Cham: Springer International Publishing, pp. 462–470, doi:10.1007/978-3-030-58147-3_51, ISBN 978-3-030-58146-6, S2CID 220069113 - ^ Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules, in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA. - ^ Bassel, George W.; Glaab, Enrico; Marquez, Julietta; Holdsworth, Michael J.; Bacardit, Jaume (1 September 2011). \"Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets\". The Plant Cell. 23 (9): 3101–3116. Bibcode:2011PlanC..23.3101B. doi:10.1105/tpc.111.088153. ISSN 1532-298X. PMC 3203449. PMID 21896882. - ^ Agrawal, R.; Imieliński, T.; Swami, A. (1993). \"Mining association rules between sets of items in large databases\". Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93. p. 207. CiteSeerX 10.1.1.40.6984. doi:10.1145/170035.170072. ISBN 978-0-89791-592-2. S2CID 490415. - ^ Urbanowicz, Ryan J.; Moore, Jason H. (22 September 2009). \"Learning Classifier Systems: A Complete Introduction, Review, and Roadmap\". Journal of Artificial Evolution and Applications. 2009: 1–25. doi:10.1155/2009/736398. ISSN 1687-6229. - ^ Plotkin G.D. Automatic Methods of Inductive Inference Archived 22 December 2017 at the Wayback Machine, PhD thesis, University of Edinburgh, 1970. - ^ Shapiro, Ehud Y. Inductive inference of theories from facts Archived 21 August 2021 at the Wayback Machine, Research Report 192, Yale University, Department of Computer Science, 1981. Reprinted in J.-L. Lassez, G. Plotkin (Eds.), Computational Logic, The MIT Press, Cambridge, MA, 1991, pp. 199–254. - ^ Shapiro, Ehud Y. (1983). Algorithmic program debugging. Cambridge, Mass: MIT Press. ISBN 0-262-19218-7 - ^ Shapiro, Ehud Y. \"The model inference system Archived 2023-04-06 at the Wayback Machine.\" Proceedings of the 7th international joint conference on Artificial intelligence-Volume 2. Morgan Kaufmann Publishers Inc., 1981. - ^ Burkov, Andriy (2019). The hundred-page machine learning book. Polen: Andriy Burkov. ISBN 978-1-9995795-0-0. - ^ Russell, Stuart J.; Norvig, Peter (2021). Artificial intelligence: a modern approach. Pearson series in artificial intelligence (Fourth ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3. - ^ Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. \"Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations Archived 2017-10-18 at the Wayback Machine\" Proceedings of the 26th Annual International Conference on Machine Learning, 2009. - ^ \"RandomForestRegressor\". scikit-learn. Retrieved 12 February 2025. - ^ \"What Is Random Forest? | IBM\". www.ibm.com. 20 October 2021. Retrieved 12 February 2025. - ^ Cortes, Corinna; Vapnik, Vladimir N. (1995). \"Support-vector networks\". Machine Learning. 20 (3): 273–297. doi:10.1007/BF00994018. - ^ Stevenson, Christopher. \"Tutorial: Polynomial Regression in Excel\". facultystaff.richmond.edu. Archived from the original on 2 June 2013. Retrieved 22 January 2017. - ^ Wanta, Damian; Smolik, Aleksander; Smolik, Waldemar T.; Midura, Mateusz; Wróblewski, Przemysław (2025). \"Image reconstruction using machine-learned pseudoinverse in electrical capacitance tomography\". Engineering Applications of Artificial Intelligence. 142 109888. doi:10.1016/j.engappai.2024.109888. - ^ The documentation for scikit-learn also has similar examples Archived 2 November 2022 at the Wayback Machine. - ^ Goldberg, David E.; Holland, John H. (1988). \"Genetic algorithms and machine learning\" (PDF). Machine Learning. 3 (2): 95–99. doi:10.1007/bf00113892. S2CID 35506513. Archived (PDF) from the original on 16 May 2011. Retrieved 3 September 2019. - ^ Michie, D.; Spiegelhalter, D. J.; Taylor, C. C. (1994). \"Machine Learning, Neural and Statistical Classification\". Ellis Horwood Series in Artificial Intelligence. Bibcode:1994mlns.book.....M. - ^ Zhang, Jun; Zhan, Zhi-hui; Lin, Ying; Chen, Ni; Gong, Yue-jiao; Zhong, Jing-hui; Chung, Henry S.H.; Li, Yun; Shi, Yu-hui (2011). \"Evolutionary Computation Meets Machine Learning: A Survey\". IEEE Computational Intelligence Magazine. 6 (4): 68–75. Bibcode:2011ICIM....6d..68Z. doi:10.1109/mci.2011.942584. S2CID 6760276. - ^ Verbert, K.; Babuška, R.; De Schutter, B. (1 April 2017). \"Bayesian and Dempster–Shafer reasoning for knowledge-based fault diagnosis–A comparative study\". Engineering Applications of Artificial Intelligence. 60: 136–150. doi:10.1016/j.engappai.2017.01.011. ISSN 0952-1976. - ^ Yoosefzadeh-Najafabadi, Mohsen; Hugh, Earl; Tulpan, Dan; Sulik, John; Eskandari, Milad (2021). \"Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean?\". Front. Plant Sci. 11 624273. Bibcode:2021FrPS...1124273Y. doi:10.3389/fpls.2020.624273. PMC 7835636. PMID 33510761. - ^ Urbanowicz, Ryan J.; Moore, Jason H. (22 September 2009). \"Learning Classifier Systems: A Complete Introduction, Review, and Roadmap\". Journal of Artificial Evolution and Applications. 2009: 1–25. doi:10.1155/2009/736398. ISSN 1687-6229. - ^ Zhang, C. and Zhang, S., 2002. Association rule mining: models and algorithms. Springer-Verlag. - ^ De Castro, Leandro Nunes, and Jonathan Timmis. Artificial immune systems: a new computational intelligence approach. Springer Science & Business Media, 2002. - ^ \"Federated Learning: Collaborative Machine Learning without Centralized Training Data\". Google AI Blog. 6 April 2017. Archived from the original on 7 June 2019. Retrieved 8 June 2019. - ^ Machine learning is included in the CFA Curriculum; see: [1] {{Webarchive|url=https://www.cfainstitute.org/ - ^ Marcos M. López de Prado (2010). Machine Learning for Asset Managers. Cambridge University Press. ISBN 9781108883658 - ^ Ivanenko, Mikhail; Smolik, Waldemar T.; Wanta, Damian; Midura, Mateusz; Wróblewski, Przemysław; Hou, Xiaohan; Yan, Xiaoheng (2023). \"Image Reconstruction Using Supervised Learning in Wearable Electrical Impedance Tomography of the Thorax\". Sensors. 23 (18): 7774. Bibcode:2023Senso..23.7774I. doi:10.3390/s23187774. PMC 10538128. PMID 37765831. - ^ \"BelKor Home Page\" research.att.com - ^ \"The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1)\". 6 April 2012. Archived from the original on 31 May 2016. Retrieved 8 August 2015. - ^ Scott Patterson (13 July 2010). \"Letting the Machines Decide\". The Wall Street Journal. Archived from the original on 24 June 2018. Retrieved 24 June 2018. - ^ Vinod Khosla (10 January 2012). \"Do We Need Doctors or Algorithms?\". Tech Crunch. Archived from the original on 18 June 2018. Retrieved 20 October 2016. - ^ When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed Archived 4 June 2016 at the Wayback Machine, The Physics at ArXiv blog - ^ Vincent, James (10 April 2019). \"The first AI-generated textbook shows what robot writers are actually good at\". The Verge. Archived from the original on 5 May 2019. Retrieved 5 May 2019. - ^ Vaishya, Raju; Javaid, Mohd; Khan, Ibrahim Haleem; Haleem, Abid (1 July 2020). \"Artificial Intelligence (AI) applications for COVID-19 pandemic\". Diabetes & Metabolic Syndrome: Clinical Research & Reviews. 14 (4): 337–339. doi:10.1016/j.dsx.2020.04.012. PMC 7195043. PMID 32305024. - ^ Rezapouraghdam, Hamed; Akhshik, Arash; Ramkissoon, Haywantee (10 March 2021). \"Application of machine learning to predict visitors' green behavior in marine protected areas: evidence from Cyprus\". Journal of Sustainable Tourism. 31 (11): 2479–2505. doi:10.1080/09669582.2021.1887878. hdl:10037/24073. - ^ Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (15 June 2020). \"User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs\". 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE) (PDF). pp. 1728–1733. doi:10.23919/DATE48585.2020.9116294. ISBN 978-3-9819263-4-7. S2CID 219858480. Archived from the original on 13 December 2021. Retrieved 20 January 2022. - ^ Quested, Tony. \"Smartphones get smarter with Essex innovation\". Business Weekly. Archived from the original on 24 June 2021. Retrieved 17 June 2021. - ^ Williams, Rhiannon (21 July 2020). \"Future smartphones 'will prolong their own battery life by monitoring owners' behaviour'\". i. Archived from the original on 24 June 2021. Retrieved 17 June 2021. - ^ Rasekhschaffe, Keywan Christian; Jones, Robert C. (1 July 2019). \"Machine Learning for Stock Selection\". Financial Analysts Journal. 75 (3): 70–88. doi:10.1080/0015198X.2019.1596678. ISSN 0015-198X. S2CID 108312507. Archived from the original on 26 November 2023. Retrieved 26 November 2023. - ^ Chung, Yunsie; Green, William H. (2024). \"Machine learning from quantum chemistry to predict experimental solvent effects on reaction rates\". Chemical Science. 15 (7): 2410–2424. doi:10.1039/D3SC05353A. ISSN 2041-6520. PMC 10866337. PMID 38362410. - ^ Sun, Yuran; Huang, Shih-Kai; Zhao, Xilei (1 February 2024). \"Predicting Hurricane Evacuation Decisions with Interpretable Machine Learning Methods\". International Journal of Disaster Risk Science. 15 (1): 134–148. arXiv:2303.06557. Bibcode:2024IJDRS..15..134S. doi:10.1007/s13753-024-00541-1. ISSN 2192-6395. - ^ Sun, Yuran; Zhao, Xilei; Lovreglio, Ruggiero; Kuligowski, Erica (1 January 2024), Naser, M. Z. (ed.), \"8 - AI for large-scale evacuation modeling: promises and challenges\", Interpretable Machine Learning for the Analysis, Design, Assessment, and Informed Decision Making for Civil Infrastructure, Woodhead Publishing Series in Civil and Structural Engineering, Woodhead Publishing, pp. 185–204, ISBN 978-0-12-824073-1, archived from the original on 19 May 2024, retrieved 19 May 2024 - ^ Xu, Ningzhe; Lovreglio, Ruggiero; Kuligowski, Erica D.; Cova, Thomas J.; Nilsson, Daniel; Zhao, Xilei (1 March 2023). \"Predicting and Assessing Wildfire Evacuation Decision-Making Using Machine Learning: Findings from the 2019 Kincade Fire\". Fire Technology. 59 (2): 793–825. doi:10.1007/s10694-023-01363-1. ISSN 1572-8099. - ^ Wang, Ke; Shi, Xiupeng; Goh, Algena Pei Xuan; Qian, Shunzhi (1 June 2019). \"A machine learning based study on pedestrian movement dynamics under emergency evacuation\". Fire Safety Journal. 106: 163–176. Bibcode:2019FirSJ.106..163W. doi:10.1016/j.firesaf.2019.04.008. hdl:10356/143390. ISSN 0379-7112. Archived from the original on 19 May 2024. Retrieved 19 May 2024. - ^ Zhao, Xilei; Lovreglio, Ruggiero; Nilsson, Daniel (1 May 2020). \"Modelling and interpreting pre-evacuation decision-making using machine learning\". Automation in Construction. 113 103140. doi:10.1016/j.autcon.2020.103140. hdl:10179/17315. ISSN 0926-5805. Archived from the original on 19 May 2024. Retrieved 19 May 2024. - ^ \"Why Machine Learning Models Often Fail to Learn: QuickTake Q&A\". Bloomberg.com. 10 November 2016. Archived from the original on 20 March 2017. Retrieved 10 April 2017. - ^ \"The First Wave of Corporate AI Is Doomed to Fail\". Harvard Business Review. 18 April 2017. Archived from the original on 21 August 2018. Retrieved 20 August 2018. - ^ \"Why the A.I. euphoria is doomed to fail\". VentureBeat. 18 September 2016. Archived from the original on 19 August 2018. Retrieved 20 August 2018. - ^ \"9 Reasons why your machine learning project will fail\". www.kdnuggets.com. Archived from the original on 21 August 2018. Retrieved 20 August 2018. - ^ a b Babuta, Alexander; Oswald, Marion; Rinik, Christine (2018). Transparency and Intelligibility (Report). Royal United Services Institute (RUSI). pp. 17–22. Archived from the original on 9 December 2023. Retrieved 9 December 2023. - ^ \"Why Uber's self-driving car killed a pedestrian\". The Economist. Archived from the original on 21 August 2018. Retrieved 20 August 2018. - ^ \"IBM's Watson recommended 'unsafe and incorrect' cancer treatments – STAT\". STAT. 25 July 2018. Archived from the original on 21 August 2018. Retrieved 21 August 2018. - ^ Hernandez, Daniela; Greenwald, Ted (11 August 2018). \"IBM Has a Watson Dilemma\". The Wall Street Journal. ISSN 0099-9660. Archived from the original on 21 August 2018. Retrieved 21 August 2018. - ^ Allyn, Bobby (27 February 2023). \"How Microsoft's experiment in artificial intelligence tech backfired\". National Public Radio. Archived from the original on 8 December 2023. Retrieved 8 December 2023. - ^ Reddy, Shivani M.; Patel, Sheila; Weyrich, Meghan; Fenton, Joshua; Viswanathan, Meera (2020). \"Comparison of a traditional systematic review approach with review-of-reviews and semi-automation as strategies to update the evidence\". Systematic Reviews. 9 (1): 243. doi:10.1186/s13643-020-01450-2. ISSN 2046-4053. PMC 7574591. PMID 33076975. - ^ Rudin, Cynthia (2019). \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\". Nature Machine Intelligence. 1 (5): 206–215. doi:10.1038/s42256-019-0048-x. PMC 9122117. PMID 35603010. - ^ Hu, Tongxi; Zhang, Xuesong; Bohrer, Gil; Liu, Yanlan; Zhou, Yuyu; Martin, Jay; LI, Yang; Zhao, Kaiguang (2023). \"Crop yield prediction via explainable AI and interpretable machine learning: Dangers of black box models for evaluating climate change impacts on crop yield\". Agricultural and Forest Meteorology. 336 109458. Bibcode:2023AgFM..33609458H. doi:10.1016/j.agrformet.2023.109458. S2CID 258552400. - ^ Domingos 2015, Chapter 6, Chapter 7. - ^ Domingos 2015, p. 286. - ^ \"Single pixel change fools AI programs\". BBC News. 3 November 2017. Archived from the original on 22 March 2018. Retrieved 12 March 2018. - ^ \"AI Has a Hallucination Problem That's Proving Tough to Fix\". WIRED. 2018. Archived from the original on 12 March 2018. Retrieved 12 March 2018. - ^ Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. (4 September 2019). \"Towards deep learning models resistant to adversarial attacks\". arXiv:1706.06083 [stat.ML]. - ^ \"Adversarial Machine Learning – CLTC UC Berkeley Center for Long-Term Cybersecurity\". CLTC. Archived from the original on 17 May 2022. Retrieved 25 May 2022. - ^ \"Machine-learning models vulnerable to undetectable backdoors\". The Register. Archived from the original on 13 May 2022. Retrieved 13 May 2022. - ^ \"Undetectable Backdoors Plantable In Any Machine-Learning Algorithm\". IEEE Spectrum. 10 May 2022. Archived from the original on 11 May 2022. Retrieved 13 May 2022. - ^ Goldwasser, Shafi; Kim, Michael P.; Vaikuntanathan, Vinod; Zamir, Or (14 April 2022). \"Planting Undetectable Backdoors in Machine Learning Models\". arXiv:2204.06974 [cs.LG]. - ^ Kohavi, Ron (1995). \"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\" (PDF). International Joint Conference on Artificial Intelligence. Archived (PDF) from the original on 12 July 2018. Retrieved 26 March 2023. - ^ Catal, Cagatay (2012). \"Performance Evaluation Metrics for Software Fault Prediction Studies\" (PDF). Acta Polytechnica Hungarica. 9 (4). Retrieved 2 October 2016. - ^ a b Müller, Vincent C. (30 April 2020). \"Ethics of Artificial Intelligence and Robotics\". Stanford Encyclopedia of Philosophy. Archived from the original on 10 October 2020. - ^ \"Assessing potential future artificial intelligence risks, benefits and policy imperatives\". OECD. 14 November 2024. Retrieved 4 August 2025. - ^ a b Garcia, Megan (2016). \"Racist in the Machine\". World Policy Journal. 33 (4): 111–117. doi:10.1215/07402775-3813015. ISSN 0740-2775. S2CID 151595343. - ^ Bostrom, Nick (2011). \"The Ethics of Artificial Intelligence\" (PDF). Archived from the original (PDF) on 4 March 2016. Retrieved 11 April 2016. - ^ Edionwe, Tolulope. \"The fight against racist algorithms\". The Outline. Archived from the original on 17 November 2017. Retrieved 17 November 2017. - ^ Jeffries, Adrianne. \"Machine learning is racist because the internet is racist\". The Outline. Archived from the original on 17 November 2017. Retrieved 17 November 2017. - ^ a b Silva, Selena; Kenney, Martin (2018). \"Algorithms, Platforms, and Ethnic Bias: An Integrative Essay\" (PDF). Phylon. 55 (1 & 2): 9–37. ISSN 0031-8906. JSTOR 26545017. Archived (PDF) from the original on 27 January 2024. - ^ Wong, Carissa (30 March 2023). \"AI 'fairness' research held back by lack of diversity\". Nature. doi:10.1038/d41586-023-00935-z. PMID 36997714. S2CID 257857012. Archived from the original on 12 April 2023. Retrieved 9 December 2023. - ^ a b Zhang, Jack Clark. \"Artificial Intelligence Index Report 2021\" (PDF). Stanford Institute for Human-Centered Artificial Intelligence. Archived from the original (PDF) on 19 May 2024. Retrieved 9 December 2023. - ^ Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind (14 April 2017). \"Semantics derived automatically from language corpora contain human-like biases\". Science. 356 (6334): 183–186. arXiv:1608.07187. Bibcode:2017Sci...356..183C. doi:10.1126/science.aal4230. ISSN 0036-8075. PMID 28408601. S2CID 23163324. - ^ Wang, Xinan; Dasgupta, Sanjoy (2016), Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I. (eds.), \"An algorithm for L1 nearest neighbor search via monotonic embedding\" (PDF), Advances in Neural Information Processing Systems 29, Curran Associates, Inc., pp. 983–991, archived (PDF) from the original on 7 April 2017, retrieved 20 August 2018 - ^ M.O.R. Prates; P.H.C. Avelar; L.C. Lamb (11 March 2019). \"Assessing Gender Bias in Machine Translation – A Case Study with Google Translate\". arXiv:1809.02208 [cs.CY]. - ^ Narayanan, Arvind (24 August 2016). \"Language necessarily contains human biases, and so will machines trained on language corpora\". Freedom to Tinker. Archived from the original on 25 June 2018. Retrieved 19 November 2016. - ^ Metz, Rachel (24 March 2016). \"Why Microsoft Accidentally Unleashed a Neo-Nazi Sexbot\". MIT Technology Review. Archived from the original on 9 November 2018. Retrieved 20 August 2018. - ^ Vincent, James (12 January 2018). \"Google 'fixed' its racist algorithm by removing gorillas from its image-labeling tech\". The Verge. Archived from the original on 21 August 2018. Retrieved 20 August 2018. - ^ Crawford, Kate (25 June 2016). \"Opinion | Artificial Intelligence's White Guy Problem\". New York Times. Archived from the original on 14 January 2021. Retrieved 20 August 2018. - ^ Simonite, Tom (30 March 2017). \"Microsoft: AI Isn't Yet Adaptable Enough to Help Businesses\". MIT Technology Review. Archived from the original on 9 November 2018. Retrieved 20 August 2018. - ^ Hempel, Jessi (13 November 2018). \"Fei-Fei Li's Quest to Make Machines Better for Humanity\". Wired. ISSN 1059-1028. Archived from the original on 14 December 2020. Retrieved 17 February 2019. - ^ Char, D. S.; Shah, N. H.; Magnus, D. (2018). \"Implementing Machine Learning in Health Care—Addressing Ethical Challenges\". New England Journal of Medicine. 378 (11): 981–983. doi:10.1056/nejmp1714229. PMC 5962261. PMID 29539284. - ^ Research, AI (23 October 2015). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition\". airesearch.com. Archived from the original on 1 February 2016. Retrieved 23 October 2015. - ^ \"GPUs Continue to Dominate the AI Accelerator Market for Now\". InformationWeek. December 2019. Archived from the original on 10 June 2020. Retrieved 11 June 2020. - ^ Ray, Tiernan (2019). \"AI is changing the entire nature of compute\". ZDNet. Archived from the original on 25 May 2020. Retrieved 11 June 2020. - ^ \"AI and compute\". OpenAI. 16 May 2018. Archived from the original on 17 June 2020. Retrieved 11 June 2020. - ^ Jouppi, Norman P.; Young, Cliff; Patil, Nishant; Patterson, David; Agrawal, Gaurav; Bajwa, Raminder; Bates, Sarah; Bhatia, Suresh; Boden, Nan; Borchers, Al; Boyle, Rick; Cantin, Pierre-luc; Chao, Clifford; Clark, Chris; Coriell, Jeremy (24 June 2017). \"In-Datacenter Performance Analysis of a Tensor Processing Unit\". Proceedings of the 44th Annual International Symposium on Computer Architecture. ISCA '17. New York, NY, US: Association for Computing Machinery. pp. 1–12. arXiv:1704.04760. doi:10.1145/3079856.3080246. ISBN 978-1-4503-4892-8. - ^ Best, Jo (8 December 2020). \"What is neuromorphic computing? Everything you need to know about how it is changing the future of computing\". ZDNET. Retrieved 21 November 2024. - ^ Hecate He (27 May 2021). Michael Sarazen; Chain Zhang (eds.). \"Cornell & NTT's Physical Neural Networks: A \"Radical Alternative for Implementing Deep Neural Networks\" That Enables Arbitrary Physical Systems Training\". Synced. Archived from the original on 27 October 2021. Retrieved 12 October 2021. - ^ Clark, Lindsay (5 October 2021). \"Nano-spaghetti to solve neural network power consumption\". The Register. Archived from the original on 6 October 2021. Retrieved 12 October 2021. - ^ Fafoutis, Xenofon; Marchegiani, Letizia; Elsts, Atis; Pope, James; Piechocki, Robert; Craddock, Ian (7 May 2018). \"Extending the battery lifetime of wearable sensors with embedded machine learning\". 2018 IEEE 4th World Forum on Internet of Things (WF-IoT). pp. 269–274. doi:10.1109/WF-IoT.2018.8355116. hdl:1983/b8fdb58b-7114-45c6-82e4-4ab239c1327f. ISBN 978-1-4673-9944-9. S2CID 19192912. Archived from the original on 18 January 2022. Retrieved 17 January 2022. - ^ \"A Beginner's Guide To Machine learning For Embedded Systems\". Analytics India Magazine. 2 June 2021. Archived from the original on 18 January 2022. Retrieved 17 January 2022. - ^ Synced (12 January 2022). \"Google, Purdue & Harvard U's Open-Source Framework for TinyML Achieves up to 75x Speedups on FPGAs | Synced\". syncedreview.com. Archived from the original on 18 January 2022. Retrieved 17 January 2022. - ^ AlSelek, Mohammad; Alcaraz-Calero, Jose M.; Wang, Qi (2024). \"Dynamic AI-IoT: Enabling Updatable AI Models in Ultralow-Power 5G IoT Devices\". IEEE Internet of Things Journal. 11 (8): 14192–14205. Bibcode:2024IITJ...1114192A. doi:10.1109/JIOT.2023.3340858. - ^ Giri, Davide; Chiu, Kuan-Lin; Di Guglielmo, Giuseppe; Mantovani, Paolo; Carloni, Luca P. (15 June 2020). \"ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning\". 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE). pp. 1049–1054. arXiv:2004.03640. doi:10.23919/DATE48585.2020.9116317. ISBN 978-3-9819263-4-7. S2CID 210928161. Archived from the original on 18 January 2022. Retrieved 17 January 2022. - ^ Louis, Marcia Sahaya; Azad, Zahra; Delshadtehrani, Leila; Gupta, Suyog; Warden, Pete; Reddi, Vijay Janapa; Joshi, Ajay (2019). \"Towards Deep Learning using TensorFlow Lite on RISC-V\". Harvard University. Archived from the original on 17 January 2022. Retrieved 17 January 2022. - ^ Ibrahim, Ali; Osta, Mario; Alameh, Mohamad; Saleh, Moustafa; Chible, Hussein; Valle, Maurizio (21 January 2019). \"Approximate Computing Methods for Embedded Machine Learning\". 2018 25th IEEE International Conference on Electronics, Circuits and Systems (ICECS). pp. 845–848. doi:10.1109/ICECS.2018.8617877. ISBN 978-1-5386-9562-3. S2CID 58670712. - ^ \"dblp: TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning\". dblp.org. Archived from the original on 18 January 2022. Retrieved 17 January 2022. - ^ Branco, Sérgio; Ferreira, André G.; Cabral, Jorge (5 November 2019). \"Machine Learning in Resource-Scarce Embedded Systems, FPGAs, and End-Devices: A Survey\". Electronics. 8 (11): 1289. doi:10.3390/electronics8111289. hdl:1822/62521. ISSN 2079-9292. Sources [edit]- Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7. - Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019. - Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020. - Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2. Further reading [edit]- Alpaydin, Ethem (2020). Introduction to Machine Learning, (4th edition) MIT Press, ISBN 9780262043793. - Bishop, Christopher (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2. - Bishop, Christopher (2006) Pattern Recognition and Machine Learning, Springer. ISBN 978-0-387-31073-2 - Domingos, Pedro (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7 - Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3. - Hastie, Trevor; Tibshirani, Robert & Friedman, Jerome H. (2009) The Elements of Statistical Learning, Springer. doi:10.1007/978-0-387-84858-7 ISBN 0-387-95284-5. - MacKay, David J. C. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1 - Murphy, Kevin P. (2021). Probabilistic Machine Learning: An Introduction Archived 11 April 2021 at the Wayback Machine, MIT Press. - Nilsson, Nils J. (2015) Introduction to Machine Learning Archived 16 August 2019 at the Wayback Machine. - Russell, Stuart & Norvig, Peter (2020). Artificial Intelligence – A Modern Approach. (4th edition) Pearson, ISBN 978-0134610993. - Solomonoff, Ray, (1956) An Inductive Inference Machine Archived 26 April 2011 at the Wayback Machine A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI. - Witten, Ian H. & Frank, Eibe (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0. External links [edit]- International Machine Learning Society - mloss is an academic database of open-source machine learning software.",
    "text_length": 107240,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:11.034956"
  },
  {
    "id": "page_12",
    "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
    "domain": "en.wikipedia.org",
    "title": "Symbolic artificial intelligence - Wikipedia",
    "text": "Symbolic artificial intelligence | Part of a series on | | Artificial intelligence (AI) | |---| In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence or logic-based artificial intelligence)[1][2] is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic, and search.[3] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems), symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to important ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s.[4] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field.[5] An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up.[6][7] A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace.[8][9] That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment.[9] Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed.[10] Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition.[11] Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning.[12][13] Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.[14] Neural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012. Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams,[15] and work in convolutional neural networks by LeCun et al. in 1989.[16] However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\"[17] Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation, though symbolic approaches continue to be useful in a few domains such as computer algebra systems and proof assistants. However, given the inherent complexity of intelligence itself, it remains an open question whether symbolic AI will be completely supplanted by connectionist AI, or whether symbolic AI may yet experience a resurgence. More recently, work by Zhang et al. has further argued that, at least at the theoretical level, there is no clear superiority among different technological paradigms[18]. History [edit]A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture[19] and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity. The first AI summer: irrational exuberance, 1948–1966 [edit]Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history. Approaches inspired by human or animal cognition or behavior [edit]Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.[20] An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.[21] During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background. Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems.[22][23] This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[24][25] Heuristic search [edit]In addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\"[26] Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.[26] Early work on knowledge representation and reasoning [edit]Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner. Modeling formal reasoning with logic: the \"neats\" [edit]Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic,[27] regardless of whether people used the same algorithms.[a] His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[31] Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[32][33] Modeling implicit common-sense knowledge with frames and scripts: the \"scruffies\" [edit]Researchers at MIT (such as Marvin Minsky and Seymour Papert)[34][35][7] found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).[36][37] Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[38][39][40] The first AI winter: crushed dreams, 1967–1977 [edit]The first AI winter was a shock: During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs. ... Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.[41] The second AI summer: knowledge is power, 1978–1987 [edit]Knowledge-based systems [edit]As limitations with weak, domain-independent methods became more and more apparent,[42] researchers from all three traditions began to build knowledge into AI applications.[43][8] The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications. Edward Feigenbaum said: - \"In the knowledge lies the power.\"[44] to describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: (1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates. (2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.[45] Success with expert systems [edit]This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.[46][47][48] Key expert systems were: - DENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings. - MYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. \"With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.\"[49] - INTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases. - GUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.[50] - XCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.[10] DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum: One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space. We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results. The generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.[51] The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling.[50] XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it: By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.[49] Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.[52] Architecture of knowledge-based and expert systems [edit]A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.[53] The simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion. Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies. Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture[54] was originally inspired by studies of how humans plan to perform multiple tasks in a trip.[55] An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring. The second AI winter, 1988–1993 [edit]At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations. Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed: Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.[10] Adding in more rigorous foundations, 1993–2011 [edit]Uncertain reasoning [edit]Both statistical approaches and extensions to logic were tried. One statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work.[12] Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.[56] and Bayesian approaches were applied successfully in expert systems.[57] Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic. Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions.[58] Lotfi Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.[59] Machine learning [edit]Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as ...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems. We did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.[51] In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3[60] and then later extending its capabilities to C4.5.[61] The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules. Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far.[62] More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.[63] Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.[64] Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples.[65] John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.[66] As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory,[67] focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem.[68] Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.[69] Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include: - Learning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to \"avoid taking points.\"[70] - Learning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.[71] - Learning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.[72][73] - Apprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.[74] - Learning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row.[75] - Learning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.[76] Deep learning and neuro-symbolic AI 2011–now [edit]With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.[77][78] Neuro-symbolic AI: integrating neural and symbolic approaches [edit]Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant[79] and many others,[80] the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\",[81] and in particular: \"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"[82] Henry Kautz,[19] Francesca Rossi,[83] and Bart Selman[84] have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed. Garcez and Lamb describe research in this area as being ongoing for at least the past twenty years,[85] dating from their 2002 book on neurosymbolic learning systems.[86] A series of workshops on neuro-symbolic reasoning has been held every year since 2005.[87] In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that: The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.[80] Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows: - Symbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3. - Symbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions. - Neural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically. - Neural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples. - Neural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover,[88] which constructs a neural network from an AND–OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks[89] also fall into this category. - Neural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state. Many key research questions remain, such as: - What is the best way to integrate neural and symbolic architectures?[90] - How should symbolic structures be represented within neural networks and extracted from them? - How should common-sense knowledge be learned and reasoned about? - How can abstract knowledge that is hard to encode logically be handled? Techniques and contributions [edit]This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section. AI programming languages [edit]The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code. Other key innovations pioneered by LISP that have spread to other programming languages include: Programs were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages. In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog. Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article. Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages. Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail. Smalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.[91] For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses. Search [edit]Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions. Knowledge representation and reasoning [edit]Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning. Knowledge representation [edit]Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used. Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.[92] First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together. Automatic theorem proving [edit]Examples of automated theorem provers for first-order logic are: Prover9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm. Reasoning in knowledge-based systems [edit]Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store. Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog. A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture. Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks. Commonsense reasoning [edit]Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning. Qualitative simulation, such as Benjamin Kuipers's QSIM,[93] approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure. Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers. Constraints and constraint-based reasoning [edit]Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR). Automated planning [edit]The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards. Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem. Natural language processing [edit]Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions. Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles. New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque. Agents and multi-agent systems [edit]Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication.[94] The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture[90] that includes deep learning for perception.[95] In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization. Controversies [edit]Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters. The Frame Problem: knowledge representation challenges for first-order logic [edit]Limitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed. McCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\"[96] A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change. A similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly. McCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions. Other ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules. McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules. Similar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures. This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle). McCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above.[97] He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\" Connectionist AI: philosophical challenges and sociological conflicts [edit]Connectionist approaches include earlier work on neural networks,[98] such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning. Three philosophical positions[99] have been outlined among connectionists: - Implementationism—where connectionist architectures implement the capabilities for symbolic processing, - Radical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it, - Moderate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence. Olazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids: The third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).[100] Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical: To think that we can simply abandon symbol-manipulation is to suspend disbelief. And yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples. According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\": When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes. ... Since then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.[101] Part of these disputes may be due to unclear terminology: Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.[102] It is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are recursively isomorphic, provided they are universal or have equivalent expressive power.[103] This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization of artificial general intelligence (AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI. Situated robotics: the world as a model [edit]Another critique of symbolic AI is the embodied cognition approach: The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.[104] Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\"[105] He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"[106] In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"[107] His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\"[108] and the use of the blocks world in symbolic AI systems such as SHRDLU. Current views [edit]Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge. Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward.[19][83][84] Russell and Norvig conclude that: Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.[104] See also [edit]- Artificial intelligence - Automated planning and scheduling - Automated theorem proving - Belief revision - Case-based reasoning - Cognitive architecture - Cognitive science - Connectionism - Constraint programming - Deep learning - First-order logic - GOFAI - History of artificial intelligence - Inductive logic programming - Knowledge-based systems - Knowledge representation and reasoning - Logic programming - Machine learning - Model checking - Model-based reasoning - Multi-agent system - Natural language processing - Neuro-symbolic AI - Ontology - Philosophy of artificial intelligence - Physical symbol systems hypothesis - Semantic Web - Sequential pattern mining - Statistical relational learning - Symbolic mathematics - YAGO ontology - WordNet Notes [edit]- ^ McCarthy once said: \"This is AI, so we don't care if it's psychologically real\".[4] McCarthy reiterated his position in 2006 at the AI@50 conference where he said \"Artificial intelligence is not, by definition, simulation of human intelligence\".[28] Pamela McCorduck writes that there are \"two major branches of artificial intelligence: one aimed at producing intelligent behavior regardless of how it was accomplished, and the other aimed at modeling intelligent processes found in nature, particularly human ones.\",[29] Stuart Russell and Peter Norvig wrote \"Aeronautical engineering texts do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool even other pigeons.'\"[30] Citations [edit]- ^ Garnelo, Marta; Shanahan, Murray (October 2019). \"Reconciling deep learning with symbolic artificial intelligence: representing objects and relations\". Current Opinion in Behavioral Sciences. 29: 17–23. doi:10.1016/j.cobeha.2018.12.010. hdl:10044/1/67796. - ^ Thomason, Richmond (February 27, 2024). \"Logic-Based Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. - ^ Garnelo, Marta; Shanahan, Murray (2019-10-01). \"Reconciling deep learning with symbolic artificial intelligence: representing objects and relations\". Current Opinion in Behavioral Sciences. 29: 17–23. doi:10.1016/j.cobeha.2018.12.010. hdl:10044/1/67796. S2CID 72336067. - ^ a b Kolata 1982. - ^ Newell, Allen; Simon, Herbert A. (1976-03-01). \"Computer science as empirical inquiry: symbols and search\". Commun. ACM. 19 (3): 113–126. doi:10.1145/360018.360022. ISSN 0001-0782. - ^ Kautz 2022, pp. 107–109. - ^ a b Russell & Norvig 2021, p. 19. - ^ a b Russell & Norvig 2021, pp. 22–23. - ^ a b Kautz 2022, pp. 109–110. - ^ a b c Kautz 2022, p. 110. - ^ Kautz 2022, pp. 110–111. - ^ a b Russell & Norvig 2021, p. 25. - ^ Kautz 2022, p. 111. - ^ Kautz 2020, pp. 110–111. - ^ Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (1986). \"Learning representations by back-propagating errors\". Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0. ISSN 1476-4687. S2CID 205001834. - ^ LeCun, Y.; Boser, B.; Denker, I.; Henderson, D.; Howard, R.; Hubbard, W.; Tackel, L. (1989). \"Backpropagation Applied to Handwritten Zip Code Recognition\". Neural Computation. 1 (4): 541–551. doi:10.1162/neco.1989.1.4.541. S2CID 41312633. - ^ Marcus & Davis 2019. - ^ Zhang, Heng; Jiang, Guifei; Quan, Donghui (2025-04-11). \"A Theory of Formalisms for Representing Knowledge\". Proceedings of the AAAI Conference on Artificial Intelligence. 39 (14): 15257–15264. doi:10.1609/aaai.v39i14.33674. ISSN 2374-3468. - ^ a b c Kautz 2020. - ^ Kautz 2022, p. 106. - ^ Newell & Simon 1972. - ^ & McCorduck 2004, pp. 139–179, 245–250, 322–323 (EPAM). - ^ Crevier 1993, pp. 145–149. - ^ McCorduck 2004, pp. 450–451. - ^ Crevier 1993, pp. 258–263. - ^ a b Kautz 2022, p. 108. - ^ Russell & Norvig 2021, p. 9 (logicist AI), p. 19 (McCarthy's work). - ^ Maker 2006. - ^ McCorduck 2004, pp. 100–101. - ^ Russell & Norvig 2021, p. 2. - ^ McCorduck 2004, pp. 251–259. - ^ Crevier 1993, pp. 193–196. - ^ Howe 1994. - ^ McCorduck 2004, pp. 259–305. - ^ Crevier 1993, pp. 83–102, 163–176. - ^ McCorduck 2004, pp. 421–424, 486–489. - ^ Crevier 1993, p. 168. - ^ McCorduck 2004, p. 489. - ^ Crevier 1993, pp. 239–243. - ^ Russell & Norvig 2021, p. 316, 340. - ^ Kautz 2022, p. 109. - ^ Russell & Norvig 2021, p. 22. - ^ McCorduck 2004, pp. 266–276, 298–300, 314, 421. - ^ Shustek, Len (June 2010). \"An interview with Ed Feigenbaum\". Communications of the ACM. 53 (6): 41–45. doi:10.1145/1743546.1743564. ISSN 0001-0782. S2CID 10239007. Retrieved 2022-07-14. - ^ Lenat, Douglas B; Feigenbaum, Edward A (1988). \"On the thresholds of knowledge\". Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications. pp. 291–300. doi:10.1109/AIIA.1988.13308. S2CID 11778085. - ^ Russell & Norvig 2021, pp. 22–24. - ^ McCorduck 2004, pp. 327–335, 434–435. - ^ Crevier 1993, pp. 145–62, 197–203. - ^ a b Russell & Norvig 2021, p. 23. - ^ a b Clancey 1987. - ^ a b Shustek, Len (2010). \"An interview with Ed Feigenbaum\". Communications of the ACM. 53 (6): 41–45. doi:10.1145/1743546.1743564. ISSN 0001-0782. S2CID 10239007. Retrieved 2022-08-05. - ^ \"The fascination with AI: what is artificial intelligence?\". IONOS Digitalguide. Retrieved 2021-12-02. - ^ Hayes-Roth, Murray & Adelman 2015. - ^ Hayes-Roth, Barbara (1985). \"A blackboard architecture for control\". Artificial Intelligence. 26 (3): 251–321. doi:10.1016/0004-3702(85)90063-3. - ^ Hayes-Roth, Barbara (1980). Human Planning Processes. RAND. - ^ Pearl 1988. - ^ Spiegelhalter et al. 1993. - ^ Russell & Norvig 2021, pp. 335–337. - ^ Russell & Norvig 2021, p. 459. - ^ Quinlan, J. Ross. \"Chapter 15: Learning Efficient Classification Procedures and their Application to Chess End Games\". In Michalski, Carbonell & Mitchell (1983). - ^ Quinlan, J. Ross (1992-10-15). C4.5: Programs for Machine Learning (1st ed.). San Mateo, Calif: Morgan Kaufmann. ISBN 978-1-55860-238-0. - ^ Mitchell, Tom M.; Utgoff, Paul E.; Banerji, Ranan. \"Chapter 6: Learning by Experimentation: Acquiring and Refining Problem-Solving Heuristics\". In Michalski, Carbonell & Mitchell (1983). - ^ Valiant, L. G. (1984-11-05). \"A theory of the learnable\". Communications of the ACM. 27 (11): 1134–1142. doi:10.1145/1968.1972. ISSN 0001-0782. S2CID 12837541. - ^ Koedinger, K. R.; Anderson, J. R.; Hadley, W. H.; Mark, M. A.; others (1997). \"Intelligent tutoring goes to school in the big city\". International Journal of Artificial Intelligence in Education. 8: 30–43. Retrieved 2012-08-18. - ^ Shapiro, Ehud Y (1981). \"The Model Inference System\". Proceedings of the 7th international joint conference on Artificial intelligence. IJCAI. Vol. 2. p. 1064. - ^ Manna, Zohar; Waldinger, Richard (1980-01-01). \"A Deductive Approach to Program Synthesis\". ACM Trans. Program. Lang. Syst. 2 (1): 90–121. doi:10.1145/357084.357090. S2CID 14770735. - ^ Schank, Roger C. (1983-01-28). Dynamic Memory: A Theory of Reminding and Learning in Computers and People. Cambridge Cambridgeshire : New York: Cambridge University Press. ISBN 978-0-521-27029-8. - ^ Hammond, Kristian J. (1989-04-11). Case-Based Planning: Viewing Planning as a Memory Task. Boston: Academic Press. ISBN 978-0-12-322060-8. - ^ Koza, John R. (1992-12-11). Genetic Programming: On the Programming of Computers by Means of Natural Selection (1st ed.). Cambridge, Mass: A Bradford Book. ISBN 978-0-262-11170-6. - ^ Mostow, David Jack. \"Chapter 12: Machine Transformation of Advice into a Heuristic Search Procedure\". In Michalski, Carbonell & Mitchell (1983). - ^ Bareiss, Ray; Porter, Bruce; Wier, Craig. \"Chapter 4: Protos: An Exemplar-Based Learning Apprentice\". In Michalski, Carbonell & Mitchell (1986), pp. 112–139. - ^ Carbonell, Jaime. \"Chapter 5: Learning by Analogy: Formulating and Generalizing Plans from Past Experience\". In Michalski, Carbonell & Mitchell (1983), pp. 137–162. - ^ Carbonell, Jaime. \"Chapter 14: Derivational Analogy: A Theory of Reconstructive Problem Solving and Expertise Acquisition\". In Michalski, Carbonell & Mitchell (1986), pp. 371–392. - ^ Mitchell, Tom; Mabadevan, Sridbar; Steinberg, Louis. \"Chapter 10: LEAP: A Learning Apprentice for VLSI Design\". In Kodratoff & Michalski (1990), pp. 271–289. - ^ Lenat, Douglas. \"Chapter 9: The Role of Heuristics in Learning by Discovery: Three Case Studies\". In Michalski, Carbonell & Mitchell (1983), pp. 243–306. - ^ Korf, Richard E. (1985). Learning to Solve Problems by Searching for Macro-Operators. Research Notes in Artificial Intelligence. Pitman Publishing. ISBN 0-273-08690-1. - ^ Rossi, Francesca. \"Thinking Fast and Slow in AI\". AAAI. Retrieved 5 July 2022. - ^ Selman, Bart. \"AAAI Presidential Address: The State of AI\". AAAI. Retrieved 5 July 2022. - ^ Valiant 2008. - ^ a b Garcez et al. 2015. - ^ Marcus 2020, p. 44. - ^ Marcus 2020, p. 17. - ^ a b Rossi 2022. - ^ a b Selman 2022. - ^ Garcez & Lamb 2020, p. 2. - ^ Garcez et al. 2002. - ^ http://www.neural-symbolic.org - ^ Rocktäschel, Tim; Riedel, Sebastian (2016). \"Learning Knowledge Base Inference with Neural Theorem Provers\". Proceedings of the 5th Workshop on Automated Knowledge Base Construction. San Diego, CA: Association for Computational Linguistics. pp. 45–50. doi:10.18653/v1/W16-1309. Retrieved 2022-08-06. - ^ Serafini, Luciano; Garcez, Artur d'Avila (2016), Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, arXiv:1606.04422 - ^ a b Garcez, Artur d'Avila; Lamb, Luis C.; Gabbay, Dov M. (2009). Neural-Symbolic Cognitive Reasoning (1st ed.). Berlin-Heidelberg: Springer. Bibcode:2009nscr.book.....D. doi:10.1007/978-3-540-73246-4. ISBN 978-3-540-73245-7. S2CID 14002173. - ^ Kiczales, Gregor; Rivieres, Jim des; Bobrow, Daniel G. (1991-07-30). The Art of the Metaobject Protocol (1st ed.). Cambridge, Mass: The MIT Press. ISBN 978-0-262-61074-2. - ^ Motik, Boris; Shearer, Rob; Horrocks, Ian (2009-10-28). \"Hypertableau Reasoning for Description Logics\". Journal of Artificial Intelligence Research. 36: 165–228. arXiv:1401.3485. doi:10.1613/jair.2811. ISSN 1076-9757. S2CID 190609. - ^ Kuipers, Benjamin (1994). Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge. MIT Press. ISBN 978-0-262-51540-5. - ^ Russell & Norvig 2021. - ^ Leo de Penning, Artur S. d'Avila Garcez, Luís C. Lamb, John-Jules Ch. Meyer: \"A Neural-Symbolic Cognitive Agent for Online Learning and Reasoning.\" IJCAI 2011: 1653-1658 - ^ McCarthy & Hayes 1969. - ^ McCarthy 1959. - ^ Nilsson 1998, p. 7. - ^ Olazaran 1993, pp. 411–416. - ^ Olazaran 1993, pp. 415–416. - ^ Marcus 2020, p. 20. - ^ Garcez & Lamb 2020, p. 8. - ^ Zhang, Heng; Jiang, Guifei; Quan, Donghui (2025-04-11). \"A Theory of Formalisms for Representing Knowledge\". Proceedings of the AAAI Conference on Artificial Intelligence. 39 (14): 15257–15264. arXiv:2412.11855. doi:10.1609/aaai.v39i14.33674. ISSN 2374-3468. - ^ a b Russell & Norvig 2021, p. 982. - ^ Brooks 1991, p. 143. - ^ Brooks 1991, p. 151. - ^ Brooks 1991, p. 150. - ^ Brooks 1991, p. 142. References [edit]- Brooks, Rodney A. (1991). \"Intelligence without representation\". Artificial Intelligence. 47 (1): 139–159. doi:10.1016/0004-3702(91)90053-M. ISSN 0004-3702. S2CID 207507849. Retrieved 2022-09-13. - Clancey, William (1987). Knowledge-Based Tutoring: The GUIDON Program (MIT Press Series in Artificial Intelligence) (Hardcover ed.). - Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3.. - Dreyfus, Hubert L (1981). \"From micro-worlds to knowledge representation: AI at an impasse\" (PDF). Mind Design. MIT Press, Cambridge, MA: 161–204. - Garcez, Artur S. d'Avila; Broda, Krysia; Gabbay, Dov M.; Gabbay, Augustus de Morgan Professor of Logic Dov M. (2002). Neural-Symbolic Learning Systems: Foundations and Applications. Springer Science & Business Media. ISBN 978-1-85233-512-0. - Garcez, Artur; Besold, Tarek; De Raedt, Luc; Földiák, Peter; Hitzler, Pascal; Icard, Thomas; Kühnberger, Kai-Uwe; Lamb, Luís; Miikkulainen, Risto; Silver, Daniel (2015). Neural-Symbolic Learning and Reasoning: Contributions and Challenges. AAI Spring Symposium - Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches. Stanford, CA: AAAI Press. doi:10.13140/2.1.1779.4243. - Garcez, Artur d'Avila; Gori, Marco; Lamb, Luis C.; Serafini, Luciano; Spranger, Michael; Tran, Son N. (2019), Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning, arXiv:1905.06088 - Garcez, Artur d'Avila; Lamb, Luis C. (2020), Neurosymbolic AI: The 3rd Wave, arXiv:2012.05876 - Haugeland, John (1985), Artificial Intelligence: The Very Idea, Cambridge, Mass: MIT Press, ISBN 0-262-08153-9 - Hayes-Roth, Frederick; Murray, William; Adelman, Leonard (2015). \"Expert systems\". AccessScience. doi:10.1036/1097-8542.248550. - Honavar, Vasant; Uhr, Leonard (1994). Symbolic Artificial Intelligence, Connectionist Networks & Beyond (Technical report). Iowa State University Digital Repository, Computer Science Technical Reports. 76. p. 6. - Honavar, Vasant (1995). Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy. The Springer International Series In Engineering and Computer Science. Springer US. pp. 351–388. doi:10.1007/978-0-585-29599-2_11. - Howe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\". Archived from the original on 15 May 2007. Retrieved 30 August 2007. - Kautz, Henry (2020-02-11). The Third AI Summer, Henry Kautz, AAAI 2020 Robert S. Engelmore Memorial Award Lecture. Retrieved 2022-07-06. - Kautz, Henry (2022). \"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture\". AI Magazine. 43 (1): 93–104. doi:10.1609/aimag.v43i1.19122. ISSN 2371-9621. S2CID 248213051. Retrieved 2022-07-12. - Kodratoff, Yves; Michalski, Ryszard, eds. (1990). Machine Learning : an Artificial Intelligence Approach. Vol. III. San Mateo, Calif.: Morgan Kaufman. ISBN 0-934613-09-5. OCLC 893488404. - Kolata, G. (1982). \"How can computers get common sense?\". Science. 217 (4566): 1237–1238. Bibcode:1982Sci...217.1237K. doi:10.1126/science.217.4566.1237. PMID 17837639. - Maker, Meg Houston (2006). \"AI@50: AI Past, Present, Future\". Dartmouth College. Archived from the original on 3 January 2007. Retrieved 16 October 2008. - Marcus, Gary; Davis, Ernest (2019). Rebooting AI: Building Artificial Intelligence We Can Trust. New York: Pantheon Books. ISBN 9781524748258. OCLC 1083223029. - Marcus, Gary (2020), The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence, arXiv:2002.06177 - McCarthy, John (1959). PROGRAMS WITH COMMON SENSE. Symposium on Mechanization of Thought Processes. NATIONAL PHYSICAL LABORATORY, TEDDINGTON, UK. p. 8. - McCarthy, John; Hayes, Patrick (1969). \"Some Philosophical Problems From the Standpoint of Artificial Intelligence\". Machine Intelligence 4. B. Meltzer, Donald Michie (eds.): 463–502. - McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, Massachusetts: A. K. Peters, ISBN 1-5688-1205-1 - Michalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1983). Machine Learning : an Artificial Intelligence Approach. Vol. I. Palo Alto, Calif.: Tioga Publishing Company. ISBN 0-935382-05-4. OCLC 9262069. - Michalski, Ryszard; Carbonell, Jaime; Mitchell, Tom, eds. (1986). Machine Learning : an Artificial Intelligence Approach. Vol. II. Los Altos, Calif.: Morgan Kaufman. ISBN 0-934613-00-1. - Newell, Allen; Simon, Herbert A. (1972). Human Problem Solving (1st ed.). Englewood Cliffs, New Jersey: Prentice Hall. ISBN 0-13-445403-0. - Newell, Allen; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022. - Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019. - Olazaran, Mikel (1993-01-01), \"A Sociological History of the Neural Network Controversy\", in Yovits, Marshall C. (ed.), Advances in Computers Volume 37, vol. 37, Elsevier, pp. 335–425, doi:10.1016/S0065-2458(08)60408-8, ISBN 9780120121373, retrieved 2023-10-31 - Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, California: Morgan Kaufmann. ISBN 978-1-55860-479-7. OCLC 249625842. - Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3. LCCN 20190474. - Rossi, Francesca (2022-07-06). \"AAAI2022: Thinking Fast and Slow in AI (AAAI 2022 Invited Talk)\". Retrieved 2022-07-06. - Selman, Bart (2022-07-06). \"AAAI2022: Presidential Address: The State of AI\". Retrieved 2022-07-06. - Serafini, Luciano; Garcez, Artur d'Avila (2016-07-07), Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge, arXiv:1606.04422 - Spiegelhalter, David J.; Dawid, A. Philip; Lauritzen, Steffen; Cowell, Robert G. (1993). \"Bayesian analysis in expert systems\". Statistical Science. 8 (3). - Turing, A. M. (1950). \"I.—Computing Machinery and Intelligence\". Mind. LIX (236): 433–460. doi:10.1093/mind/LIX.236.433. ISSN 0026-4423. Retrieved 2022-09-14. - Valiant, Leslie G (2008). \"Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence\". In Hariharan, R.; Mukund, M.; Vinay, V. (eds.). Foundations of Software Technology and Theoretical Computer Science (Bangalore). pp. 415–422. - Xifan Yao; Jiajun Zhou; Jiangming Zhang; Claudio R. Boer (2017). \"From Intelligent Manufacturing to Smart Manufacturing for Industry 4.0 Driven by Next Generation Artificial Intelligence and Further on\". 2017 5th International Conference on Enterprise Systems (ES). IEEE. pp. 311–318. doi:10.1109/es.2017.58. ISBN 978-1-5386-0936-1.",
    "text_length": 75502,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:13.373627"
  },
  {
    "id": "page_13",
    "url": "https://en.wikipedia.org/wiki/Deep_learning",
    "domain": "en.wikipedia.org",
    "title": "Deep learning - Wikipedia",
    "text": "Deep learning | Part of a series on | | Artificial intelligence (AI) | |---| In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.[2] Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5] Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6] Overview [edit]Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7] Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2] The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8] Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12] The term deep learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16] Interpretations [edit]Deep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24] The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26] The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27] History [edit]Before 1980 [edit]There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model[28][29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was republished by John Hopfield in 1982.[32] Other early recurrent neural networks were published by Kaoru Nakano in 1971.[33][34] Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime,[35] containing \"ideas related to artificial evolution and learning RNNs\".[31] Frank Rosenblatt (1958)[36] proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]: section 16 The book cites an earlier network by R. D. Joseph (1960)[38] \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,[39] or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships.[40] A 1971 paper described a deep network with eight layers trained by this method,[41] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".[31] The first deep learning multilayer perceptron trained by stochastic gradient descent[42] was published in 1967 by Shun'ichi Amari.[43] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.[31] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[25][31] The rectifier has become the most popular activation function for deep learning.[44] Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[45][46] Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[47] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[37] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[48] The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).[49][50][31] G.M. Ostrovski et al. republished it in 1971.[51][52] Paul Werbos applied backpropagation to neural networks in 1982[53] (his 1974 PhD thesis, reprinted in a 1994 book,[54] did not yet describe the algorithm[52]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[55][56] 1980s-2000s [edit]The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58] In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59] In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[60] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[61] In 1991, a CNN was applied to medical image object segmentation[62] and breast cancer detection in mammograms.[63] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.[64] Recurrent neural networks (RNN)[28][30] were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[65] and the Elman network (1990),[66] which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68] This \"neural history compressor\" uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.[67][68][31] In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[69] The \"P\" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991)[70] implemented the neural history compressor,[67] and identified and analyzed the vanishing gradient problem.[70][71] Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[72] LSTM can learn \"very deep learning\" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[73] which became the standard RNN architecture. In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[74][75] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).[76] During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[77] restricted Boltzmann machine,[78] Helmholtz machine,[79] and the wake-sleep algorithm.[80] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 [81]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[82] Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[83][84][85] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[86] Key difficulties have been analyzed, including gradient diminishing[70] and weak temporal correlation structure in neural predictive models.[87][88] Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[89][90] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91] The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[90] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[92] 2000s [edit]Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed] In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93] In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[94] in stacks of LSTMs.[95] In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.[96][9] In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[97][98] deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.[99] They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.[100][101][102] The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103] Industrial applications of deep learning to large-scale speech recognition started around 2010. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105] That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108] In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106] Deep learning revolution [edit]The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112] including CNNs,[113] faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114] A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113] In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115] In 2011, a CNN named DanNet[116][117] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9] It then won more contests.[118][119] They also showed how max-pooling CNNs on GPU improved performance significantly.[3] In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[120] In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[121] and Google's Inceptionv3.[122] The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[123][124][125] In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.[126] Stacking too many layers led to a steep reduction in training accuracy,[127] known as the \"degradation\" problem.[128] In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet)[129] in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015),[130] both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014)[131] (based on Jürgen Schmidhuber's principle of artificial curiosity[74][76]) became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[132] based on the Progressive GAN by Tero Karras et al.[133] Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[134] Diffusion models (2015)[135] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.[136][137] Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][138] Convolutional neural networks were superseded for ASR by LSTM.[137][139][140][141] but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".[142] Neural networks [edit]Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"[144]). Deep neural networks [edit]A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[145] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed] For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[146] and complex DNN have many layers, hence the name \"deep\" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[147] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[148] Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[146] DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[149] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[150][151][152][153][154] Long short-term memory is particularly effective for this use.[155][156] Convolutional neural networks (CNNs) are used in computer vision.[157] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[158] Challenges [edit]As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[41] or weight decay (-regularization) or sparsity (-regularization) can be applied during training to combat overfitting.[159] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[160] Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[161] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[162] DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[163] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[164][165] Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[166][167] Hardware [edit]Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[168] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[169] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[170][171] Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[172] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[173] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[174][175] Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[176] In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[177] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[177] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[177] Applications [edit]Automatic speech recognition [edit]Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[9] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[156] is competitive with traditional speech recognizers on certain tasks.[93] The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[178] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991. | Method | Percent phone error rate (PER) (%) | |---|---| | Randomly Initialized RNN[179] | 26.1 | | Bayesian Triphone GMM-HMM | 25.6 | | Hidden Trajectory (Generative) Model | 24.8 | | Monophone Randomly Initialized DNN | 23.4 | | Monophone DBN-DNN | 22.4 | | Triphone GMM-HMM with BMMI Training | 21.7 | | Monophone DBN-DNN on fbank | 20.7 | | Convolutional DNN[180] | 20.0 | | Convolutional DNN w. Heterogeneous Pooling | 18.7 | | Ensemble DNN/CNN/RNN[181] | 18.3 | | Bidirectional LSTM | 17.8 | | Hierarchical Convolutional Deep Maxout Network[182] | 16.5 | The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[23][108][106] - Scale-up/out and accelerated DNN training and decoding - Sequence discriminative training - Feature processing by deep models with solid understanding of the underlying mechanisms - Adaptation of DNNs and related deep models - Multi-task and transfer learning by DNNs and related deep models - CNNs and how to design them to best exploit domain knowledge of speech - RNN and its rich LSTM variants - Other types of deep models including tensor-based models and integrated deep generative/discriminative models. More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications.[183][184][185] All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[23][186][187] Image recognition [edit]A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[188] Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[189][190] Deep learning-trained vehicles now interpret 360° camera views.[191] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. Visual art processing [edit]Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of - identifying the style period of a given painting[192][193] - Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video[192][193] - generating striking imagery based on random visual input fields.[192][193] Natural language processing [edit]Neural networks have been used for implementing language models since the early 2000s.[150] LSTM helped to improve machine translation and language modeling.[151][152][153] Other key techniques in this field are negative sampling[194] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[195] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[195] Deep neural architectures provide the best results for constituency parsing,[196] sentiment analysis,[197] information retrieval,[198][199] spoken language understanding,[200] machine translation,[151][201] contextual entity linking,[201] writing style recognition,[202] named-entity recognition (token classification),[203] text classification, and others.[204] Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[205][206][207][208] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\".[206] It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.[206] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[206][209] GT uses English as an intermediate between most language pairs.[209] Drug discovery and toxicology [edit]A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[210][211] Research has explored use of deep learning to predict the biomolecular targets,[212][213] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[214][215][216] AtomNet is a deep learning system for structure-based rational drug design.[217] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[218] and multiple sclerosis.[219][218] In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[220] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[221][222] Recommendation systems [edit]Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[223][224] Multi-view deep learning has been applied for learning user preferences from multiple domains.[225] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. Bioinformatics [edit]An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[226] In medical informatics, deep learning was used to predict sleep quality based on data from wearables[227] and predictions of health complications from electronic health record data.[228] Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[229][230] Deep Neural Network Estimations [edit]Deep neural networks can be used to estimate the entropy of a stochastic process through an arrangement called a Neural Joint Entropy Estimator (NJEE).[231] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in cases of large alphabet sizes.[231] Medical image analysis [edit]Deep learning has been shown to produce competitive results in medical applications such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[232][233] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[234][235] Mobile advertising [edit]Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[236] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. Image restoration [edit]Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[237] These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[238] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration. Financial fraud detection [edit]Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[239] and anti-money laundering.[240] Materials science [edit]In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[241][242][243] Military [edit]The United States Department of Defense applied deep learning to train robots in new tasks through observation.[244] Partial differential equations [edit]Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[245] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.[246][247] It is evident that geometric and physical constraints have a synergistic effect on neural PDE surrogates, thereby enhancing their efficacy in predicting stable and super long rollouts.[248] Deep backward stochastic differential equation method [edit]Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[249] In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems. Image reconstruction [edit]Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [250] and ultrasound imaging.[251] Weather prediction [edit]Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[252][253] Epigenetic clock [edit]An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[254] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. Relation to human cognitive and brain development [edit]Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[255][256][257][258] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".[259] A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[260][261] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[262][263] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[264] Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[265] and neural populations.[266] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[267] both at the single-unit[268] and at the population[269] levels. Commercial activity [edit]Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[270] Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[271][272][273] Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[274] As of 2008,[275] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[244] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[244] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".[276] Criticism and comment [edit]Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory [edit]A main criticism concerns the lack of theory surrounding some methods.[277] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[278] In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[279] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[280] website. With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning[281] in BRICS and OECD countries using data from Google Trends. Errors [edit]Some deep learning architectures display problematic behaviors,[282] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[283] and misclassifying minuscule perturbations of correctly classified images (2013).[284] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[282] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[285] decompositions of observed entities and events.[282] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[286] and artificial intelligence (AI).[287] Cyber threat [edit]As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[288] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".[289] In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[290] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[291] Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[290] ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[290] In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".[290] In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[290] Data collection ethics [edit]The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both.[292] It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[293] The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[293] See also [edit]- Applications of artificial intelligence - Comparison of deep learning software - Compressed sensing - Differentiable programming - Echo state network - List of artificial intelligence projects - Liquid state machine - List of datasets for machine-learning research - Reservoir computing - Scale space and deep learning - Sparse coding - Stochastic parrot - Topological deep learning References [edit]- ^ Schulz, Hannes; Behnke, Sven (1 November 2012). \"Deep Learning\". KI - Künstliche Intelligenz. 26 (4): 357–363. doi:10.1007/s13218-012-0198-z. ISSN 1610-1987. S2CID 220523562. - ^ a b LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). \"Deep Learning\" (PDF). Nature. 521 (7553): 436–444. Bibcode:2015Natur.521..436L. doi:10.1038/nature14539. PMID 26017442. S2CID 3074096. - ^ a b Ciresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image classification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8. S2CID 2161592. - ^ a b Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey (2012). \"ImageNet Classification with Deep Convolutional Neural Networks\" (PDF). NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada. Archived (PDF) from the original on 2017-01-10. Retrieved 2017-05-24. - ^ \"Google's AlphaGo AI wins three-match series against the world's best Go player\". TechCrunch. 25 May 2017. Archived from the original on 17 June 2018. Retrieved 17 June 2018. - ^ \"Study urges caution when comparing neural networks to the brain\". MIT News | Massachusetts Institute of Technology. 2022-11-02. Retrieved 2023-12-06. - ^ a b c d Bengio, Yoshua (2009). \"Learning Deep Architectures for AI\" (PDF). Foundations and Trends in Machine Learning. 2 (1): 1–127. CiteSeerX 10.1.1.701.9550. doi:10.1561/2200000006. S2CID 207178999. Archived from the original (PDF) on 4 March 2016. Retrieved 3 September 2015. - ^ a b c d e Bengio, Y.; Courville, A.; Vincent, P. (2013). \"Representation Learning: A Review and New Perspectives\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8): 1798–1828. arXiv:1206.5538. Bibcode:2013ITPAM..35.1798B. doi:10.1109/tpami.2013.50. PMID 23787338. S2CID 393948. - ^ a b c d e f g h Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117. arXiv:1404.7828. Bibcode:2015NN.....61...85S. doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509. - ^ Shigeki, Sugiyama (12 April 2019). Human Behavior and Another Kind in Consciousness: Emerging Research and Opportunities: Emerging Research and Opportunities. IGI Global. ISBN 978-1-5225-8218-2. - ^ Bengio, Yoshua; Lamblin, Pascal; Popovici, Dan; Larochelle, Hugo (2007). Greedy layer-wise training of deep networks (PDF). Advances in neural information processing systems. pp. 153–160. Archived (PDF) from the original on 2019-10-20. Retrieved 2019-10-06. - ^ a b Hinton, G.E. (2009). \"Deep belief networks\". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947. - ^ Rina Dechter (1986). Learning while searching in constraint-satisfaction problems. University of California, Computer Science Department, Cognitive Systems Laboratory.Online Archived 2016-04-19 at the Wayback Machine - ^ Aizenberg, I.N.; Aizenberg, N.N.; Vandewalle, J. (2000). Multi-Valued and Universal Binary Neurons. Science & Business Media. doi:10.1007/978-1-4757-3115-6. ISBN 978-0-7923-7824-2. Retrieved 27 December 2023. - ^ Co-evolving recurrent neurons learn deep memory POMDPs. Proc. GECCO, Washington, D. C., pp. 1795–1802, ACM Press, New York, NY, USA, 2005. - ^ Fradkov, Alexander L. (2020-01-01). \"Early History of Machine Learning\". IFAC-PapersOnLine. 21st IFAC World Congress. 53 (2): 1385–1390. doi:10.1016/j.ifacol.2020.12.1888. ISSN 2405-8963. S2CID 235081987. - ^ a b c Cybenko (1989). \"Approximations by superpositions of sigmoidal functions\" (PDF). Mathematics of Control, Signals, and Systems. 2 (4): 303–314. Bibcode:1989MCSS....2..303C. doi:10.1007/bf02551274. S2CID 3958369. Archived from the original (PDF) on 10 October 2015. - ^ a b c Hornik, Kurt (1991). \"Approximation Capabilities of Multilayer Feedforward Networks\". Neural Networks. 4 (2): 251–257. Bibcode:1991NN......4..251H. doi:10.1016/0893-6080(91)90009-t. S2CID 7343126. - ^ a b Haykin, Simon S. (1999). Neural Networks: A Comprehensive Foundation. Prentice Hall. ISBN 978-0-13-273350-2. - ^ a b Hassoun, Mohamad H. (1995). Fundamentals of Artificial Neural Networks. MIT Press. p. 48. ISBN 978-0-262-08239-6. - ^ a b Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). The Expressive Power of Neural Networks: A View from the Width Archived 2019-02-13 at the Wayback Machine. Neural Information Processing Systems, 6231-6239. - ^ Orhan, A. E.; Ma, W. J. (2017). \"Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback\". Nature Communications. 8 (1): 138. Bibcode:2017NatCo...8..138O. doi:10.1038/s41467-017-00181-8. PMC 5527101. PMID 28743932. - ^ a b c d e Deng, L.; Yu, D. (2014). \"Deep Learning: Methods and Applications\" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039. Archived (PDF) from the original on 2016-03-14. Retrieved 2014-10-18. - ^ a b c d Murphy, Kevin P. (24 August 2012). Machine Learning: A Probabilistic Perspective. MIT Press. ISBN 978-0-262-01802-9. - ^ a b Fukushima, K. (1969). \"Visual feature extraction by a multilayered network of analog threshold elements\". IEEE Transactions on Systems Science and Cybernetics. 5 (4): 322–333. Bibcode:1969ITSSC...5..322F. doi:10.1109/TSSC.1969.300225. - ^ Sonoda, Sho; Murata, Noboru (2017). \"Neural network with unbounded activation functions is universal approximator\". Applied and Computational Harmonic Analysis. 43 (2): 233–268. arXiv:1505.03654. doi:10.1016/j.acha.2015.12.005. S2CID 12149203. - ^ Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning (PDF). Springer. ISBN 978-0-387-31073-2. Archived (PDF) from the original on 2017-01-11. Retrieved 2017-08-06. - ^ a b \"bibliotheca Augustana\". www.hs-augsburg.de. - ^ Brush, Stephen G. (1967). \"History of the Lenz-Ising Model\". Reviews of Modern Physics. 39 (4): 883–893. Bibcode:1967RvMP...39..883B. doi:10.1103/RevModPhys.39.883. - ^ a b Amari, Shun-Ichi (1972). \"Learning patterns and pattern sequences by self-organizing nets of threshold elements\". IEEE Transactions. C (21): 1197–1206. - ^ a b c d e f g Schmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\". arXiv:2212.11279 [cs.NE]. - ^ Hopfield, J. J. (1982). \"Neural networks and physical systems with emergent collective computational abilities\". Proceedings of the National Academy of Sciences. 79 (8): 2554–2558. Bibcode:1982PNAS...79.2554H. doi:10.1073/pnas.79.8.2554. PMC 346238. PMID 6953413. - ^ Nakano, Kaoru (1971). \"Learning Process in a Model of Associative Memory\". Pattern Recognition and Machine Learning. pp. 172–186. doi:10.1007/978-1-4615-7566-5_15. ISBN 978-1-4615-7568-9. - ^ Nakano, Kaoru (1972). \"Associatron-A Model of Associative Memory\". IEEE Transactions on Systems, Man, and Cybernetics. SMC-2 (3): 380–388. Bibcode:1972ITSMC...2..380N. doi:10.1109/TSMC.1972.4309133. - ^ Turing, Alan (1992) [1948]. \"Intelligent Machinery\". In Ince, D.C. (ed.). Collected Works of AM Turing: Mechanical Intelligence. Vol. 1. Elsevier Science Publishers. p. 107. ISBN 0-444-88058-5. - ^ Rosenblatt, F. (1958). \"The perceptron: A probabilistic model for information storage and organization in the brain\". Psychological Review. 65 (6): 386–408. doi:10.1037/h0042519. ISSN 1939-1471. PMID 13602029. - ^ a b Rosenblatt, Frank (1962). Principles of Neurodynamics. Spartan, New York. - ^ Joseph, R. D. (1960). Contributions to Perceptron Theory, Cornell Aeronautical Laboratory Report No. VG-11 96--G-7, Buffalo. - ^ Ivakhnenko, A. G.; Lapa, V. G. (1967). Cybernetics and Forecasting Techniques. American Elsevier Publishing Co. ISBN 978-0-444-00020-0. - ^ Ivakhnenko, A.G. (March 1970). \"Heuristic self-organization in problems of engineering cybernetics\". Automatica. 6 (2): 207–219. Bibcode:1970Autom...6..207I. doi:10.1016/0005-1098(70)90092-0. - ^ a b Ivakhnenko, Alexey (1971). \"Polynomial theory of complex systems\" (PDF). IEEE Transactions on Systems, Man, and Cybernetics. SMC-1 (4): 364–378. Bibcode:1971ITSMC...1..364I. doi:10.1109/TSMC.1971.4308320. Archived (PDF) from the original on 2017-08-29. Retrieved 2019-11-05. - ^ Robbins, H.; Monro, S. (1951). \"A Stochastic Approximation Method\". The Annals of Mathematical Statistics. 22 (3): 400. doi:10.1214/aoms/1177729586. - ^ Amari, Shun'ichi (1967). \"A theory of adaptive pattern classifier\". IEEE Transactions. EC (16): 279–307. - ^ Ramachandran, Prajit; Barret, Zoph; Quoc, V. Le (October 16, 2017). \"Searching for Activation Functions\". arXiv:1710.05941 [cs.NE]. - ^ Fukushima, K. (1979). \"Neural network model for a mechanism of pattern recognition unaffected by shift in position—Neocognitron\". Trans. IECE (In Japanese). J62-A (10): 658–665. doi:10.1007/bf00344251. PMID 7370364. S2CID 206775608. - ^ Fukushima, K. (1980). \"Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\". Biol. Cybern. 36 (4): 193–202. doi:10.1007/bf00344251. PMID 7370364. S2CID 206775608. - ^ Leibniz, Gottfried Wilhelm Freiherr von (1920). The Early Mathematical Manuscripts of Leibniz: Translated from the Latin Texts Published by Carl Immanuel Gerhardt with Critical and Historical Notes (Leibniz published the chain rule in a 1676 memoir). Open court publishing Company. ISBN 978-0-598-81846-1. {{cite book}} : ISBN / Date incompatibility (help) - ^ Kelley, Henry J. (1960). \"Gradient theory of optimal flight paths\". ARS Journal. 30 (10): 947–954. doi:10.2514/8.5282. - ^ Linnainmaa, Seppo (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors (Masters) (in Finnish). University of Helsinki. p. 6–7. - ^ Linnainmaa, Seppo (1976). \"Taylor expansion of the accumulated rounding error\". BIT Numerical Mathematics. 16 (2): 146–160. doi:10.1007/bf01931367. S2CID 122357351. - ^ Ostrovski, G.M., Volin,Y.M., and Boris, W.W. (1971). On the computation of derivatives. Wiss. Z. Tech. Hochschule for Chemistry, 13:382–384. - ^ a b Schmidhuber, Juergen (25 Oct 2014). \"Who Invented Backpropagation?\". IDSIA, Switzerland. Archived from the original on 30 July 2024. Retrieved 14 Sep 2024. - ^ Werbos, Paul (1982). \"Applications of advances in nonlinear sensitivity analysis\" (PDF). System modeling and optimization. Springer. pp. 762–770. Archived (PDF) from the original on 14 April 2016. Retrieved 2 July 2017. - ^ Werbos, Paul J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting. New York: John Wiley & Sons. ISBN 0-471-59897-6. - ^ Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J. (October 1986). \"Learning representations by back-propagating errors\". Nature. 323 (6088): 533–536. Bibcode:1986Natur.323..533R. doi:10.1038/323533a0. ISSN 1476-4687. - ^ Rumelhart, David E., Geoffrey E. Hinton, and R. J. Williams. \"Learning Internal Representations by Error Propagation Archived 2022-10-13 at the Wayback Machine\". David E. Rumelhart, James L. McClelland, and the PDP research group. (editors), Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundation. MIT Press, 1986. - ^ Waibel, Alex (December 1987). Phoneme Recognition Using Time-Delay Neural Networks (PDF). Meeting of the Institute of Electrical, Information and Communication Engineers (IEICE). Tokyo, Japan. - ^ Alexander Waibel et al., Phoneme Recognition Using Time-Delay Neural Networks IEEE Transactions on Acoustics, Speech, and Signal Processing, Volume 37, No. 3, pp. 328. – 339 March 1989. - ^ Zhang, Wei (1988). \"Shift-invariant pattern recognition neural network and its optical architecture\". Proceedings of Annual Conference of the Japan Society of Applied Physics. - ^ LeCun et al., \"Backpropagation Applied to Handwritten Zip Code Recognition\", Neural Computation, 1, pp. 541–551, 1989. - ^ Zhang, Wei (1990). \"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\". Applied Optics. 29 (32): 4790–7. Bibcode:1990ApOpt..29.4790Z. doi:10.1364/AO.29.004790. PMID 20577468. - ^ Zhang, Wei (1991). \"Image processing of human corneal endothelium based on a learning network\". Applied Optics. 30 (29): 4211–7. Bibcode:1991ApOpt..30.4211Z. doi:10.1364/AO.30.004211. PMID 20706526. - ^ Zhang, Wei (1994). \"Computerized detection of clustered microcalcifications in digital mammograms using a shift-invariant artificial neural network\". Medical Physics. 21 (4): 517–24. Bibcode:1994MedPh..21..517Z. doi:10.1118/1.597177. PMID 8058017. - ^ LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). \"Gradient-based learning applied to document recognition\" (PDF). Proceedings of the IEEE. 86 (11): 2278–2324. Bibcode:1998IEEEP..86.2278L. CiteSeerX 10.1.1.32.9552. doi:10.1109/5.726791. S2CID 14542261. Retrieved October 7, 2016. - ^ Jordan, Michael I. (1986). \"Attractor dynamics and parallelism in a connectionist sequential machine\". Proceedings of the Annual Meeting of the Cognitive Science Society. 8. - ^ Elman, Jeffrey L. (March 1990). \"Finding Structure in Time\". Cognitive Science. 14 (2): 179–211. doi:10.1207/s15516709cog1402_1. ISSN 0364-0213. - ^ a b c Schmidhuber, Jürgen (April 1991). \"Neural Sequence Chunkers\" (PDF). TR FKI-148, TU Munich. - ^ a b Schmidhuber, Jürgen (1992). \"Learning complex, extended sequences using the principle of history compression (based on TR FKI-148, 1991)\" (PDF). Neural Computation. 4 (2): 234–242. doi:10.1162/neco.1992.4.2.234. S2CID 18271205. - ^ Schmidhuber, Jürgen (1993). Habilitation thesis: System modeling and optimization (PDF). Archived from the original (PDF) on May 16, 2022. Page 150 ff demonstrates credit assignment across the equivalent of 1,200 layers in an unfolded RNN. - ^ a b c S. Hochreiter., \"Untersuchungen zu dynamischen neuronalen Netzen\". Archived 2015-03-06 at the Wayback Machine. Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber, 1991. - ^ Hochreiter, S.; et al. (15 January 2001). \"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\". In Kolen, John F.; Kremer, Stefan C. (eds.). A Field Guide to Dynamical Recurrent Networks. John Wiley & Sons. ISBN 978-0-7803-5369-5. - ^ Sepp Hochreiter; Jürgen Schmidhuber (21 August 1995), Long Short Term Memory, Wikidata Q98967430 - ^ Gers, Felix; Schmidhuber, Jürgen; Cummins, Fred (1999). \"Learning to forget: Continual prediction with LSTM\". 9th International Conference on Artificial Neural Networks: ICANN '99. Vol. 1999. pp. 850–855. doi:10.1049/cp:19991218. ISBN 0-85296-721-7. - ^ a b Schmidhuber, Jürgen (1991). \"A possibility for implementing curiosity and boredom in model-building neural controllers\". Proc. SAB'1991. MIT Press/Bradford Books. pp. 222–227. - ^ Schmidhuber, Jürgen (2010). \"Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010)\". IEEE Transactions on Autonomous Mental Development. 2 (3): 230–247. Bibcode:2010ITAMD...2..230S. doi:10.1109/TAMD.2010.2056368. S2CID 234198. - ^ a b Schmidhuber, Jürgen (2020). \"Generative Adversarial Networks are Special Cases of Artificial Curiosity (1990) and also Closely Related to Predictability Minimization (1991)\". Neural Networks. 127: 58–66. arXiv:1906.04493. doi:10.1016/j.neunet.2020.04.008. PMID 32334341. S2CID 216056336. - ^ Ackley, David H.; Hinton, Geoffrey E.; Sejnowski, Terrence J. (1985-01-01). \"A learning algorithm for boltzmann machines\". Cognitive Science. 9 (1): 147–169. doi:10.1016/S0364-0213(85)80012-4. ISSN 0364-0213. - ^ Smolensky, Paul (1986). \"Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory\" (PDF). In Rumelhart, David E.; McLelland, James L. (eds.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations. MIT Press. pp. 194–281. ISBN 0-262-68053-X. - ^ Peter, Dayan; Hinton, Geoffrey E.; Neal, Radford M.; Zemel, Richard S. (1995). \"The Helmholtz machine\". Neural Computation. 7 (5): 889–904. doi:10.1162/neco.1995.7.5.889. hdl:21.11116/0000-0002-D6D3-E. PMID 7584891. S2CID 1890561. - ^ Hinton, Geoffrey E.; Dayan, Peter; Frey, Brendan J.; Neal, Radford (1995-05-26). \"The wake-sleep algorithm for unsupervised neural networks\". Science. 268 (5214): 1158–1161. Bibcode:1995Sci...268.1158H. doi:10.1126/science.7761831. PMID 7761831. S2CID 871473. - ^ Sejnowski, Terrence J. (2018). The Deep Learning Revolution. Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-03803-4. - ^ Qian, Ning; Sejnowski, Terrence J. (1988-08-20). \"Predicting the secondary structure of globular proteins using neural network models\". Journal of Molecular Biology. 202 (4): 865–884. Bibcode:1988JMBio.202..865Q. doi:10.1016/0022-2836(88)90564-5. ISSN 0022-2836. PMID 3172241. - ^ Morgan, Nelson; Bourlard, Hervé; Renals, Steve; Cohen, Michael; Franco, Horacio (1 August 1993). \"Hybrid neural network/hidden markov model systems for continuous speech recognition\". International Journal of Pattern Recognition and Artificial Intelligence. 07 (4): 899–916. doi:10.1142/s0218001493000455. ISSN 0218-0014. - ^ Robinson, T. (1992). \"A real-time recurrent error propagation network word recognition system\". ICASSP. Icassp'92: 617–620. ISBN 978-0-7803-0532-8. Archived from the original on 2021-05-09. Retrieved 2017-06-12. - ^ Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (March 1989). \"Phoneme recognition using time-delay neural networks\" (PDF). IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. Bibcode:1989ITASS..37..328W. doi:10.1109/29.21701. hdl:10338.dmlcz/135496. ISSN 0096-3518. S2CID 9563026. Archived (PDF) from the original on 2021-04-27. Retrieved 2019-09-24. - ^ Baker, J.; Deng, Li; Glass, Jim; Khudanpur, S.; Lee, C.-H.; Morgan, N.; O'Shaughnessy, D. (2009). \"Research Developments and Directions in Speech Recognition and Understanding, Part 1\". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/msp.2009.932166. hdl:1721.1/51891. S2CID 357467. - ^ Bengio, Y. (1991). \"Artificial Neural Networks and their Application to Speech/Sequence Recognition\". McGill University Ph.D. thesis. Archived from the original on 2021-05-09. Retrieved 2017-06-12. - ^ Deng, L.; Hassanein, K.; Elmasry, M. (1994). \"Analysis of correlation structure for a neural predictive model with applications to speech recognition\". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2. - ^ Doddington, G.; Przybocki, M.; Martin, A.; Reynolds, D. (2000). \"The NIST speaker recognition evaluation ± Overview, methodology, systems, results, perspective\". Speech Communication. 31 (2): 225–254. doi:10.1016/S0167-6393(99)00080-1. - ^ a b Heck, L.; Konig, Y.; Sonmez, M.; Weintraub, M. (2000). \"Robustness to Telephone Handset Distortion in Speaker Recognition by Discriminative Feature Design\". Speech Communication. 31 (2): 181–192. doi:10.1016/s0167-6393(99)00077-1. - ^ L.P Heck and R. Teunen. \"Secure and Convenient Transactions with Nuance Verifier\". Nuance Users Conference, April 1998. - ^ \"Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR (PDF Download Available)\". ResearchGate. Archived from the original on 9 May 2021. Retrieved 14 June 2017. - ^ a b Graves, Alex; Eck, Douglas; Beringer, Nicole; Schmidhuber, Jürgen (2003). \"Biologically Plausible Speech Recognition with LSTM Neural Nets\" (PDF). 1st Intl. Workshop on Biologically Inspired Approaches to Advanced Information Technology, Bio-ADIT 2004, Lausanne, Switzerland. pp. 175–184. Archived from the original (PDF) on 2017-07-06. Retrieved 2016-04-09. - ^ Graves, Alex; Fernández, Santiago; Gomez, Faustino; Schmidhuber, Jürgen (2006). \"Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks\". Proceedings of the International Conference on Machine Learning, ICML 2006: 369–376. CiteSeerX 10.1.1.75.6306. - ^ Santiago Fernandez, Alex Graves, and Jürgen Schmidhuber (2007). An application of recurrent neural networks to discriminative keyword spotting Archived 2018-11-18 at the Wayback Machine. Proceedings of ICANN (2), pp. 220–229. - ^ Graves, Alex; and Schmidhuber, Jürgen; Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks, in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC, Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552 - ^ Hinton, Geoffrey E. (1 October 2007). \"Learning multiple layers of representation\". Trends in Cognitive Sciences. 11 (10): 428–434. doi:10.1016/j.tics.2007.09.004. ISSN 1364-6613. PMID 17921042. S2CID 15066318. Archived from the original on 11 October 2013. Retrieved 12 June 2017. - ^ Hinton, G. E.; Osindero, S.; Teh, Y. W. (2006). \"A Fast Learning Algorithm for Deep Belief Nets\" (PDF). Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. PMID 16764513. S2CID 2309950. Archived (PDF) from the original on 2015-12-23. Retrieved 2011-07-20. - ^ G. E. Hinton., \"Learning multiple layers of representation\". Archived 2018-05-22 at the Wayback Machine. Trends in Cognitive Sciences, 11, pp. 428–434, 2007. - ^ Hinton, Geoffrey E. (October 2007). \"Learning multiple layers of representation\". Trends in Cognitive Sciences. 11 (10): 428–434. doi:10.1016/j.tics.2007.09.004. PMID 17921042. - ^ Hinton, Geoffrey E.; Osindero, Simon; Teh, Yee-Whye (July 2006). \"A Fast Learning Algorithm for Deep Belief Nets\". Neural Computation. 18 (7): 1527–1554. doi:10.1162/neco.2006.18.7.1527. ISSN 0899-7667. PMID 16764513. - ^ Hinton, Geoffrey E. (2009-05-31). \"Deep belief networks\". Scholarpedia. 4 (5): 5947. Bibcode:2009SchpJ...4.5947H. doi:10.4249/scholarpedia.5947. ISSN 1941-6016. - ^ Yann LeCun (2016). Slides on Deep Learning Online Archived 2016-04-23 at the Wayback Machine - ^ a b c Hinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups\". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/msp.2012.2205597. S2CID 206485943. - ^ a b c Deng, L.; Hinton, G.; Kingsbury, B. (May 2013). \"New types of deep neural network learning for speech recognition and related applications: An overview (ICASSP)\" (PDF). Microsoft. Archived (PDF) from the original on 2017-09-26. Retrieved 27 December 2023. - ^ a b c Yu, D.; Deng, L. (2014). Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer). Springer. ISBN 978-1-4471-5779-3. - ^ \"Deng receives prestigious IEEE Technical Achievement Award - Microsoft Research\". Microsoft Research. 3 December 2015. Archived from the original on 16 March 2018. Retrieved 16 March 2018. - ^ a b Li, Deng (September 2014). \"Keynote talk: 'Achievements and Challenges of Deep Learning - From Speech Analysis and Recognition To Language and Multimodal Processing'\". Interspeech. Archived from the original on 2017-09-26. Retrieved 2017-06-12. - ^ Yu, D.; Deng, L. (2010). \"Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition\". NIPS Workshop on Deep Learning and Unsupervised Feature Learning. Archived from the original on 2017-10-12. Retrieved 2017-06-14. - ^ Seide, F.; Li, G.; Yu, D. (2011). \"Conversational speech transcription using context-dependent deep neural networks\". Interspeech 2011. pp. 437–440. doi:10.21437/Interspeech.2011-169. S2CID 398770. Archived from the original on 2017-10-12. Retrieved 2017-06-14. - ^ Deng, Li; Li, Jinyu; Huang, Jui-Ting; Yao, Kaisheng; Yu, Dong; Seide, Frank; Seltzer, Mike; Zweig, Geoff; He, Xiaodong (1 May 2013). \"Recent Advances in Deep Learning for Speech Research at Microsoft\". Microsoft Research. Archived from the original on 12 October 2017. Retrieved 14 June 2017. - ^ a b Oh, K.-S.; Jung, K. (2004). \"GPU implementation of neural networks\". Pattern Recognition. 37 (6): 1311–1314. Bibcode:2004PatRe..37.1311O. doi:10.1016/j.patcog.2004.01.013. - ^ a b Chellapilla, Kumar; Puri, Sidd; Simard, Patrice (2006), High performance convolutional neural networks for document processing, archived from the original on 2020-05-18, retrieved 2021-02-14 - ^ Sze, Vivienne; Chen, Yu-Hsin; Yang, Tien-Ju; Emer, Joel (2017). \"Efficient Processing of Deep Neural Networks: A Tutorial and Survey\". arXiv:1703.09039 [cs.CV]. - ^ Raina, Rajat; Madhavan, Anand; Ng, Andrew Y. (2009-06-14). \"Large-scale deep unsupervised learning using graphics processors\". Proceedings of the 26th Annual International Conference on Machine Learning. ICML '09. New York, NY, USA: Association for Computing Machinery. pp. 873–880. doi:10.1145/1553374.1553486. ISBN 978-1-60558-516-1. - ^ Cireşan, Dan Claudiu; Meier, Ueli; Gambardella, Luca Maria; Schmidhuber, Jürgen (21 September 2010). \"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition\". Neural Computation. 22 (12): 3207–3220. arXiv:1003.0358. Bibcode:2010NeCom..22.3207C. doi:10.1162/neco_a_00052. ISSN 0899-7667. PMID 20858131. S2CID 1918673. - ^ Ciresan, D. C.; Meier, U.; Masci, J.; Gambardella, L.M.; Schmidhuber, J. (2011). \"Flexible, High Performance Convolutional Neural Networks for Image Classification\" (PDF). International Joint Conference on Artificial Intelligence. doi:10.5591/978-1-57735-516-8/ijcai11-210. Archived (PDF) from the original on 2014-09-29. Retrieved 2017-06-13. - ^ Ciresan, Dan; Giusti, Alessandro; Gambardella, Luca M.; Schmidhuber, Jürgen (2012). Pereira, F.; Burges, C. J. C.; Bottou, L.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 25 (PDF). Curran Associates, Inc. pp. 2843–2851. Archived (PDF) from the original on 2017-08-09. Retrieved 2017-06-13. - ^ Ciresan, D.; Giusti, A.; Gambardella, L.M.; Schmidhuber, J. (2013). \"Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks\". Medical Image Computing and Computer-Assisted Intervention – MICCAI 2013. Lecture Notes in Computer Science. Vol. 7908. pp. 411–418. doi:10.1007/978-3-642-40763-5_51. ISBN 978-3-642-38708-1. PMID 24579167. - ^ Ng, Andrew; Dean, Jeff (2012). \"Building High-level Features Using Large Scale Unsupervised Learning\". arXiv:1112.6209 [cs.LG]. - ^ Simonyan, Karen; Andrew, Zisserman (2014). \"Very Deep Convolution Networks for Large Scale Image Recognition\". arXiv:1409.1556 [cs.CV]. - ^ Szegedy, Christian (2015). \"Going deeper with convolutions\" (PDF). Cvpr2015. arXiv:1409.4842. - ^ Vinyals, Oriol; Toshev, Alexander; Bengio, Samy; Erhan, Dumitru (2014). \"Show and Tell: A Neural Image Caption Generator\". arXiv:1411.4555 [cs.CV].. - ^ Fang, Hao; Gupta, Saurabh; Iandola, Forrest; Srivastava, Rupesh; Deng, Li; Dollár, Piotr; Gao, Jianfeng; He, Xiaodong; Mitchell, Margaret; Platt, John C; Lawrence Zitnick, C; Zweig, Geoffrey (2014). \"From Captions to Visual Concepts and Back\". arXiv:1411.4952 [cs.CV].. - ^ Kiros, Ryan; Salakhutdinov, Ruslan; Zemel, Richard S (2014). \"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\". arXiv:1411.2539 [cs.LG].. - ^ Simonyan, Karen; Zisserman, Andrew (2015-04-10), Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv:1409.1556 - ^ He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (2016). \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\". arXiv:1502.01852 [cs.CV]. - ^ He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (10 Dec 2015). Deep Residual Learning for Image Recognition. arXiv:1512.03385. - ^ He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (2016). \"Deep Residual Learning for Image Recognition\". 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USA: IEEE. pp. 770–778. arXiv:1512.03385. doi:10.1109/CVPR.2016.90. ISBN 978-1-4673-8851-1. - ^ Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias (26 August 2015). \"A Neural Algorithm of Artistic Style\". arXiv:1508.06576 [cs.CV]. - ^ Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014). Generative Adversarial Networks (PDF). Proceedings of the International Conference on Neural Information Processing Systems (NIPS 2014). pp. 2672–2680. Archived (PDF) from the original on 22 November 2019. Retrieved 20 August 2019. - ^ \"GAN 2.0: NVIDIA's Hyperrealistic Face Generator\". SyncedReview.com. December 14, 2018. Retrieved October 3, 2019. - ^ Karras, T.; Aila, T.; Laine, S.; Lehtinen, J. (26 February 2018). \"Progressive Growing of GANs for Improved Quality, Stability, and Variation\". arXiv:1710.10196 [cs.NE]. - ^ \"Prepare, Don't Panic: Synthetic Media and Deepfakes\". witness.org. Archived from the original on 2 December 2020. Retrieved 25 November 2020. - ^ Sohl-Dickstein, Jascha; Weiss, Eric; Maheswaranathan, Niru; Ganguli, Surya (2015-06-01). \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\" (PDF). Proceedings of the 32nd International Conference on Machine Learning. 37. PMLR: 2256–2265. arXiv:1503.03585. - ^ Google Research Blog. The neural networks behind Google Voice transcription. August 11, 2015. By Françoise Beaufays http://googleresearch.blogspot.co.at/2015/08/the-neural-networks-behind-google-voice.html - ^ a b Sak, Haşim; Senior, Andrew; Rao, Kanishka; Beaufays, Françoise; Schalkwyk, Johan (September 2015). \"Google voice search: faster and more accurate\". Archived from the original on 2016-03-09. Retrieved 2016-04-09. - ^ Singh, Premjeet; Saha, Goutam; Sahidullah, Md (2021). \"Non-linear frequency warping using constant-Q transformation for speech emotion recognition\". 2021 International Conference on Computer Communication and Informatics (ICCCI). pp. 1–4. arXiv:2102.04029. doi:10.1109/ICCCI50826.2021.9402569. ISBN 978-1-7281-5875-4. S2CID 231846518. - ^ Sak, Hasim; Senior, Andrew; Beaufays, Francoise (2014). \"Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling\" (PDF). Archived from the original (PDF) on 24 April 2018. - ^ Li, Xiangang; Wu, Xihong (2014). \"Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition\". arXiv:1410.4281 [cs.CL]. - ^ Zen, Heiga; Sak, Hasim (2015). \"Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis\" (PDF). Google.com. ICASSP. pp. 4470–4474. Archived (PDF) from the original on 2021-05-09. Retrieved 2017-06-13. - ^ \"2018 ACM A.M. Turing Award Laureates\". awards.acm.org. Retrieved 2024-08-07. - ^ Ferrie, C., & Kaiser, S. (2019). Neural Networks for Babies. Sourcebooks. ISBN 978-1-4926-7120-6. {{cite book}} : CS1 maint: multiple names: authors list (link) - ^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda (January 2016). \"Mastering the game of Go with deep neural networks and tree search\". Nature. 529 (7587): 484–489. Bibcode:2016Natur.529..484S. doi:10.1038/nature16961. ISSN 1476-4687. PMID 26819042. S2CID 515925. - ^ A Guide to Deep Learning and Neural Networks, archived from the original on 2020-11-02, retrieved 2020-11-16 - ^ a b Kumar, Nishant; Raubal, Martin (2021). \"Applications of deep learning in congestion detection, prediction and alleviation: A survey\". Transportation Research Part C: Emerging Technologies. 133 103432. arXiv:2102.09759. Bibcode:2021TRPC..13303432K. doi:10.1016/j.trc.2021.103432. hdl:10230/42143. S2CID 240420107. - ^ Szegedy, Christian; Toshev, Alexander; Erhan, Dumitru (2013). \"Deep neural networks for object detection\". Advances in Neural Information Processing Systems: 2553–2561. Archived from the original on 2017-06-29. Retrieved 2017-06-13. - ^ Rolnick, David; Tegmark, Max (2018). \"The power of deeper networks for expressing natural functions\". International Conference on Learning Representations. ICLR 2018. Archived from the original on 2021-01-07. Retrieved 2021-01-05. - ^ Hof, Robert D. \"Is Artificial Intelligence Finally Coming into Its Own?\". MIT Technology Review. Archived from the original on 31 March 2019. Retrieved 10 July 2018. - ^ a b Gers, Felix A.; Schmidhuber, Jürgen (2001). \"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\". IEEE Transactions on Neural Networks. 12 (6): 1333–1340. Bibcode:2001ITNN...12.1333G. doi:10.1109/72.963769. PMID 18249962. S2CID 10192330. Archived from the original on 2020-01-26. Retrieved 2020-02-25. - ^ a b c Sutskever, L.; Vinyals, O.; Le, Q. (2014). \"Sequence to Sequence Learning with Neural Networks\" (PDF). Proc. NIPS. arXiv:1409.3215. Bibcode:2014arXiv1409.3215S. Archived (PDF) from the original on 2021-05-09. Retrieved 2017-06-13. - ^ a b Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). \"Exploring the Limits of Language Modeling\". arXiv:1602.02410 [cs.CL]. - ^ a b Gillick, Dan; Brunk, Cliff; Vinyals, Oriol; Subramanya, Amarnag (2015). \"Multilingual Language Processing from Bytes\". arXiv:1512.00103 [cs.CL]. - ^ Mikolov, T.; et al. (2010). \"Recurrent neural network based language model\" (PDF). Interspeech: 1045–1048. doi:10.21437/Interspeech.2010-343. S2CID 17048224. Archived (PDF) from the original on 2017-05-16. Retrieved 2017-06-13. - ^ Hochreiter, Sepp; Schmidhuber, Jürgen (1 November 1997). \"Long Short-Term Memory\". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. ISSN 0899-7667. PMID 9377276. S2CID 1915014. - ^ a b \"Learning Precise Timing with LSTM Recurrent Networks (PDF Download Available)\". ResearchGate. Archived from the original on 9 May 2021. Retrieved 13 June 2017. - ^ LeCun, Y.; et al. (1998). \"Gradient-based learning applied to document recognition\". Proceedings of the IEEE. 86 (11): 2278–2324. Bibcode:1998IEEEP..86.2278L. doi:10.1109/5.726791. S2CID 14542261. - ^ Sainath, Tara N.; Mohamed, Abdel-Rahman; Kingsbury, Brian; Ramabhadran, Bhuvana (2013). \"Deep convolutional neural networks for LVCSR\". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 8614–8618. doi:10.1109/icassp.2013.6639347. ISBN 978-1-4799-0356-6. S2CID 13816461. - ^ Bengio, Yoshua; Boulanger-Lewandowski, Nicolas; Pascanu, Razvan (2013). \"Advances in optimizing recurrent networks\". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 8624–8628. arXiv:1212.0901. CiteSeerX 10.1.1.752.9151. doi:10.1109/icassp.2013.6639349. ISBN 978-1-4799-0356-6. S2CID 12485056. - ^ Dahl, G.; et al. (2013). \"Improving DNNs for LVCSR using rectified linear units and dropout\" (PDF). ICASSP. Archived (PDF) from the original on 2017-08-12. Retrieved 2017-06-13. - ^ Kumar, Nishant; Martin, Henry; Raubal, Martin (2024). \"Enhancing Deep Learning-Based City-Wide Traffic Prediction Pipelines Through Complexity Analysis\". Data Science for Transportation. 6 (3) 24. doi:10.1007/s42421-024-00109-x. hdl:20.500.11850/695425. - ^ \"Data Augmentation - deeplearning.ai | Coursera\". Coursera. Archived from the original on 1 December 2017. Retrieved 30 November 2017. - ^ Hinton, G. E. (2010). \"A Practical Guide to Training Restricted Boltzmann Machines\". Tech. Rep. UTML TR 2010-003. Archived from the original on 2021-05-09. Retrieved 2017-06-13. - ^ You, Yang; Buluç, Aydın; Demmel, James (November 2017). \"Scaling deep learning on GPU and knights landing clusters\". Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on - SC '17. SC '17, ACM. pp. 1–12. arXiv:1708.02983. doi:10.1145/3126908.3126912. ISBN 978-1-4503-5114-0. S2CID 8869270. Archived from the original on 29 July 2020. Retrieved 5 March 2018. - ^ Viebke, André; Memeti, Suejb; Pllana, Sabri; Abraham, Ajith (2019). \"CHAOS: a parallelization scheme for training convolutional neural networks on Intel Xeon Phi\". The Journal of Supercomputing. 75: 197–227. arXiv:1702.07908. Bibcode:2017arXiv170207908V. doi:10.1007/s11227-017-1994-x. S2CID 14135321. - ^ Ting Qin, et al. \"A learning algorithm of CMAC based on RLS\". Neural Processing Letters 19.1 (2004): 49-61. - ^ Ting Qin, et al. \"Continuous CMAC-QRLS and its systolic array\". Archived 2018-11-18 at the Wayback Machine. Neural Processing Letters 22.1 (2005): 1-16. - ^ Research, AI (23 October 2015). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition\". airesearch.com. Archived from the original on 1 February 2016. Retrieved 23 October 2015. - ^ \"GPUs Continue to Dominate the AI Accelerator Market for Now\". InformationWeek. December 2019. Archived from the original on 10 June 2020. Retrieved 11 June 2020. - ^ Ray, Tiernan (2019). \"AI is changing the entire nature of computation\". ZDNet. Archived from the original on 25 May 2020. Retrieved 11 June 2020. - ^ \"AI and Compute\". OpenAI. 16 May 2018. Archived from the original on 17 June 2020. Retrieved 11 June 2020. - ^ \"HUAWEI Reveals the Future of Mobile AI at IFA 2017 | HUAWEI Latest News | HUAWEI Global\". consumer.huawei.com. - ^ P, JouppiNorman; YoungCliff; PatilNishant; PattersonDavid; AgrawalGaurav; BajwaRaminder; BatesSarah; BhatiaSuresh; BodenNan; BorchersAl; BoyleRick (2017-06-24). \"In-Datacenter Performance Analysis of a Tensor Processing Unit\". ACM SIGARCH Computer Architecture News. 45 (2): 1–12. arXiv:1704.04760. doi:10.1145/3140659.3080246. - ^ Woodie, Alex (2021-11-01). \"Cerebras Hits the Accelerator for Deep Learning Workloads\". Datanami. Retrieved 2022-08-03. - ^ \"Cerebras launches new AI supercomputing processor with 2.6 trillion transistors\". VentureBeat. 2021-04-20. Retrieved 2022-08-03. - ^ Marega, Guilherme Migliato; Zhao, Yanfei; Avsar, Ahmet; Wang, Zhenyu; Tripati, Mukesh; Radenovic, Aleksandra; Kis, Anras (2020). \"Logic-in-memory based on an atomically thin semiconductor\". Nature. 587 (2): 72–77. Bibcode:2020Natur.587...72M. doi:10.1038/s41586-020-2861-0. PMC 7116757. PMID 33149289. - ^ a b c Feldmann, J.; Youngblood, N.; Karpov, M.; et al. (2021). \"Parallel convolutional processing using an integrated photonic tensor\". Nature. 589 (2): 52–58. arXiv:2002.00281. doi:10.1038/s41586-020-03070-1. PMID 33408373. S2CID 211010976. - ^ Garofolo, J.S.; Lamel, L.F.; Fisher, W.M.; Fiscus, J.G.; Pallett, D.S.; Dahlgren, N.L.; Zue, V. (1993). TIMIT Acoustic-Phonetic Continuous Speech Corpus. Linguistic Data Consortium. doi:10.35111/17gk-bn40. ISBN 1-58563-019-5. Retrieved 27 December 2023. - ^ Robinson, Tony (30 September 1991). \"Several Improvements to a Recurrent Error Propagation Network Phone Recognition System\". Cambridge University Engineering Department Technical Report. CUED/F-INFENG/TR82. doi:10.13140/RG.2.2.15418.90567. - ^ Abdel-Hamid, O.; et al. (2014). \"Convolutional Neural Networks for Speech Recognition\". IEEE/ACM Transactions on Audio, Speech, and Language Processing. 22 (10): 1533–1545. Bibcode:2014ITASL..22.1533A. doi:10.1109/taslp.2014.2339736. S2CID 206602362. Archived from the original on 2020-09-22. Retrieved 2018-04-20. - ^ Deng, L.; Platt, J. (2014). \"Ensemble Deep Learning for Speech Recognition\". Proc. Interspeech: 1915–1919. doi:10.21437/Interspeech.2014-433. S2CID 15641618. - ^ Tóth, Laszló (2015). \"Phone Recognition with Hierarchical Convolutional Deep Maxout Networks\" (PDF). EURASIP Journal on Audio, Speech, and Music Processing. 2015 25. doi:10.1186/s13636-015-0068-3. S2CID 217950236. Archived (PDF) from the original on 2020-09-24. Retrieved 2019-04-01. - ^ Aaron van den Oord; Dieleman, Sander; Zen, Heiga; Simonyan, Karen; Vinyals, Oriol; Graves, Alex; Kalchbrenner, Nal; Senior, Andrew; Kavukcuoglu, Koray (2016). \"WaveNet: A Generative Model for Raw Audio\". arXiv:1609.03499 [cs.SD]. - ^ \"WaveNet: A generative model for raw audio\". Google DeepMind. 2016-09-08. Retrieved 2025-07-31. - ^ Latif, Siddique; Zaidi, Aun; Cuayahuitl, Heriberto; Shamshad, Fahad; Shoukat, Moazzam; Usama, Muhammad; Qadir, Junaid (2023). \"Transformers in Speech Processing: A Survey\". arXiv:2303.11607 [cs.CL]. - ^ McMillan, Robert (17 December 2014). \"How Skype Used AI to Build Its Amazing New Language Translator | WIRED\". Wired. Archived from the original on 8 June 2017. Retrieved 14 June 2017. - ^ Hannun, Awni; Case, Carl; Casper, Jared; Catanzaro, Bryan; Diamos, Greg; Elsen, Erich; Prenger, Ryan; Satheesh, Sanjeev; Sengupta, Shubho; Coates, Adam; Ng, Andrew Y (2014). \"Deep Speech: Scaling up end-to-end speech recognition\". arXiv:1412.5567 [cs.CL]. - ^ \"MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges\". yann.lecun.com. Archived from the original on 2014-01-13. Retrieved 2014-01-28. - ^ Cireşan, Dan; Meier, Ueli; Masci, Jonathan; Schmidhuber, Jürgen (August 2012). \"Multi-column deep neural network for traffic sign classification\". Neural Networks. Selected Papers from IJCNN 2011. 32: 333–338. CiteSeerX 10.1.1.226.8219. doi:10.1016/j.neunet.2012.02.023. PMID 22386783. - ^ Chaochao Lu; Xiaoou Tang (2014). \"Surpassing Human Level Face Recognition\". arXiv:1404.3840 [cs.CV]. - ^ Nvidia Demos a Car Computer Trained with \"Deep Learning\" (6 January 2015), David Talbot, MIT Technology Review - ^ a b c G. W. Smith; Frederic Fol Leymarie (10 April 2017). \"The Machine as Artist: An Introduction\". Arts. 6 (4): 5. doi:10.3390/arts6020005. - ^ a b c Blaise Agüera y Arcas (29 September 2017). \"Art in the Age of Machine Intelligence\". Arts. 6 (4): 18. doi:10.3390/arts6040018. - ^ Goldberg, Yoav; Levy, Omar (2014). \"word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method\". arXiv:1402.3722 [cs.CL]. - ^ a b Socher, Richard; Manning, Christopher. \"Deep Learning for NLP\" (PDF). Archived (PDF) from the original on 6 July 2014. Retrieved 26 October 2014. - ^ Socher, Richard; Bauer, John; Manning, Christopher; Ng, Andrew (2013). \"Parsing With Compositional Vector Grammars\" (PDF). Proceedings of the ACL 2013 Conference. Archived (PDF) from the original on 2014-11-27. Retrieved 2014-09-03. - ^ Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C.D.; Ng, A.; Potts, C. (October 2013). \"Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank\" (PDF). Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. pp. 1631–1642. doi:10.18653/v1/D13-1170. Archived (PDF) from the original on 28 December 2016. Retrieved 21 December 2023. - ^ Shen, Yelong; He, Xiaodong; Gao, Jianfeng; Deng, Li; Mesnil, Gregoire (1 November 2014). \"A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval\". Microsoft Research. Archived from the original on 27 October 2017. Retrieved 14 June 2017. - ^ Huang, Po-Sen; He, Xiaodong; Gao, Jianfeng; Deng, Li; Acero, Alex; Heck, Larry (1 October 2013). \"Learning Deep Structured Semantic Models for Web Search using Clickthrough Data\". Microsoft Research. Archived from the original on 27 October 2017. Retrieved 14 June 2017. - ^ Mesnil, G.; Dauphin, Y.; Yao, K.; Bengio, Y.; Deng, L.; Hakkani-Tur, D.; He, X.; Heck, L.; Tur, G.; Yu, D.; Zweig, G. (2015). \"Using recurrent neural networks for slot filling in spoken language understanding\". IEEE Transactions on Audio, Speech, and Language Processing. 23 (3): 530–539. Bibcode:2015ITASL..23..530M. doi:10.1109/taslp.2014.2383614. S2CID 1317136. - ^ a b Gao, Jianfeng; He, Xiaodong; Yih, Scott Wen-tau; Deng, Li (1 June 2014). \"Learning Continuous Phrase Representations for Translation Modeling\". Microsoft Research. Archived from the original on 27 October 2017. Retrieved 14 June 2017. - ^ Brocardo, Marcelo Luiz; Traore, Issa; Woungang, Isaac; Obaidat, Mohammad S. (2017). \"Authorship verification using deep belief network systems\". International Journal of Communication Systems. 30 (12) e3259. doi:10.1002/dac.3259. S2CID 40745740. - ^ Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). \"Precision information extraction for rare disease epidemiology at scale\". Journal of Translational Medicine. 21 (1): 157. doi:10.1186/s12967-023-04011-y. PMC 9972634. PMID 36855134. - ^ \"Deep Learning for Natural Language Processing: Theory and Practice (CIKM2014 Tutorial) - Microsoft Research\". Microsoft Research. Archived from the original on 13 March 2017. Retrieved 14 June 2017. - ^ Turovsky, Barak (15 November 2016). \"Found in translation: More accurate, fluent sentences in Google Translate\". The Keyword Google Blog. Archived from the original on 7 April 2017. Retrieved 23 March 2017. - ^ a b c d Schuster, Mike; Johnson, Melvin; Thorat, Nikhil (22 November 2016). \"Zero-Shot Translation with Google's Multilingual Neural Machine Translation System\". Google Research Blog. Archived from the original on 10 July 2017. Retrieved 23 March 2017. - ^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg; et al. (2016). \"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL]. - ^ Metz, Cade (27 September 2016). \"An Infusion of AI Makes Google Translate More Powerful Than Ever\". Wired. Archived from the original on 8 November 2020. Retrieved 12 October 2017. - ^ a b Boitet, Christian; Blanchon, Hervé; Seligman, Mark; Bellynck, Valérie (2010). \"MT on and for the Web\" (PDF). Archived from the original (PDF) on 29 March 2017. Retrieved 1 December 2016. - ^ Arrowsmith, J; Miller, P (2013). \"Trial watch: Phase II and phase III attrition rates 2011-2012\". Nature Reviews Drug Discovery. 12 (8): 569. doi:10.1038/nrd4090. PMID 23903212. S2CID 20246434. - ^ Verbist, B; Klambauer, G; Vervoort, L; Talloen, W; The Qstar, Consortium; Shkedy, Z; Thas, O; Bender, A; Göhlmann, H. W.; Hochreiter, S (2015). \"Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the QSTAR project\". Drug Discovery Today. 20 (5): 505–513. doi:10.1016/j.drudis.2014.12.014. hdl:1942/18723. PMID 25582842. - ^ \"Merck Molecular Activity Challenge\". kaggle.com. Archived from the original on 2020-07-16. Retrieved 2020-07-16. - ^ \"Multi-task Neural Networks for QSAR Predictions | Data Science Association\". www.datascienceassn.org. Archived from the original on 30 April 2017. Retrieved 14 June 2017. - ^ \"Toxicology in the 21st century Data Challenge\" - ^ \"NCATS Announces Tox21 Data Challenge Winners\". Archived from the original on 2015-09-08. Retrieved 2015-03-05. - ^ \"NCATS Announces Tox21 Data Challenge Winners\". Archived from the original on 28 February 2015. Retrieved 5 March 2015. - ^ Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (9 October 2015). \"AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery\". arXiv:1510.02855 [cs.LG]. - ^ a b \"Toronto startup has a faster way to discover effective medicines\". The Globe and Mail. Archived from the original on 20 October 2015. Retrieved 9 November 2015. - ^ \"Startup Harnesses Supercomputers to Seek Cures\". KQED Future of You. 27 May 2015. Archived from the original on 24 December 2015. Retrieved 9 November 2015. - ^ Gilmer, Justin; Schoenholz, Samuel S.; Riley, Patrick F.; Vinyals, Oriol; Dahl, George E. (2017-06-12). \"Neural Message Passing for Quantum Chemistry\". arXiv:1704.01212 [cs.LG]. - ^ Zhavoronkov, Alex (2019). \"Deep learning enables rapid identification of potent DDR1 kinase inhibitors\". Nature Biotechnology. 37 (9): 1038–1040. doi:10.1038/s41587-019-0224-x. PMID 31477924. S2CID 201716327. - ^ Gregory, Barber. \"A Molecule Designed By AI Exhibits 'Druglike' Qualities\". Wired. Archived from the original on 2020-04-30. Retrieved 2019-09-05. - ^ van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013). Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q. (eds.). Advances in Neural Information Processing Systems 26 (PDF). Curran Associates, Inc. pp. 2643–2651. Archived (PDF) from the original on 2017-05-16. Retrieved 2017-06-14. - ^ Feng, X.Y.; Zhang, H.; Ren, Y.J.; Shang, P.H.; Zhu, Y.; Liang, Y.C.; Guan, R.C.; Xu, D. (2019). \"The Deep Learning–Based Recommender System \"Pubmender\" for Choosing a Biomedical Publication Venue: Development and Validation Study\". Journal of Medical Internet Research. 21 (5) e12957. doi:10.2196/12957. PMC 6555124. PMID 31127715. - ^ Elkahky, Ali Mamdouh; Song, Yang; He, Xiaodong (1 May 2015). \"A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems\". Microsoft Research. Archived from the original on 25 January 2018. Retrieved 14 June 2017. - ^ Chicco, Davide; Sadowski, Peter; Baldi, Pierre (1 January 2014). \"Deep autoencoder neural networks for gene ontology annotation predictions\". Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM. pp. 533–540. doi:10.1145/2649387.2649442. hdl:11311/964622. ISBN 978-1-4503-2894-4. S2CID 207217210. Archived from the original on 9 May 2021. Retrieved 23 November 2015. - ^ Sathyanarayana, Aarti (1 January 2016). \"Sleep Quality Prediction From Wearable Data Using Deep Learning\". JMIR mHealth and uHealth. 4 (4): e125. doi:10.2196/mhealth.6562. PMC 5116102. PMID 27815231. S2CID 3821594. - ^ Choi, Edward; Schuetz, Andy; Stewart, Walter F.; Sun, Jimeng (13 August 2016). \"Using recurrent neural network models for early detection of heart failure onset\". Journal of the American Medical Informatics Association. 24 (2): 361–370. doi:10.1093/jamia/ocw112. ISSN 1067-5027. PMC 5391725. PMID 27521897. - ^ \"DeepMind's protein-folding AI has solved a 50-year-old grand challenge of biology\". MIT Technology Review. Retrieved 2024-05-10. - ^ Shead, Sam (2020-11-30). \"DeepMind solves 50-year-old 'grand challenge' with protein folding A.I.\" CNBC. Retrieved 2024-05-10. - ^ a b Shalev, Y.; Painsky, A.; Ben-Gal, I. (2022). \"Neural Joint Entropy Estimation\" (PDF). IEEE Transactions on Neural Networks and Learning Systems. PP (4): 5488–5500. arXiv:2012.11197. doi:10.1109/TNNLS.2022.3204919. PMID 36155469. S2CID 229339809. - ^ Litjens, Geert; Kooi, Thijs; Bejnordi, Babak Ehteshami; Setio, Arnaud Arindra Adiyoso; Ciompi, Francesco; Ghafoorian, Mohsen; van der Laak, Jeroen A.W.M.; van Ginneken, Bram; Sánchez, Clara I. (December 2017). \"A survey on deep learning in medical image analysis\". Medical Image Analysis. 42: 60–88. arXiv:1702.05747. Bibcode:2017arXiv170205747L. doi:10.1016/j.media.2017.07.005. PMID 28778026. S2CID 2088679. - ^ Forslid, Gustav; Wieslander, Hakan; Bengtsson, Ewert; Wahlby, Carolina; Hirsch, Jan-Michael; Stark, Christina Runow; Sadanandan, Sajith Kecheril (2017). \"Deep Convolutional Neural Networks for Detecting Cellular Changes Due to Malignancy\". 2017 IEEE International Conference on Computer Vision Workshops (ICCVW). pp. 82–89. doi:10.1109/ICCVW.2017.18. ISBN 978-1-5386-1034-3. S2CID 4728736. Archived from the original on 2021-05-09. Retrieved 2019-11-12. - ^ Dong, Xin; Zhou, Yizhao; Wang, Lantian; Peng, Jingfeng; Lou, Yanbo; Fan, Yiqun (2020). \"Liver Cancer Detection Using Hybridized Fully Convolutional Neural Network Based on Deep Learning Framework\". IEEE Access. 8: 129889–129898. Bibcode:2020IEEEA...8l9889D. doi:10.1109/ACCESS.2020.3006362. ISSN 2169-3536. S2CID 220733699. - ^ Lyakhov, Pavel Alekseevich; Lyakhova, Ulyana Alekseevna; Nagornov, Nikolay Nikolaevich (2022-04-03). \"System for the Recognizing of Pigmented Skin Lesions with Fusion and Analysis of Heterogeneous Data Based on a Multimodal Neural Network\". Cancers. 14 (7): 1819. doi:10.3390/cancers14071819. ISSN 2072-6694. PMC 8997449. PMID 35406591. - ^ De, Shaunak; Maity, Abhishek; Goel, Vritti; Shitole, Sanjay; Bhattacharya, Avik (2017). \"Predicting the popularity of instagram posts for a lifestyle magazine using deep learning\". 2017 2nd International Conference on Communication Systems, Computing and IT Applications (CSCITA). pp. 174–177. doi:10.1109/CSCITA.2017.8066548. ISBN 978-1-5090-4381-1. S2CID 35350962. - ^ \"Colorizing and Restoring Old Images with Deep Learning\". FloydHub Blog. 13 November 2018. Archived from the original on 11 October 2019. Retrieved 11 October 2019. - ^ Schmidt, Uwe; Roth, Stefan. Shrinkage Fields for Effective Image Restoration (PDF). Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. Archived (PDF) from the original on 2018-01-02. Retrieved 2018-01-01. - ^ Kleanthous, Christos; Chatzis, Sotirios (2020). \"Gated Mixture Variational Autoencoders for Value Added Tax audit case selection\". Knowledge-Based Systems. 188 105048. doi:10.1016/j.knosys.2019.105048. S2CID 204092079. - ^ Czech, Tomasz (28 June 2018). \"Deep learning: the next frontier for money laundering detection\". Global Banking and Finance Review. Archived from the original on 2018-11-16. Retrieved 2018-07-15. - ^ Nuñez, Michael (2023-11-29). \"Google DeepMind's materials AI has already discovered 2.2 million new crystals\". VentureBeat. Retrieved 2023-12-19. - ^ Merchant, Amil; Batzner, Simon; Schoenholz, Samuel S.; Aykol, Muratahan; Cheon, Gowoon; Cubuk, Ekin Dogus (December 2023). \"Scaling deep learning for materials discovery\". Nature. 624 (7990): 80–85. Bibcode:2023Natur.624...80M. doi:10.1038/s41586-023-06735-9. ISSN 1476-4687. PMC 10700131. PMID 38030720. - ^ Peplow, Mark (2023-11-29). \"Google AI and robots join forces to build new materials\". Nature. doi:10.1038/d41586-023-03745-5. PMID 38030771. S2CID 265503872. - ^ a b c \"Army researchers develop new algorithms to train robots\". EurekAlert!. Archived from the original on 28 August 2018. Retrieved 29 August 2018. - ^ Raissi, M.; Perdikaris, P.; Karniadakis, G. E. (2019-02-01). \"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations\". Journal of Computational Physics. 378: 686–707. Bibcode:2019JCoPh.378..686R. doi:10.1016/j.jcp.2018.10.045. ISSN 0021-9991. OSTI 1595805. S2CID 57379996. - ^ Mao, Zhiping; Jagtap, Ameya D.; Karniadakis, George Em (2020-03-01). \"Physics-informed neural networks for high-speed flows\". Computer Methods in Applied Mechanics and Engineering. 360 112789. Bibcode:2020CMAME.360k2789M. doi:10.1016/j.cma.2019.112789. ISSN 0045-7825. S2CID 212755458. - ^ Raissi, Maziar; Yazdani, Alireza; Karniadakis, George Em (2020-02-28). \"Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations\". Science. 367 (6481): 1026–1030. Bibcode:2020Sci...367.1026R. doi:10.1126/science.aaw4741. PMC 7219083. PMID 32001523. - ^ Huang, Yunfei and Greenberg, David S. \"Geometric and Physical Constraints Synergistically Enhance Neural PDE Surrogates.\" Proceedings of the 42th international conference on Machine learning. ACM, 2025. - ^ Han, J.; Jentzen, A.; E, W. (2018). \"Solving high-dimensional partial differential equations using deep learning\". Proceedings of the National Academy of Sciences. 115 (34): 8505–8510. arXiv:1707.02568. Bibcode:2018PNAS..115.8505H. doi:10.1073/pnas.1718942115. PMC 6112690. PMID 30082389. - ^ Oktem, Figen S.; Kar, Oğuzhan Fatih; Bezek, Can Deniz; Kamalabadi, Farzad (2021). \"High-Resolution Multi-Spectral Imaging With Diffractive Lenses and Learned Reconstruction\". IEEE Transactions on Computational Imaging. 7: 489–504. arXiv:2008.11625. Bibcode:2021ITCI....7..489O. doi:10.1109/TCI.2021.3075349. ISSN 2333-9403. S2CID 235340737. - ^ Bernhardt, Melanie; Vishnevskiy, Valery; Rau, Richard; Goksel, Orcun (December 2020). \"Training Variational Networks With Multidomain Simulations: Speed-of-Sound Image Reconstruction\". IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control. 67 (12): 2584–2594. arXiv:2006.14395. Bibcode:2020ITUFF..67.2584B. doi:10.1109/TUFFC.2020.3010186. ISSN 1525-8955. PMID 32746211. S2CID 220055785. - ^ Lam, Remi; Sanchez-Gonzalez, Alvaro; Willson, Matthew; Wirnsberger, Peter; Fortunato, Meire; Alet, Ferran; Ravuri, Suman; Ewalds, Timo; Eaton-Rosen, Zach; Hu, Weihua; Merose, Alexander; Hoyer, Stephan; Holland, George; Vinyals, Oriol; Stott, Jacklynn (2023-12-22). \"Learning skillful medium-range global weather forecasting\". Science. 382 (6677): 1416–1421. arXiv:2212.12794. Bibcode:2023Sci...382.1416L. doi:10.1126/science.adi2336. ISSN 0036-8075. PMID 37962497. - ^ Sivakumar, Ramakrishnan (2023-11-27). \"GraphCast: A breakthrough in Weather Forecasting\". Medium. Retrieved 2024-05-19. - ^ Galkin, F.; Mamoshina, P.; Kochetov, K.; Sidorenko, D.; Zhavoronkov, A. (2020). \"DeepMAge: A Methylation Aging Clock Developed with Deep Learning\". Aging and Disease. doi:10.14336/AD. - ^ Utgoff, P. E.; Stracuzzi, D. J. (2002). \"Many-layered learning\". Neural Computation. 14 (10): 2497–2529. doi:10.1162/08997660260293319. PMID 12396572. S2CID 1119517. - ^ Elman, Jeffrey L. (1998). Rethinking Innateness: A Connectionist Perspective on Development. MIT Press. ISBN 978-0-262-55030-7. - ^ Shrager, J.; Johnson, MH (1996). \"Dynamic plasticity influences the emergence of function in a simple cortical array\". Neural Networks. 9 (7): 1119–1129. doi:10.1016/0893-6080(96)00033-0. PMID 12662587. - ^ Quartz, SR; Sejnowski, TJ (1997). \"The neural basis of cognitive development: A constructivist manifesto\". Behavioral and Brain Sciences. 20 (4): 537–556. CiteSeerX 10.1.1.41.7854. doi:10.1017/s0140525x97001581. PMID 10097006. S2CID 5818342. - ^ S. Blakeslee, \"In brain's early growth, timetable may be critical\", The New York Times, Science Section, pp. B5–B6, 1995. - ^ Mazzoni, P.; Andersen, R. A.; Jordan, M. I. (15 May 1991). \"A more biologically plausible learning rule for neural networks\". Proceedings of the National Academy of Sciences. 88 (10): 4433–4437. Bibcode:1991PNAS...88.4433M. doi:10.1073/pnas.88.10.4433. ISSN 0027-8424. PMC 51674. PMID 1903542. - ^ O'Reilly, Randall C. (1 July 1996). \"Biologically Plausible Error-Driven Learning Using Local Activation Differences: The Generalized Recirculation Algorithm\". Neural Computation. 8 (5): 895–938. doi:10.1162/neco.1996.8.5.895. ISSN 0899-7667. S2CID 2376781. - ^ Testolin, Alberto; Zorzi, Marco (2016). \"Probabilistic Models and Generative Neural Networks: Towards an Unified Framework for Modeling Normal and Impaired Neurocognitive Functions\". Frontiers in Computational Neuroscience. 10: 73. doi:10.3389/fncom.2016.00073. ISSN 1662-5188. PMC 4943066. PMID 27468262. S2CID 9868901. - ^ Testolin, Alberto; Stoianov, Ivilin; Zorzi, Marco (September 2017). \"Letter perception emerges from unsupervised deep learning and recycling of natural image features\". Nature Human Behaviour. 1 (9): 657–664. doi:10.1038/s41562-017-0186-2. ISSN 2397-3374. PMID 31024135. S2CID 24504018. - ^ Buesing, Lars; Bill, Johannes; Nessler, Bernhard; Maass, Wolfgang (3 November 2011). \"Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons\". PLOS Computational Biology. 7 (11) e1002211. Bibcode:2011PLSCB...7E2211B. doi:10.1371/journal.pcbi.1002211. ISSN 1553-7358. PMC 3207943. PMID 22096452. S2CID 7504633. - ^ Cash, S.; Yuste, R. (February 1999). \"Linear summation of excitatory inputs by CA1 pyramidal neurons\". Neuron. 22 (2): 383–394. doi:10.1016/s0896-6273(00)81098-3. ISSN 0896-6273. PMID 10069343. S2CID 14663106. - ^ Olshausen, B; Field, D (1 August 2004). \"Sparse coding of sensory inputs\". Current Opinion in Neurobiology. 14 (4): 481–487. doi:10.1016/j.conb.2004.07.007. ISSN 0959-4388. PMID 15321069. S2CID 16560320. - ^ Yamins, Daniel L K; DiCarlo, James J (March 2016). \"Using goal-driven deep learning models to understand sensory cortex\". Nature Neuroscience. 19 (3): 356–365. doi:10.1038/nn.4244. ISSN 1546-1726. PMID 26906502. S2CID 16970545. - ^ Zorzi, Marco; Testolin, Alberto (19 February 2018). \"An emergentist perspective on the origin of number sense\". Phil. Trans. R. Soc. B. 373 (1740) 20170043. doi:10.1098/rstb.2017.0043. ISSN 0962-8436. PMC 5784047. PMID 29292348. S2CID 39281431. - ^ Güçlü, Umut; van Gerven, Marcel A. J. (8 July 2015). \"Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream\". Journal of Neuroscience. 35 (27): 10005–10014. arXiv:1411.6422. doi:10.1523/jneurosci.5023-14.2015. PMC 6605414. PMID 26157000. - ^ Metz, C. (12 December 2013). \"Facebook's 'Deep Learning' Guru Reveals the Future of AI\". Wired. Archived from the original on 28 March 2014. Retrieved 26 August 2017. - ^ Gibney, Elizabeth (2016). \"Google AI algorithm masters ancient game of Go\". Nature. 529 (7587): 445–446. Bibcode:2016Natur.529..445G. doi:10.1038/529445a. PMID 26819021. S2CID 4460235. - ^ Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; Driessche, George van den; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda; Lanctot, Marc; Dieleman, Sander; Grewe, Dominik; Nham, John; Kalchbrenner, Nal; Sutskever, Ilya; Lillicrap, Timothy; Leach, Madeleine; Kavukcuoglu, Koray; Graepel, Thore; Hassab",
    "text_length": 120000,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:17.730169"
  },
  {
    "id": "page_14",
    "url": "https://en.wikipedia.org/wiki/Bayesian_network",
    "domain": "en.wikipedia.org",
    "title": "Bayesian network - Wikipedia",
    "text": "Bayesian network This article includes a list of general references, but it lacks sufficient corresponding inline citations. (February 2011) | | Part of a series on | | Bayesian statistics | |---| | Posterior = Likelihood × Prior ÷ Evidence | | Background | | Model building | | Posterior approximation | | Estimators | | Evidence approximation | | Model evaluation | A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).[1] While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. Graphical model [edit]Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if parent nodes represent Boolean variables, then the probability function could be represented by a table of entries, one entry for each of the possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks. Example [edit]Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false). The joint probability function is, by the chain rule of probability, where G = \"Grass wet (true/false)\", S = \"Sprinkler turned on (true/false)\", and R = \"Raining (true/false)\". The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using the conditional probability formula and summing over all nuisance variables: Using the expansion for the joint probability function and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example, Then the numerical results (subscripted by the associated variable values) are To answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function obtained by removing the factor from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action: To predict the impact of turning the sprinkler on: with the term removed, showing that the action affects the grass but not the rain. These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action can still be predicted, however, whenever the back-door criterion is satisfied.[2][3] It states that, if a set Z of nodes can be observed that d-separates[4] (or blocks) all back-door paths from X to Y then A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious (apparent dependence arising from a common cause, R). (see Simpson's paradox) To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\"[2][5] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6] Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most values. One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions. Inference and learning [edit]Bayesian networks perform three main inference tasks: - Inferring unobserved variables - Parameter learning for the probability distributions of each node in the network - Structure learning of the graphical network Inferring unobserved variables [edit]Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems. The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods. Parameter learning [edit]In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.) Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters. A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable. Structure learning [edit]In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data. Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[7] and rests on the distinction between the three possible patterns allowed in a 3-node DAG: | Pattern | Model | |---|---| | Chain | | | Fork | | | Collider | The first 2 represent the same dependencies ( and are independent given ) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since and are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when and have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10] An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo (MCMC) can avoid getting trapped in local minima. Finding a structure that maximizes the mutual information, typically by restricting the parent candidate set to k nodes[11][12][13] or by finding an optimal k on a node-per-node basis,[14] is a technique that consistently achieves high scores on benchmark datasets. A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[15] Such method can handle problems with up to 100 variables. In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[16] Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[17] Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[18] Statistical introduction [edit]Given data and parameter , a simple Bayesian analysis starts with a prior probability (prior) and likelihood to compute a posterior probability . Often the prior on depends in turn on other parameters that are not mentioned in the likelihood. So, the prior must be replaced by a likelihood , and a prior on the newly introduced parameters is required, resulting in a posterior probability This is the simplest example of a hierarchical Bayes model. The process may be repeated; for example, the parameters may depend in turn on additional parameters , which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters. Introductory examples [edit]This section needs expansion. You can help by expanding it. (March 2009) | Given the measured quantities each with normally distributed errors of known standard deviation , Suppose we are interested in estimating the . An approach would be to estimate the using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply However, if the quantities are related, so that for example the individual have themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g., with improper priors , . When , this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models. Restrictions on priors [edit]Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible. Definitions and concepts [edit]Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V. Factorization definition [edit]X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[19] where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge). For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[19] Using the definition above, this can be written as: The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables. Local Markov property [edit]X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[20] where de(v) is the set of descendants and V \\ de(v) is the set of non-descendants of v. This can be expressed in terms similar to the first definition, as The set of parents is a subset of the set of non-descendants because the graph is acyclic. Marginal independence structure [edit]In general, learning a Bayesian network from data is known to be NP-hard.[21] This is due in part to the combinatorial explosion of enumerating DAGs as the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure:[22] while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by a simple undirected graph with special properties such as equal intersection and independence numbers. Developing Bayesian networks [edit]Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[23] Markov blanket [edit]The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[20] d-separation [edit]This definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional.[2] We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that. Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds: - P contains (but does not need to be entirely) a directed chain, or , such that the middle node m is in Z, - P contains a fork, , such that the middle node m is in Z, or - P contains an inverted fork (or collider), , such that the middle node m is not in Z and no descendant of m is in Z. The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected. X is a Bayesian network with respect to G if, for any two nodes u, v: where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.) Causal networks [edit]Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs: are equivalent: that is they impose exactly the same conditional independence requirements. A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[2] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted. Inference complexity and approximation algorithms [edit]In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[24] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[25] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2. At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[26][27] In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[28] developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by where was any polynomial of the number of nodes in the network, . Software [edit]Notable software for Bayesian networks include: - OpenBUGS – Open-source development of WinBUGS. - SPSS Modeler – Commercial software that includes an implementation for Bayesian networks. - Stan (software) – Stan is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler (NUTS),[29] a variant of Hamiltonian Monte Carlo. - WinBUGS – One of the first computational implementations of MCMC samplers. No longer maintained. History [edit]The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[30] - the often subjective nature of the input information - the reliance on Bayes' conditioning as the basis for updating information - the distinction between causal and evidential modes of reasoning[31] In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[32] and Neapolitan's Probabilistic Reasoning in Expert Systems[33] summarized their properties and established them as a field of study. See also [edit]- Bayesian epistemology - Bayesian programming - Causal inference - Causal loop diagram - Chow–Liu tree - Computational intelligence - Computational phylogenetics - Deep belief network - Dempster–Shafer theory – a generalization of Bayes' theorem - Expectation–maximization algorithm - Factor graph - Hierarchical temporal memory - Kalman filter - Memory-prediction framework - Mixture distribution - Mixture model - Naive Bayes classifier - Plate notation - Polytree - Sensor fusion - Sequence alignment - Staged tree - Structural equation modeling - Subjective logic - Variable-order Bayesian network Notes [edit]- ^ Ruggeri, Fabrizio; Kenett, Ron S.; Faltin, Frederick W., eds. (2007-12-14). Encyclopedia of Statistics in Quality and Reliability (1 ed.). Wiley. p. 1. doi:10.1002/9780470061572.eqr089. ISBN 978-0-470-01861-3. - ^ a b c d e Pearl, Judea (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press. ISBN 978-0-521-77362-1. OCLC 42291253. - ^ \"The Back-Door Criterion\" (PDF). Retrieved 2014-09-18. - ^ \"d-Separation without Tears\" (PDF). Retrieved 2014-09-18. - ^ Pearl J (1994). \"A Probabilistic Calculus of Actions\". In Lopez de Mantaras R, Poole D (eds.). UAI'94 Proceedings of the Tenth international conference on Uncertainty in artificial intelligence. San Mateo CA: Morgan Kaufmann. pp. 454–462. arXiv:1302.6835. Bibcode:2013arXiv1302.6835P. ISBN 1-55860-332-8. - ^ Shpitser I, Pearl J (2023). \"Identification of Conditional Interventional Distributions\". In Dechter R, Richardson TS (eds.). Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks. Vol. 14. Corvallis, OR: AUAI Press. pp. 437–444. arXiv:1206.6876. doi:10.2140/astat.2023.14.233. {{cite book}} :|journal= ignored (help) - ^ Rebane G, Pearl J (1987). \"The Recovery of Causal Poly-trees from Statistical Data\". Proceedings, 3rd Workshop on Uncertainty in AI. Seattle, WA. pp. 222–228. arXiv:1304.2736. {{cite book}} : CS1 maint: location missing publisher (link) - ^ Spirtes P, Glymour C (1991). \"An algorithm for fast recovery of sparse causal graphs\" (PDF). Social Science Computer Review. 9 (1): 62–72. CiteSeerX 10.1.1.650.2922. doi:10.1177/089443939100900106. S2CID 38398322. - ^ Spirtes P, Glymour CN, Scheines R (1993). Causation, Prediction, and Search (1st ed.). Springer-Verlag. ISBN 978-0-387-97979-3. - ^ Verma T, Pearl J (1991). \"Equivalence and synthesis of causal models\". In Bonissone P, Henrion M, Kanal LN, Lemmer JF (eds.). UAI '90 Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence. Elsevier. pp. 255–270. ISBN 0-444-89264-8. - ^ Sahami, Mehran (1996-08-02). \"Learning limited dependence Bayesian classifiers\". Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. KDD'96. Portland, Oregon: AAAI Press: 335–338. - ^ Friedman N, Geiger D, Goldszmidt M (November 1997). \"Bayesian Network Classifiers\". Machine Learning. 29 (2–3): 131–163. doi:10.1023/A:1007465528199. - ^ Friedman N, Linial M, Nachman I, Pe'er D (August 2000). \"Using Bayesian networks to analyze expression data\". Journal of Computational Biology. 7 (3–4): 601–20. CiteSeerX 10.1.1.191.139. doi:10.1089/106652700750050961. PMID 11108481. - ^ Rubio, Arcadio; Gámez, José Antonio (2011-07-12). \"Flexible learning of k-dependence Bayesian network classifiers\". Proceedings of the 13th annual conference on Genetic and evolutionary computation. GECCO '11. New York, NY, USA: Association for Computing Machinery. pp. 1219–1226. doi:10.1145/2001576.2001741. ISBN 978-1-4503-0557-0. - ^ Cussens J (2011). \"Bayesian network learning with cutting planes\" (PDF). Proceedings of the 27th Conference Annual Conference on Uncertainty in Artificial Intelligence: 153–160. arXiv:1202.3713. Bibcode:2012arXiv1202.3713C. Archived from the original on March 27, 2022. - ^ Scanagatta M, de Campos CP, Corani G, Zaffalon M (2015). \"Learning Bayesian Networks with Thousands of Variables\". NIPS-15: Advances in Neural Information Processing Systems. Vol. 28. Curran Associates. pp. 1855–1863. - ^ Petitjean F, Webb GI, Nicholson AE (2013). Scaling log-linear analysis to high-dimensional data (PDF). International Conference on Data Mining. Dallas, TX, USA: IEEE. - ^ M. Scanagatta, G. Corani, C. P. de Campos, and M. Zaffalon. Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables. In NIPS-16: Advances in Neural Information Processing Systems 29, 2016. - ^ a b Russell & Norvig 2003, p. 496. - ^ a b Russell & Norvig 2003, p. 499. - ^ Chickering, David M.; Heckerman, David; Meek, Christopher (2004). \"Large-sample learning of Bayesian networks is NP-hard\" (PDF). Journal of Machine Learning Research. 5: 1287–1330. - ^ Deligeorgaki, Danai; Markham, Alex; Misra, Pratik; Solus, Liam (2023). \"Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks\". Algebraic Statistics. 14 (2): 233–286. arXiv:2210.00822. doi:10.2140/astat.2023.14.233. - ^ Neapolitan RE (2004). Learning Bayesian networks. Prentice Hall. ISBN 978-0-13-012534-7. - ^ Cooper GF (1990). \"The Computational Complexity of Probabilistic Inference Using Bayesian Belief Networks\" (PDF). Artificial Intelligence. 42 (2–3): 393–405. doi:10.1016/0004-3702(90)90060-d. S2CID 43363498. - ^ Dagum P, Luby M (1993). \"Approximating probabilistic inference in Bayesian belief networks is NP-hard\". Artificial Intelligence. 60 (1): 141–153. CiteSeerX 10.1.1.333.1586. doi:10.1016/0004-3702(93)90036-b. - ^ D. Roth, On the hardness of approximate reasoning, IJCAI (1993) - ^ D. Roth, On the hardness of approximate reasoning, Artificial Intelligence (1996) - ^ Dagum P, Luby M (1997). \"An optimal approximation algorithm for Bayesian inference\". Artificial Intelligence. 93 (1–2): 1–27. CiteSeerX 10.1.1.36.7946. doi:10.1016/s0004-3702(97)00013-1. Archived from the original on 2017-07-06. Retrieved 2015-12-19. - ^ Hoffman, Matthew D.; Gelman, Andrew (2011). \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\". arXiv:1111.4246 [stat.CO]. - ^ Pearl J (1985). Bayesian Networks: A Model of Self-Activated Memory for Evidential Reasoning (UCLA Technical Report CSD-850017). Proceedings of the 7th Conference of the Cognitive Science Society, University of California, Irvine, CA. pp. 329–334. Retrieved 2009-05-01. - ^ Bayes T, Price (1763). \"An Essay Towards Solving a Problem in the Doctrine of Chances\". Philosophical Transactions of the Royal Society. 53: 370–418. doi:10.1098/rstl.1763.0053. - ^ Pearl J (1988-09-15). Probabilistic Reasoning in Intelligent Systems. San Francisco CA: Morgan Kaufmann. p. 1988. ISBN 978-1-55860-479-7. - ^ Neapolitan RE (1989). Probabilistic reasoning in expert systems: theory and algorithms. Wiley. ISBN 978-0-471-61840-9. References [edit]- Ben Gal I (2007). \"Bayesian Networks\" (PDF). In Ruggeri F, Kennett RS, Faltin FW (eds.). Support-Page. Encyclopedia of Statistics in Quality and Reliability. John Wiley & Sons. doi:10.1002/9780470061572.eqr089. ISBN 978-0-470-01861-3. Archived from the original (PDF) on 2016-11-23. Retrieved 2007-08-27. - Bertsch McGrayne S (2011). The Theory That Would not Die. New Haven: Yale University Press. - Borgelt C, Kruse R (March 2002). Graphical Models: Methods for Data Analysis and Mining. Chichester, UK: Wiley. ISBN 978-0-470-84337-6. - Borsuk ME (2008). \"Ecological informatics: Bayesian networks\". In Jørgensen, Sven Erik, Fath, Brian (eds.). Encyclopedia of Ecology. Elsevier. ISBN 978-0-444-52033-3. - Castillo E, Gutiérrez JM, Hadi AS (1997). \"Learning Bayesian Networks\". Expert Systems and Probabilistic Network Models. Monographs in computer science. New York: Springer-Verlag. pp. 481–528. ISBN 978-0-387-94858-4. - Comley JW, Dowe DL (June 2003). \"General Bayesian networks and asymmetric languages\". Proceedings of the 2nd Hawaii International Conference on Statistics and Related Fields. - Comley JW, Dowe DL (2005). \"Minimum Message Length and Generalized Bayesian Nets with Asymmetric Languages\". In Grünwald PD, Myung IJ, Pitt MA (eds.). Advances in Minimum Description Length: Theory and Applications. Neural information processing series. Cambridge, Massachusetts: Bradford Books (MIT Press) (published April 2005). pp. 265–294. ISBN 978-0-262-07262-5. (This paper puts decision trees in internal nodes of Bayes networks using Minimum Message Length (MML). - Darwiche A (2009). Modeling and Reasoning with Bayesian Networks. Cambridge University Press. ISBN 978-0-521-88438-9. - Dowe, David L. (2011-05-31). \"Hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness\" (PDF). Philosophy of Statistics. Elsevier. pp. 901–982. ISBN 978-0-08-093096-1. - Fenton N, Neil ME (November 2007). \"Managing Risk in the Modern World: Applications of Bayesian Networks\" (PDF). A Knowledge Transfer Report from the London Mathematical Society and the Knowledge Transfer Network for Industrial Mathematics. London (England): London Mathematical Society. Archived from the original (PDF) on 2008-05-14. Retrieved 2008-10-29. - Fenton N, Neil ME (July 23, 2004). \"Combining evidence in risk analysis using Bayesian Networks\" (PDF). Safety Critical Systems Club Newsletter. Vol. 13, no. 4. Newcastle upon Tyne, England. pp. 8–13. Archived from the original (PDF) on 2007-09-27. - Gelman A, Carlin JB, Stern HS, Rubin DB (2003). \"Part II: Fundamentals of Bayesian Data Analysis: Ch.5 Hierarchical models\". Bayesian Data Analysis. CRC Press. pp. 120–. ISBN 978-1-58488-388-3. - Heckerman, David (March 1, 1995). \"Tutorial on Learning with Bayesian Networks\". In Jordan, Michael Irwin (ed.). Learning in Graphical Models. Adaptive Computation and Machine Learning. Cambridge, Massachusetts: MIT Press (published 1998). pp. 301–354. ISBN 978-0-262-60032-3. Archived from the original on July 19, 2006. Retrieved September 15, 2006. {{cite book}} : CS1 maint: bot: original URL status unknown (link):Also appears as Heckerman, David (March 1997). \"Bayesian Networks for Data Mining\". Data Mining and Knowledge Discovery. 1 (1): 79–119. doi:10.1023/A:1009730122752. S2CID 6294315. - An earlier version appears as, Microsoft Research, March 1, 1995. The paper is about both parameter and structure learning in Bayesian networks. - Jensen FV, Nielsen TD (June 6, 2007). Bayesian Networks and Decision Graphs. Information Science and Statistics series (2nd ed.). New York: Springer-Verlag. ISBN 978-0-387-68281-5. - Karimi K, Hamilton HJ (2000). \"Finding temporal relations: Causal bayesian networks vs. C4. 5\" (PDF). Twelfth International Symposium on Methodologies for Intelligent Systems. - Korb KB, Nicholson AE (December 2010). Bayesian Artificial Intelligence. CRC Computer Science & Data Analysis (2nd ed.). Chapman & Hall (CRC Press). doi:10.1007/s10044-004-0214-5. ISBN 978-1-58488-387-6. S2CID 22138783. - Lunn D, Spiegelhalter D, Thomas A, Best N (November 2009). \"The BUGS project: Evolution, critique and future directions\". Statistics in Medicine. 28 (25): 3049–67. doi:10.1002/sim.3680. PMID 19630097. S2CID 7717482. - Neil M, Fenton N, Tailor M (August 2005). Greenberg, Michael R. (ed.). \"Using Bayesian networks to model expected and unexpected operational losses\" (PDF). Risk Analysis. 25 (4): 963–72. Bibcode:2005RiskA..25..963N. doi:10.1111/j.1539-6924.2005.00641.x. PMID 16268944. S2CID 3254505. - Pearl J (September 1986). \"Fusion, propagation, and structuring in belief networks\". Artificial Intelligence. 29 (3): 241–288. doi:10.1016/0004-3702(86)90072-X. - Pearl J (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Representation and Reasoning Series (2nd printing ed.). San Francisco, California: Morgan Kaufmann. ISBN 978-0-934613-73-6. - Pearl J, Russell S (November 2002). \"Bayesian Networks\". In Arbib MA (ed.). Handbook of Brain Theory and Neural Networks. Cambridge, Massachusetts: Bradford Books (MIT Press). pp. 157–160. ISBN 978-0-262-01197-6. - Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2. - Zhang NL, Poole D (May 1994). \"A simple approach to Bayesian network computations\" (PDF). Proceedings of the Tenth Biennial Canadian Artificial Intelligence Conference (AI-94).: 171–178. This paper presents variable elimination for belief networks. Further reading [edit]- Conrady S, Jouffe L (2015-07-01). Bayesian Networks and BayesiaLab – A practical introduction for researchers. Franklin, Tennessee: Bayesian USA. ISBN 978-0-9965333-0-0. - Charniak E (Winter 1991). \"Bayesian networks without tears\" (PDF). AI Magazine. - Kruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P (2013). Computational Intelligence A Methodological Introduction. London: Springer-Verlag. ISBN 978-1-4471-5012-1. - Borgelt C, Steinbrecher M, Kruse R (2009). Graphical Models – Representations for Learning, Reasoning and Data Mining (Second ed.). Chichester: Wiley. ISBN 978-0-470-74956-2. External links [edit]- An Introduction to Bayesian Networks and their Contemporary Applications - On-line Tutorial on Bayesian nets and probability - Web-App to create Bayesian nets and run it with a Monte Carlo method - Continuous Time Bayesian Networks - Bayesian Networks: Explanation and Analogy - A live tutorial on learning Bayesian networks - A hierarchical Bayes Model for handling sample heterogeneity in classification problems, provides a classification model taking into consideration the uncertainty associated with measuring replicate samples. - Hierarchical Naive Bayes Model for handling sample uncertainty Archived 2007-09-28 at the Wayback Machine, shows how to perform classification and learning with continuous and discrete variables with replicated measurements.",
    "text_length": 37917,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:20.956595"
  },
  {
    "id": "page_15",
    "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
    "domain": "en.wikipedia.org",
    "title": "Evolutionary algorithm - Wikipedia",
    "text": "Evolutionary algorithm | Part of a series on the | | Evolutionary algorithm | |---| | Genetic algorithm (GA) | | Genetic programming (GP) | | Differential evolution | | Evolution strategy | | Evolutionary programming | | Related topics | | Part of a series on | | Artificial intelligence (AI) | |---| Evolutionary algorithms (EA) reproduce essential elements of biological evolution in a computer algorithm in order to solve \"difficult\" problems, at least approximately, for which no exact or satisfactory solution methods are known. They are metaheuristics and population-based bio-inspired algorithms[1] and evolutionary computation, which itself are part of the field of computational intelligence.[2] The mechanisms of biological evolution that an EA mainly imitates are reproduction, mutation, recombination and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolution (microevolutionary processes) and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.[3] In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems;[4][5][6] therefore, there may be no direct link between algorithm complexity and problem complexity. Generic definition [edit]The following is an example of a generic evolutionary algorithm:[7][8][9] - Randomly generate the initial population of individuals, the first generation. - Evaluate the fitness of each individual in the population. - Check, if the goal is reached and the algorithm can be terminated. - Select individuals as parents, preferably of higher fitness. - Produce offspring with optional crossover (mimicking reproduction). - Apply mutation operations on the offspring. - Select individuals preferably of lower fitness for replacement with new individuals (mimicking natural selection). - Return to 2 Types [edit]Similar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem. - Genetic algorithm – This is the most popular type of EA. One seeks the solution of a problem in the form of strings of numbers (traditionally binary, although the best representations are usually those that reflect something about the problem being solved),[3] by applying operators such as recombination and mutation (sometimes one, sometimes both). This type of EA is often used in optimization problems. - Genetic programming – Here the solutions are in the form of computer programs, and their fitness is determined by their ability to solve a computational problem. There are many variants of Genetic Programming: - Evolutionary programming – Similar to evolution strategy, but with a deterministic selection of all parents. - Evolution strategy (ES) – Works with vectors of real numbers as representations of solutions, and typically uses self-adaptive mutation rates. The method is mainly used for numerical optimization, although there are also variants for combinatorial tasks.[10][11][12] - Differential evolution – Based on vector differences and is therefore primarily suited for numerical optimization problems. - Coevolutionary algorithm – Similar to genetic algorithms and evolution strategies, but the created solutions are compared on the basis of their outcomes from interactions with other solutions. Solutions can either compete or cooperate during the search process. Coevolutionary algorithms are often used in scenarios where the fitness landscape is dynamic, complex, or involves competitive interactions.[13][14] - Neuroevolution – Similar to genetic programming but the genomes represent artificial neural networks by describing structure and connection weights. The genome encoding can be direct or indirect. - Learning classifier system – Here the solution is a set of classifiers (rules or conditions). A Michigan-LCS evolves at the level of individual classifiers whereas a Pittsburgh-LCS uses populations of classifier-sets. Initially, classifiers were only binary, but now include real, neural net, or S-expression types. Fitness is typically determined with either a strength or accuracy based reinforcement learning or supervised learning approach. - Quality–Diversity algorithms – QD algorithms simultaneously aim for high-quality and diverse solutions. Unlike traditional optimization algorithms that solely focus on finding the best solution to a problem, QD algorithms explore a wide variety of solutions across a problem space and keep those that are not just high performing, but also diverse and unique.[15][16][17] Theoretical background [edit]The following theoretical principles apply to all or almost all EAs. No free lunch theorem [edit]The no free lunch theorem of optimization states that all optimization strategies are equally effective when the set of all optimization problems is considered. Under the same condition, no evolutionary algorithm is fundamentally better than another. This can only be the case if the set of all problems is restricted. This is exactly what is inevitably done in practice. Therefore, to improve an EA, it must exploit problem knowledge in some form (e.g. by choosing a certain mutation strength or a problem-adapted coding). Thus, if two EAs are compared, this constraint is implied. In addition, an EA can use problem specific knowledge by, for example, not randomly generating the entire start population, but creating some individuals through heuristics or other procedures.[18][19] Another possibility to tailor an EA to a given problem domain is to involve suitable heuristics, local search procedures or other problem-related procedures in the process of generating the offspring. This form of extension of an EA is also known as a memetic algorithm. Both extensions play a major role in practical applications, as they can speed up the search process and make it more robust.[18][20] Convergence [edit]For EAs in which, in addition to the offspring, at least the best individual of the parent generation is used to form the subsequent generation (so-called elitist EAs), there is a general proof of convergence under the condition that an optimum exists. Without loss of generality, a maximum search is assumed for the proof: From the property of elitist offspring acceptance and the existence of the optimum it follows that per generation an improvement of the fitness of the respective best individual will occur with a probability . Thus: I.e., the fitness values represent a monotonically non-decreasing sequence, which is bounded due to the existence of the optimum. From this follows the convergence of the sequence against the optimum. Since the proof makes no statement about the speed of convergence, it is of little help in practical applications of EAs. But it does justify the recommendation to use elitist EAs. However, when using the usual panmictic population model, elitist EAs tend to converge prematurely more than non-elitist ones.[21] In a panmictic population model, mate selection (see step 4 of the generic definition) is such that every individual in the entire population is eligible as a mate. In non-panmictic populations, selection is suitably restricted, so that the dispersal speed of better individuals is reduced compared to panmictic ones. Thus, the general risk of premature convergence of elitist EAs can be significantly reduced by suitable population models that restrict mate selection.[22][23] Virtual alphabets [edit]With the theory of virtual alphabets, David E. Goldberg showed in 1990 that by using a representation with real numbers, an EA that uses classical recombination operators (e.g. uniform or n-point crossover) cannot reach certain areas of the search space, in contrast to a coding with binary numbers.[24] This results in the recommendation for EAs with real representation to use arithmetic operators for recombination (e.g. arithmetic mean or intermediate recombination). With suitable operators, real-valued representations are more effective than binary ones, contrary to earlier opinion.[25][26] Comparison to other concepts [edit]Biological processes [edit]A possible limitation[according to whom?] of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism.[27][28] Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment.[29] Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.[30][improper synthesis?] Monte-Carlo methods [edit]Both method classes have in common that their individual search steps are determined by chance. The main difference, however, is that EAs, like many other metaheuristics, learn from past search steps and incorporate this experience into the execution of the next search steps in a method-specific form. With EAs, this is done firstly through the fitness-based selection operators for partner choice and the formation of the next generation. And secondly, in the type of search steps: In EA, they start from a current solution and change it or they mix the information of two solutions. In contrast, when dicing out new solutions in Monte-Carlo methods, there is usually no connection to existing solutions.[31][32] If, on the other hand, the search space of a task is such that there is nothing to learn, Monte-Carlo methods are an appropriate tool, as they do not contain any algorithmic overhead that attempts to draw suitable conclusions from the previous search. An example of such tasks is the proverbial search for a needle in a haystack, e.g. in the form of a flat (hyper)plane with a single narrow peak. Applications [edit]The areas in which evolutionary algorithms are practically used are almost unlimited[6] and range from industry,[33][34] engineering,[3][4][35] complex scheduling,[5][36][37] agriculture,[38] robot movement planning[39] and finance[40][41] to research[42][43] and art. The application of an evolutionary algorithm requires some rethinking from the inexperienced user, as the approach to a task using an EA is different from conventional exact methods and this is usually not part of the curriculum of engineers or other disciplines. For example, the fitness calculation must not only formulate the goal but also support the evolutionary search process towards it, e.g. by rewarding improvements that do not yet lead to a better evaluation of the original quality criteria. For example, if peak utilisation of resources such as personnel deployment or energy consumption is to be avoided in a scheduling task, it is not sufficient to assess the maximum utilisation. Rather, the number and duration of exceedances of a still acceptable level should also be recorded in order to reward reductions below the actual maximum peak value.[44] There are therefore some publications that are aimed at the beginner and want to help avoiding beginner's mistakes as well as leading an application project to success.[44][45][46] This includes clarifying the fundamental question of when an EA should be used to solve a problem and when it is better not to. Related techniques and other global search methods [edit]There are some other proven and widely used methods of nature inspired global search techniques such as - Memetic algorithm – A hybrid method, inspired by Richard Dawkins's notion of a meme. It commonly takes the form of a population-based algorithm (frequently an EA) coupled with individual learning procedures capable of performing local refinements. Emphasizes the exploitation of problem-specific knowledge and tries to orchestrate local and global search in a synergistic way.[47] - A cellular evolutionary or memetic algorithm uses a topological neighborhood relation between the individuals of a population for restricting the mate selection and by that reducing the propagation speed of above-average individuals. The idea is to maintain genotypic diversity in the population over a longer period of time to reduce the risk of premature convergence.[48] - Ant colony optimization is based on the ideas of ant foraging by pheromone communication to form paths. Primarily suited for combinatorial optimization and graph problems. - Particle swarm optimization is based on the ideas of animal flocking behaviour. Also primarily suited for numerical optimization problems. - Gaussian adaptation – Based on information theory. Used for maximization of manufacturing yield, mean fitness or average information. See for instance Entropy in thermodynamics and information theory.[49] In addition, many new nature-inspired or metaphor-guided algorithms have been proposed since the beginning of this century[when?]. For criticism of most publications on these, see the remarks at the end of the introduction to the article on metaheuristics. Examples [edit]In 2020, Google stated that their AutoML-Zero can successfully rediscover classic algorithms such as the concept of neural networks.[50] The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics. Gallery [edit]- A two-population EA search over a constrained Rosenbrock function with bounded global optimum - A two-population EA search over a constrained Rosenbrock function. Global optimum is not bounded. - A two-population EA search of a bounded optima of Simionescu's function References [edit]- ^ Farinati, Davide; Vanneschi, Leonardo (December 2024). \"A survey on dynamic populations in bio-inspired algorithms\". Genetic Programming and Evolvable Machines. 25 (2) 19. doi:10.1007/s10710-024-09492-4. hdl:10362/170138. - ^ Vikhar, P. A. (2016). \"Evolutionary algorithms: A critical review and its future prospects\". 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC). Jalgaon. pp. 261–265. doi:10.1109/ICGTSPICC.2016.7955308. ISBN 978-1-5090-0467-6. S2CID 22100336. {{cite book}} : CS1 maint: location missing publisher (link) - ^ a b c Cohoon, J. P.; Karro, J.; Lienig, J. (2003). \"Evolutionary Algorithms for the Physical Design of VLSI Circuits\" in Advances in Evolutionary Computing: Theory and Applications (PDF). London: Springer Verlag. pp. 683–712. ISBN 978-3-540-43330-9. - ^ a b Slowik, Adam; Kwasnicka, Halina (2020). \"Evolutionary algorithms and their applications to engineering problems\". Neural Computing and Applications. 32 (16): 12363–12379. doi:10.1007/s00521-020-04832-8. ISSN 0941-0643. S2CID 212732659. - ^ a b Mika, Marek; Waligóra, Grzegorz; Węglarz, Jan (2011). \"Modelling and solving grid resource allocation problem with network resources for workflow applications\". Journal of Scheduling. 14 (3): 291–306. doi:10.1007/s10951-009-0158-0. ISSN 1094-6136. S2CID 31859338. - ^ a b \"International Conference on the Applications of Evolutionary Computation\". The conference is part of the Evo* series. The conference proceedings are published by Springer. Retrieved 2022-12-23. - ^ Jansen, Thomas; Weyland, Dennis (7 July 2007). \"Analysis of evolutionary algorithms for the longest common subsequence problem\". Proceedings of the 9th annual conference on Genetic and evolutionary computation. Association for Computing Machinery. pp. 939–946. doi:10.1145/1276958.1277148. ISBN 978-1-59593-697-4. - ^ Jin, Yaochu (2003). \"Evolutionary Algorithms\". Advanced Fuzzy Systems Design and Applications. Studies in Fuzziness and Soft Computing. Vol. 112. Physica-Verlag HD. pp. 49–71. doi:10.1007/978-3-7908-1771-3_2. ISBN 978-3-7908-2520-6. - ^ Tavares, Jorge; Machado, Penousal; Cardoso, Amílcar; Pereira, Francisco B.; Costa, Ernesto (2004). \"On the Evolution of Evolutionary Algorithms\". Genetic Programming. Lecture Notes in Computer Science. Vol. 3003. Springer. pp. 389–398. doi:10.1007/978-3-540-24650-3_37. ISBN 978-3-540-21346-8. - ^ Nissen, Volker; Krause, Matthias (1994), \"Constrained Combinatorial Optimization with an Evolution Strategy\", in Reusch, Bernd (ed.), Fuzzy Logik, Informatik aktuell, Berlin, Heidelberg: Springer, pp. 33–40, doi:10.1007/978-3-642-79386-8_5, ISBN 978-3-642-79386-8 - ^ Coelho, V. N.; Coelho, I. M.; Souza, M. J. F.; Oliveira, T. A.; Cota, L. P.; Haddad, M. N.; Mladenovic, N.; Silva, R. C. P.; Guimarães, F. G. (2016). \"Hybrid Self-Adaptive Evolution Strategies Guided by Neighborhood Structures for Combinatorial Optimization Problems\". Evol Comput. 24 (4): 637–666. doi:10.1162/EVCO_a_00187. PMID 27258842. S2CID 13582781. - ^ Slowik, Adam; Kwasnicka, Halina (1 August 2020). \"Evolutionary algorithms and their applications to engineering problems\". Neural Computing and Applications. 32 (16): 12363–12379. doi:10.1007/s00521-020-04832-8. ISSN 1433-3058. - ^ Ma, Xiaoliang; Li, Xiaodong; Zhang, Qingfu; Tang, Ke; Liang, Zhengping; Xie, Weixin; Zhu, Zexuan (2019), \"A Survey on Cooperative Co-Evolutionary Algorithms.\", IEEE Transactions on Evolutionary Computation, 23 (3): 421–441, Bibcode:2019ITEC...23..421M, doi:10.1109/TEVC.2018.2868770, S2CID 125149900 - ^ Popovici, Elena; Bucci, Anthony; Wiegand, R. Paul; De Jong, Edwin D. (2012). \"Coevolutionary Principles\". In Rozenberg, Grzegorz; Bäck, Thomas; Kok, Joost N. (eds.). Handbook of Natural Computing. Berlin, Heidelberg: Springer Berlin Heidelberg. pp. 987–1033. doi:10.1007/978-3-540-92910-9_31. ISBN 978-3-540-92910-9. - ^ Pugh, Justin K.; Soros, Lisa B.; Stanley, Kenneth O. (2016-07-12). \"Quality Diversity: A New Frontier for Evolutionary Computation\". Frontiers in Robotics and AI. 3. doi:10.3389/frobt.2016.00040. ISSN 2296-9144. - ^ Lehman, Joel; Stanley, Kenneth O. (2011-07-12). \"Evolving a diversity of virtual creatures through novelty search and local competition\". Proceedings of the 13th annual conference on Genetic and evolutionary computation. New York, NY, USA: ACM. pp. 211–218. doi:10.1145/2001576.2001606. ISBN 9781450305570. S2CID 17338175. - ^ Cully, Antoine; Clune, Jeff; Tarapore, Danesh; Mouret, Jean-Baptiste (2015-05-27). \"Robots that can adapt like animals\". Nature. 521 (7553): 503–507. arXiv:1407.3501. Bibcode:2015Natur.521..503C. doi:10.1038/nature14422. ISSN 0028-0836. PMID 26017452. S2CID 3467239. - ^ a b Davis, Lawrence (1991). Handbook of genetic algorithms. New York: Van Nostrand Reinhold. ISBN 0-442-00173-8. OCLC 23081440. - ^ Lienig, Jens; Brandt, Holger (1994), Davidor, Yuval; Schwefel, Hans-Paul; Männer, Reinhard (eds.), \"An evolutionary algorithm for the routing of multi-chip modules\", Parallel Problem Solving from Nature — PPSN III, vol. 866, Berlin, Heidelberg: Springer, pp. 588–597, doi:10.1007/3-540-58484-6_301, ISBN 978-3-540-58484-1, retrieved 2022-10-18 - ^ Neri, Ferrante; Cotta, Carlos; Moscato, Pablo, eds. (2012). Handbook of Memetic Algorithms. Studies in Computational Intelligence. Vol. 379. Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-23247-3. ISBN 978-3-642-23246-6. - ^ Leung, Yee; Gao, Yong; Xu, Zong-Ben (1997). \"Degree of population diversity - a perspective on premature convergence in genetic algorithms and its Markov chain analysis\". IEEE Transactions on Neural Networks. 8 (5): 1165–1176. doi:10.1109/72.623217. ISSN 1045-9227. PMID 18255718. - ^ Gorges-Schleuter, Martina (1998), Eiben, Agoston E.; Bäck, Thomas; Schoenauer, Marc; Schwefel, Hans-Paul (eds.), \"A comparative study of global and local selection in evolution strategies\", Parallel Problem Solving from Nature — PPSN V, Lecture Notes in Computer Science, vol. 1498, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 367–377, doi:10.1007/bfb0056879, ISBN 978-3-540-65078-2, retrieved 2022-10-21 - ^ Dorronsoro, Bernabe; Alba, Enrique (2008). Cellular Genetic Algorithms. Operations Research/Computer Science Interfaces Series. Vol. 42. Boston, MA: Springer US. doi:10.1007/978-0-387-77610-1. ISBN 978-0-387-77609-5. - ^ Goldberg, David E. (1990), Schwefel, Hans-Paul; Männer, Reinhard (eds.), \"The theory of virtual alphabets\", Parallel Problem Solving from Nature, Lecture Notes in Computer Science, vol. 496, Berlin/Heidelberg: Springer-Verlag (published 1991), pp. 13–22, doi:10.1007/bfb0029726, ISBN 978-3-540-54148-6, retrieved 2022-10-22 - ^ Stender, J.; Hillebrand, E.; Kingdon, J. (1994). Genetic algorithms in optimisation, simulation, and modelling. Amsterdam: IOS Press. ISBN 90-5199-180-0. OCLC 47216370. - ^ Michalewicz, Zbigniew (1996). Genetic Algorithms + Data Structures = Evolution Programs (3rd ed.). Berlin Heidelberg: Springer. ISBN 978-3-662-03315-9. OCLC 851375253. - ^ G.S. Hornby and J.B. Pollack. \"Creating high-level components with a generative representation for body-brain evolution\". Artificial Life, 8(3):223–246, 2002. - ^ Jeff Clune, Benjamin Beckmann, Charles Ofria, and Robert Pennock. \"Evolving Coordinated Quadruped Gaits with the HyperNEAT Generative Encoding\" Archived 2016-06-03 at the Wayback Machine. Proceedings of the IEEE Congress on Evolutionary Computing Special Section on Evolutionary Robotics, 2009. Trondheim, Norway. - ^ J. Clune, C. Ofria, and R. T. Pennock, \"How a generative encoding fares as problem-regularity decreases\", in PPSN (G. Rudolph, T. Jansen, S. M. Lucas, C. Poloni, and N. Beume, eds.), vol. 5199 of Lecture Notes in Computer Science, pp. 358–367, Springer, 2008. - ^ Ferreira, C., 2001. \"Gene Expression Programming: A New Adaptive Algorithm for Solving Problems\". Complex Systems, Vol. 13, issue 2: 87–129. - ^ Schwefel, Hans-Paul (1995). Evolution and Optimum Seeking. Sixth-generation computer technology series. New York: Wiley. p. 109. ISBN 978-0-471-57148-3. - ^ Fogel, David B.; Bäck, Thomas; Michalewicz, Zbigniew, eds. (2000). Evolutionary Computation 1. Bristol ; Philadelphia: Institute of Physics Publishing. pp. xxx and xxxvii (Glossary). ISBN 978-0-7503-0664-5. OCLC 44807816. - ^ Sanchez, Ernesto; Squillero, Giovanni; Tonda, Alberto (2012). Industrial Applications of Evolutionary Algorithms. Intelligent Systems Reference Library. Vol. 34. Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-27467-1. ISBN 978-3-642-27466-4. - ^ Miettinen, Kaisa; Neittaanmäki, Pekka; Mäkelä, M. M.; Périaux, Jacques, eds. (1999). Evolutionary algorithms in engineering and computer science : recent advances in genetic algorithms, evolution strategies, evolutionary programming, genetic programming, and industrial applications. Chichester: Wiley and Sons. ISBN 0-585-29445-3. OCLC 45728460. - ^ Gen, Mitsuo; Cheng, Runwei (1999-12-17). Genetic Algorithms and Engineering Optimization. Wiley Series in Engineering Design and Automation. Hoboken, NJ, USA: John Wiley & Sons, Inc. doi:10.1002/9780470172261. ISBN 978-0-470-17226-1. - ^ Dahal, Keshav P.; Tan, Kay Chen; Cowling, Peter I. (2007). Evolutionary scheduling. Berlin: Springer. doi:10.1007/978-3-540-48584-1. ISBN 978-3-540-48584-1. OCLC 184984689. - ^ Jakob, Wilfried; Strack, Sylvia; Quinte, Alexander; Bengel, Günther; Stucky, Karl-Uwe; Süß, Wolfgang (2013-04-22). \"Fast Rescheduling of Multiple Workflows to Constrained Heterogeneous Resources Using Multi-Criteria Memetic Computing\". Algorithms. 6 (2): 245–277. doi:10.3390/a6020245. ISSN 1999-4893. - ^ Mayer, David G. (2002). Evolutionary Algorithms and Agricultural Systems. Boston, MA: Springer US. doi:10.1007/978-1-4615-1717-7. ISBN 978-1-4613-5693-6. - ^ Blume, Christian (2000), Cagnoni, Stefano (ed.), \"Optimized Collision Free Robot Move Statement Generation by the Evolutionary Software GLEAM\", Real-World Applications of Evolutionary Computing, LNCS 1803, vol. 1803, Berlin, Heidelberg: Springer, pp. 330–341, doi:10.1007/3-540-45561-2_32, ISBN 978-3-540-67353-8, retrieved 2022-12-28 - ^ Aranha, Claus; Iba, Hitoshi (2008), Wobcke, Wayne; Zhang, Mengjie (eds.), \"Application of a Memetic Algorithm to the Portfolio Optimization Problem\", AI 2008: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 5360, Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 512–521, doi:10.1007/978-3-540-89378-3_52, ISBN 978-3-540-89377-6, retrieved 2022-12-23 - ^ Chen, Shu-Heng, ed. (2002). Evolutionary Computation in Economics and Finance. Studies in Fuzziness and Soft Computing. Vol. 100. Heidelberg: Physica-Verlag HD. doi:10.1007/978-3-7908-1784-3. ISBN 978-3-7908-2512-1. - ^ Lohn, J.D.; Linden, D.S.; Hornby, G.S.; Kraus, W.F. (June 2004). \"Evolutionary design of an X-band antenna for NASA's Space Technology 5 mission\". IEEE Antennas and Propagation Society Symposium, 2004. Vol. 3. pp. 2313–2316 Vol.3. doi:10.1109/APS.2004.1331834. hdl:2060/20030067398. ISBN 0-7803-8302-8. - ^ Fogel, Gary; Corne, David (2003). Evolutionary Computation in Bioinformatics. Elsevier. doi:10.1016/b978-1-55860-797-2.x5000-8. ISBN 978-1-55860-797-2. - ^ a b Jakob, Wilfried (2021), Applying Evolutionary Algorithms Successfully - A Guide Gained from Realworld Applications, KIT Scientific Working Papers, vol. 170, Karlsruhe, FRG: KIT Scientific Publishing, arXiv:2107.11300, doi:10.5445/IR/1000135763, S2CID 236318422, retrieved 2022-12-23 - ^ Whitley, Darrell (2001). \"An overview of evolutionary algorithms: practical issues and common pitfalls\". Information and Software Technology. 43 (14): 817–831. doi:10.1016/S0950-5849(01)00188-4. S2CID 18637958. - ^ Eiben, A.E.; Smith, J.E. (2015). \"Working with Evolutionary Algorithms\". Introduction to Evolutionary Computing. Natural Computing Series (2nd ed.). Berlin, Heidelberg: Springer Berlin Heidelberg. pp. 147–163. doi:10.1007/978-3-662-44874-8. ISBN 978-3-662-44873-1. S2CID 20912932. - ^ Singh, Avjeet; Kumar, Anoj (2021). \"Applications of nature-inspired meta-heuristic algorithms: a survey\". International Journal of Advanced Intelligence Paradigms. 20 (3/4) 119026: 388–417. doi:10.1504/IJAIP.2021.119026. - ^ Nguyen, Phan Trung Hai; Sudholt, Dirk (October 2020). \"Memetic algorithms outperform evolutionary algorithms in multimodal optimisation\". Artificial Intelligence. 287 103345. doi:10.1016/j.artint.2020.103345. - ^ Ma, Zhongqiang; Wu, Guohua; Suganthan, Ponnuthurai Nagaratnam; Song, Aijuan; Luo, Qizhang (March 2023). \"Performance assessment and exhaustive listing of 500+ nature-inspired metaheuristic algorithms\". Swarm and Evolutionary Computation. 77 101248. doi:10.1016/j.swevo.2023.101248. - ^ Gent, Edd (13 April 2020). \"Artificial intelligence is evolving all by itself\". Science | AAAS. Archived from the original on 16 April 2020. Retrieved 16 April 2020. - ^ Simionescu, P.A.; Dozier, G.V.; Wainwright, R.L. (2006). \"A Two-Population Evolutionary Algorithm for Constrained Optimization Problems\" (PDF). 2006 IEEE International Conference on Evolutionary Computation. Proc 2006 IEEE International Conference on Evolutionary Computation. Vancouver, Canada. pp. 1647–1653. doi:10.1109/CEC.2006.1688506. ISBN 0-7803-9487-9. S2CID 1717817. Retrieved 7 January 2017. {{cite book}} : CS1 maint: location missing publisher (link) - ^ Simionescu, P.A. (2014). Computer Aided Graphing and Simulation Tools for AutoCAD Users (1st ed.). Boca Raton, FL: CRC Press. ISBN 978-1-4822-5290-3. Bibliography [edit]- Ashlock, D. (2006), Evolutionary Computation for Modeling and Optimization, Springer, New York, doi:10.1007/0-387-31909-3 ISBN 0-387-22196-4. - Bäck, T. (1996), Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford Univ. Press, New York, ISBN 978-0-19-509971-3. - Bäck, T., Fogel, D., Michalewicz, Z. (1999), Evolutionary Computation 1: Basic Algorithms and Operators, CRC Press, Boca Raton, USA, ISBN 978-0-7503-0664-5. - Bäck, T., Fogel, D., Michalewicz, Z. (2000), Evolutionary Computation 2: Advanced Algorithms and Operators, CRC Press, Boca Raton, USA, doi:10.1201/9781420034349 ISBN 978-0-3678-0637-8. - Banzhaf, W., Nordin, P., Keller, R., Francone, F. (1998), Genetic Programming - An Introduction, Morgan Kaufmann, San Francisco, ISBN 978-1-55860-510-7. - Eiben, A.E., Smith, J.E. (2003), Introduction to Evolutionary Computing, Springer, Heidelberg, New York, doi:10.1007/978-3-662-44874-8 ISBN 978-3-662-44873-1. - Holland, J. H. (1992), Adaptation in Natural and Artificial Systems, MIT Press, Cambridge, MA, ISBN 978-0-262-08213-6. - Michalewicz, Z.; Fogel, D.B. (2004), How To Solve It: Modern Heuristics. Springer, Berlin, Heidelberg, ISBN 978-3-642-06134-9, doi:10.1007/978-3-662-07807-5. - Benko, Attila; Dosa, Gyorgy; Tuza, Zsolt (2010). \"Bin Packing/Covering with Delivery, solved with the evolution of algorithms\". 2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA). pp. 298–302. doi:10.1109/BICTA.2010.5645312. ISBN 978-1-4244-6437-1. S2CID 16875144. - Price, K., Storn, R.M., Lampinen, J.A., (2005). Differential Evolution: A Practical Approach to Global Optimization, Springer, Berlin, Heidelberg, ISBN 978-3-642-42416-8, doi:10.1007/3-540-31306-0. - Ingo Rechenberg (1971), Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution (PhD thesis). Reprinted by Fromman-Holzboog (1973). ISBN 3-7728-1642-8 - Hans-Paul Schwefel (1974), Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977). - Hans-Paul Schwefel (1995), Evolution and Optimum Seeking. Wiley & Sons, New York. ISBN 0-471-57148-2 - Simon, D. (2013), Evolutionary Optimization Algorithms Archived 2014-03-10 at the Wayback Machine, Wiley & Sons, ISBN 978-0-470-93741-9 - Kruse, Rudolf; Borgelt, Christian; Klawonn, Frank; Moewes, Christian; Steinbrecher, Matthias; Held, Pascal (2013), Computational Intelligence: A Methodological Introduction. Springer, London. ISBN 978-1-4471-5012-1, doi:10.1007/978-1-4471-5013-8. - Rahman, Rosshairy Abd.; Kendall, Graham; Ramli, Razamin; Jamari, Zainoddin; Ku-Mahamud, Ku Ruhana (2017). \"Shrimp Feed Formulation via Evolutionary Algorithm with Power Heuristics for Handling Constraints\". Complexity. 2017: 1–12. doi:10.1155/2017/7053710.",
    "text_length": 31043,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:23.352382"
  },
  {
    "id": "page_16",
    "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system",
    "domain": "en.wikipedia.org",
    "title": "Hybrid intelligent system - Wikipedia",
    "text": "Hybrid intelligent system | Part of a series on | | Artificial intelligence (AI) | |---| Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as: - Neuro-symbolic systems - Neuro-fuzzy systems - Hybrid connectionist-symbolic models - Fuzzy expert systems - Connectionist expert systems - Evolutionary neural networks - Genetic fuzzy systems - Rough fuzzy hybridization - Reinforcement learning with fuzzy, neural, or evolutionary methods as well as symbolic reasoning methods. From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, Angelo Dalli and Michael A. Arbib. An example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning (even by hybrid wisdom[1]). Intelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy. See also [edit]- AI alignment - AI effect - Applications of artificial intelligence - Artificial intelligence systems integration - Intelligent control - Lists References [edit]- ^ Besharati, Mohammad Reza; Izadi, Mohammad (November 3, 2025), Hybrid Wisdom, doi:10.20944/preprints202511.0006.v1, retrieved November 3, 2025 - R. Sun & L. Bookman, (eds.), Computational Architectures Integrating Neural and Symbolic Processes. Kluwer Academic Publishers, Needham, MA. 1994. http://www.cogsci.rpi.edu/~rsun/book2-ann.html Archived 2009-05-05 at the Wayback Machine - S. Wermter and R. Sun, (eds.) Hybrid Neural Systems. Springer-Verlag, Heidelberg. 2000. http://www.cogsci.rpi.edu/~rsun/book4-ann.html Archived 2009-09-24 at the Wayback Machine - R. Sun and F. Alexandre, (eds.) Connectionist-Symbolic Integration. Lawrence Erlbaum Associates, Mahwah, NJ. 1997. - Ibaraki, S. Hybrid Intelligence interview with Angelo Dalli in IEEE Technology and Management Society. 2024. - Albus, J. S., Bostelman, R., Chang, T., Hong, T., Shackleford, W., and Shneier, M. Learning in a Hierarchical Control System: 4D/RCS in the DARPA LAGR Program NIST, 2006 - A.S. d'Avila Garcez, Luis C. Lamb & Dov M. Gabbay. Neural-Symbolic Cognitive Reasoning. Cognitive Technologies, Springer (2009). ISBN 978-3-540-73245-7. - International Journal of Hybrid Intelligent Systems - http://www.iospress.nl/html/14485869.php Archived 2005-12-11 at the Wayback Machine - International Conference on Hybrid Intelligent Systems http://his.hybridsystem.com/ - HIS'01: http://www.softcomputing.net/his01/ - HIS'02: https://web.archive.org/web/20060209160923/http://tamarugo.cec.uchile.cl/~his02/ - HIS'03: http://www.softcomputing.net/his03/ - HIS'04: https://web.archive.org/web/20060303051902/http://www.cs.nmt.edu/~his04/ - HIS'05: https://web.archive.org/web/20051223013031/http://www.ica.ele.puc-rio.br/his05/ - HIS'06 https://web.archive.org/web/20110510025133/http://his-ncei06.kedri.info/ - HIS'7 September 17–19, 2007, Kaiserslautern, Germany, http://www.eit.uni-kl.de/koenig/HIS07_Web/his07main.html - hybrid systems resources: http://www.cogsci.rpi.edu/~rsun/hybrid-resource.html Archived 2009-09-25 at the Wayback Machine Further reading [edit]- Shiralkar, Shreekant. Games for Gravitas: A Playbook to Build Human Advantage in the Hybrid Intelligence Era. [Independently published], [November, 2025]. [ISBN-13 : 979-8272610102 ]",
    "text_length": 4067,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:26.656534"
  },
  {
    "id": "page_17",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration",
    "domain": "en.wikipedia.org",
    "title": "Artificial intelligence systems integration - Wikipedia",
    "text": "Artificial intelligence systems integration | Part of a series on | | Artificial intelligence (AI) | |---| The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system. Most artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch. Integration focus [edit]The focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary. Challenges and solutions [edit]Collaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others. The outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples: - Speech synthesis - FreeTTS from CMU - Speech recognition - Sphinx from CMU - Logical reasoning - OpenCyc from Cycorp - Open Mind Common Sense Net from MIT With the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example, serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled \"When AI goes wrong: Fatal errors in oncological research reviewing assistance\".[1] The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics. Many online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with ease. Methodologies [edit]Constructionist design methodology [edit]The constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM. Examples [edit]- ASIMO, Honda's humanoid robot, and QRIO, Sony's version of a humanoid robot. - Cog, M.I.T. humanoid robot project under the direction of Rodney Brooks. - AIBO, Sony's robot dog, integrates vision, hearing and motorskills. - TOPIO, TOSY's humanoid robot can play ping-pong with human See also [edit]- Hybrid intelligent system, systems that combine the methods of traditional symbolic AI & that of Computational intelligence. - Neurosymbolic AI - Humanoid robots utilize systems integration intensely. - Constructionist design methodology - Cognitive architectures References [edit]- ^ Al-Raeei, Marwan (March 20, 2024). \"When AI goes wrong: Fatal errors in oncological research reviewing assistance\". Oral Oncology Reports. 10 100292. doi:10.1016/j.oor.2024.100292. ISSN 2772-9060. Notes [edit]- Constructionist Design Methodology, published in A.I. magazine - MissionEngine: Multi-system integration using Python in the Tactical Language Project",
    "text_length": 6581,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:28.417563"
  },
  {
    "id": "page_18",
    "url": "https://en.wikipedia.org/wiki/Open-source_artificial_intelligence",
    "domain": "en.wikipedia.org",
    "title": "Open-source artificial intelligence - Wikipedia",
    "text": "Open-source artificial intelligence | Part of a series on | | Artificial intelligence (AI) | |---| Open-source artificial intelligence, as defined by the Open Source Initiative, is an AI system that is freely available to use, study, modify, and share.[1][2] This includes datasets used to train the model, its code, and model parameters, promoting a collaborative and transparent approach to AI development so someone could create a substantially similar result.[3][4] The debate over what should count as ‘open-source’ given a range of openness among AI projects has been significant. Some large language models touted as open-sourced that only release model-weights (but not training data and code)[5][6] have been criticized as \"openwashing\"[7] systems that are mostly closed.[8] Free and open-source software (FOSS) licenses, such as the Apache License, MIT License, and GNU General Public License, outline the terms under which open-source artificial intelligence can be accessed, modified, and redistributed.[9] Popular open-source artificial intelligence project categories include large language models, machine translation tools, and chatbots.[10] Debate over the benefits and risks of open-sourced AI involve a range of factors like security, privacy and technological advancement.[11][12][8][13] History [edit]The history of open-source artificial intelligence is intertwined with both the development of AI technologies and the growth of the open-source software movement.[14] Open-source AI has evolved significantly over the past few decades, with contributions from various academic institutions, research labs, tech companies, and independent developers.[15][better source needed] This section explores the major milestones in the development of open-source AI, from its early days to its current state. 1990s: Early development of AI and open-source software [edit]The concept of AI dates back to the mid-20th century, when computer scientists like Alan Turing and John McCarthy laid the groundwork for modern AI theories and algorithms.[16] An early form of AI, the natural language processing \"doctor\" ELIZA, was re-implemented and shared in 1977 by Jeff Shrager as a BASIC program, and soon translated to many other languages. Early AI research focused on developing symbolic reasoning systems and rule-based expert systems.[17] During this period, the idea of open-source software was beginning to take shape, with pioneers like Richard Stallman advocating for free software as a means to promote collaboration and innovation in programming.[18] The Free Software Foundation, founded in 1985 by Stallman, was one of the first major organizations to promote the idea of software that could be freely used, modified, and distributed. The ideas from this movement eventually influenced the development of open-source AI, as more developers began to see the potential benefits of open collaboration in software creation, including AI models and algorithms.[19][better source needed][15][better source needed] In the 1990s, open-source software began to gain more traction,[20][better source needed] the rise of machine learning and statistical methods also led to the development of more practical AI tools. In 1993, the CMU Artificial Intelligence Repository was initiated, with a variety of openly shared software.[21][better source needed] 2000s: Emergence of open-source AI [edit]In the early 2000s open-source AI began to take off, with the release of more user-friendly foundational libraries and frameworks that were available for anyone to use and contribute to.[22][better source needed] OpenCV was released in 2000[23] with a variety of traditional AI algorithms like decision trees, k-Nearest Neighbors (kNN), Naive Bayes and Support Vector Machines (SVM).[24] 2010s: Rise of open-source AI frameworks [edit]Open-source deep learning framework as Torch was released in 2002 and made open-source with Torch7 in 2011, and was later augmented by PyTorch, and TensorFlow.[25] AlexNet was released in 2012.[26] OpenAI was founded in 2015 with a mission to create open-source artificial intelligence that benefited humanity, at least in part to help with recruitment in the early phases of the organization.[27] GPT-1 was released in 2018. 2020s: Open-weight and open-source generative AI [edit]With the announcement of GPT-2 in 2019, OpenAI originally planned to keep the source code of their models private citing concerns about malicious applications.[28] After OpenAI faced public backlash, however, it released the source code for GPT-2 to GitHub three months after its release.[28] OpenAI did not publicly release the source code or pretrained weights for the GPT-3 model.[29] At the time of GPT-3's release GPT-2 was still the most powerful open source language model in the world. Competition in building more open models included mostly smaller efforts like EleutherAI.[30][31] 2022 also saw the rise of larger and more powerful models under licenses of varying openness including Meta's OPT.[32] The Open Source Initiative consulted experts over two years to create a definition of \"open-source\" that would fit the needs of AI software and models. The most controversial aspect relates to data access, since some models are trained on sensitive data which can't be released. In 2024, they published the Open Source AI Definition 1.0 (OSAID 1.0).[1][2][3] It requires full release of the software for processing the data, training the model and making inferences from the model. For the data, it only requires access to details about the data used to train the AI so others can understand and re-create it.[2] In 2023, Llama 1 and 2 and Mistral AI's Mistral and Mixtral open-weight models were first released,[33][34] along with MosaicML's MPT open-source model.[35][36] In 2024, Meta released a collection of large AI models, including Llama 3.1 405B, which was competitive with less open models.[37] The company claimed its approach to AI would be open-source, differing from other major tech companies.[37] The Open Source Initiative and others stated that Llama is not open-source despite Meta describing it as open-source, due to Llama's software license prohibiting it from being used for some purposes.[38][39][40] DeepSeek released their V3 LLM in December 2024, and their R1 reasoning model on January 20, 2025, both as open-weights models under the MIT license.[41][42] This release made widely known how China had been embracing using and building more open AI systems as a way to reduce reliance on western software and gatekeeping as well as to help give its industries access to higher-powered AI more quickly.[43] Projects based in China have since become more widely used around the world as well as they have closed at least some of the gap with leading proprietary American models.[43][44][45] Since the release of OpenAI's proprietary ChatGPT model in late 2022, there have been only a few fully open (weights, data, code, etc.) large language models released. In September 2025, a Swiss consortium added to this short list by releasing a fully open model named Apertus.[46][47] In December 2025, the Linux Foundation created the Agentic AI Foundation, which assumed control of some open-source agentic AI protocols and other technologies created by OpenAI, Anthropic and Block.[48][49] Significance [edit]The label ‘open-source’ can provide real benefits to companies looking to hire top talent or attract customers.[4] The debate around \"openwashing” (or calling a project open-source when it is mostly closed) has big implications for the success of various projects within the industry.[7] Open-source artificial intelligence tends to get more support and adoption in countries and companies that do not have their own leading AI model.[4] These open-source projects can help to undercut the position of business and geopolitical rivals with the strongest proprietary models.[4] Applications [edit]Healthcare [edit]In the healthcare industry, open-source AI has been used in diagnostics, patient care, and personalized treatment options.[50] Open-source libraries have been used for medical imaging for tasks such as tumor detection, improving the speed and accuracy of diagnostic processes.[51][50] Additionally, OpenChem, an open-source library specifically geared toward chemistry and biology applications, enables the development of predictive models for drug discovery, helping researchers identify potential compounds for treatment.[52] Military [edit]Meta's Llama models, which have been described as open-source by Meta, were adopted by U.S. defense contractors like Lockheed Martin and Oracle after unauthorized adaptations by Chinese researchers affiliated with the People's Liberation Army (PLA) came to light.[53][54] The Open Source Initiative and others have contested Meta's use of the term open-source to describe Llama, due to Llama's license containing an acceptable use policy that prohibits use cases including non-U.S. military use.[40] Chinese researchers used an earlier version of Llama to develop tools like ChatBIT, optimized for military intelligence and decision-making, prompting Meta to expand its partnerships with U.S. contractors to ensure the technology could be used strategically for national security.[54] These applications now include logistics, maintenance, and cybersecurity enhancements.[54] Benefits [edit]Privacy and independence [edit]A Nature editorial suggests medical care could become dependent on AI models that could be taken down at any time, are difficult to evaluate, and may threaten patient privacy.[12][55] Its authors propose that health-care institutions, academic researchers, clinicians, patients and technology companies worldwide should collaborate to build open-source models for health care of which the underlying code and base models are easily accessible and can be fine-tuned freely with own data sets.[12] Free speech [edit]Open-source models are harder to censor than close-sourced ones.[55] Collaboration and faster advancements [edit]Large-scale collaborations, such as those seen in the development of open-source frameworks like TensorFlow and PyTorch, have accelerated advancements in machine learning (ML) and deep learning.[56] The open-source nature of these platforms also facilitates rapid iteration and improvement, as contributors from across the globe can propose modifications and enhancements to existing tools.[56] Democratizing access [edit]Open-source allows countries and organizations that otherwise do not have access to proprietary models a way to use and invest in AI more cheaply.[4][57][58] This can help to create an ecosystem for other businesses to sell services on top of.[59] Transparency [edit]One key benefit of open-source AI is the increased transparency it offers compared to closed-source alternatives.[60][better source needed] The open-sourced aspects of models allow those algorithms and code to be inspected, which promotes accountability and helps developers understand how a model reaches its conclusions.[61][better source needed] Additionally, open-weight models, such as Llama and Stable Diffusion, allow developers to directly access model parameters, potentially facilitating the reduced bias and increased fairness in their applications.[61][better source needed] This transparency can help create systems with human-readable outputs, or \"explainable AI\", which is a growingly key concern, especially in high-stakes applications such as healthcare, criminal justice, and finance, where the consequences of decisions made by AI systems can be significant.[62][better source needed] Concerns [edit]Quality and security [edit]Open sourced models have fewer ways to prevent them from being used for malicious activities.[55] Open-source AI may allow bioterrorism groups to remove fine-tuning and other safeguards of AI models.[11][4][55] One proposed step towards reducing these kinds of harms could be to require models to have their risks evaluated and pass a certain standard before being released.[55] A July 2024 report by the White House found it did not yet find sufficient evidence to restrict revealing model weights,[63] though a number of experts in 2024 seemed more concerned about future advances than present-day capabilities.[55] Once an open-source model is public, it cannot be rolled back or updated if serious security issues are detected.[64][better source needed] The main barrier to developing real-world terrorist schemes lies in stringent restrictions on necessary materials and equipment.[64][better source needed] Furthermore, the rapid pace of AI advancement makes it less appealing to use older models, which are more vulnerable to attacks but also less capable.[64][better source needed] Researchers have also criticized open-source artificial intelligence for existing security and ethical concerns. An analysis of over 100,000 open-source models on Hugging Face and GitHub using code vulnerability scanners like Bandit, FlawFinder, and Semgrep found that over 30% of models have high-severity vulnerabilities.[65][better source needed] Furthermore, closed models typically have fewer safety risks than open-sourced models.[64][better source needed] The freedom to augment open-source models has led to developers releasing models without ethical guidelines, such as GPT4-Chan.[64][better source needed] Practicality [edit]Even with truly open-source AI, the cost of training a model oneself can still be prohibitively expensive for many users, unlike other open-source projects that require only downloading code.[4][59] Partially open-sourced code that is released with many legal restrictions has scared off some companies from using those projects for fear of a future lawsuit[4] or a change in the terms and conditions.[59] See also [edit]References [edit]- ^ a b Williams, Rhiannon; O'Donnell, James (August 22, 2024). \"We finally have a definition for open-source AI\". MIT Technology Review. Retrieved 28 November 2024. - ^ a b c Robison, Kylie (28 October 2024). \"Open-source AI must reveal its training data, per new OSI definition\". The Verge. Retrieved 28 November 2024. - ^ a b \"The Open Source AI Definition – 1.0\". Open Source Initiative. Archived from the original on 2025-03-31. Retrieved 2024-11-14. - ^ a b c d e f g h \"A battle is raging over the definition of open-source AI\". The Economist. November 6, 2024. ISSN 0013-0613. Retrieved 2025-12-09. - ^ \"Open Weights: not quite what you've been told\". Open Source Initiative. Retrieved 2025-09-23. - ^ \"OpenAI releases lower-cost models to rival Meta, Mistral and DeepSeek\". CNBC. 2025-08-05. Retrieved 2025-09-23. - ^ a b Liesenfeld, Andreas; Dingemanse, Mark (5 June 2024). \"Rethinking open source generative AI: Open washing and the EU AI Act\". The 2024 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery. pp. 1774–1787. doi:10.1145/3630106.3659005. ISBN 979-8-4007-0450-5. - ^ a b Widder, David Gray; Whittaker, Meredith; West, Sarah Myers (November 2024). \"Why 'open' AI systems are actually closed, and why this matters\". Nature. 635 (8040): 827–833. Bibcode:2024Natur.635..827W. doi:10.1038/s41586-024-08141-1. ISSN 1476-4687. PMID 39604616. - ^ \"Licenses\". Open Source Initiative. Archived from the original on 2018-02-10. Retrieved 2024-11-14. - ^ Castelvecchi, Davide (29 June 2023). \"Open-source AI chatbots are booming — what does this mean for researchers?\". Nature. 618 (7967): 891–892. Bibcode:2023Natur.618..891C. doi:10.1038/d41586-023-01970-6. PMID 37340135. - ^ a b Sandbrink, Jonas (2023-08-07). \"ChatGPT could make bioterrorism horrifyingly easy\". Vox. Retrieved 2024-11-14. - ^ a b c Toma, Augustin; Senkaiahliyan, Senthujan; Lawler, Patrick R.; Rubin, Barry; Wang, Bo (December 2023). \"Generative AI could revolutionize health care — but not if control is ceded to big tech\". Nature. 624 (7990): 36–38. Bibcode:2023Natur.624...36T. doi:10.1038/d41586-023-03803-y. PMID 38036861. - ^ Davies, Pascale (20 February 2024). \"What is open source AI and why is profit so important to the debate?\". Euronews. Retrieved 28 November 2024. - ^ Morrone, Megan (2024-02-15). \"With the rise of AI, the software business redefines \"open\"\". Axios. Retrieved 2025-12-16. - ^ a b Daigle, Kyle (2023-11-08). \"Octoverse: The state of open source and rise of AI in 2023\". The GitHub Blog. Retrieved 2024-11-24. - ^ \"Appendix I: A Short History of AI | One Hundred Year Study on Artificial Intelligence (AI100)\". ai100.stanford.edu. Retrieved 2024-11-24. - ^ Kautz, Henry (2022-03-31). \"The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture\". AI Magazine. 43 (1): 105–125. doi:10.1002/aaai.12036. ISSN 2371-9621. - ^ \"Why Software Should Be Free - GNU Project - Free Software Foundation\". www.gnu.org. Archived from the original on 2024-12-01. Retrieved 2024-11-24. - ^ \"The Power of Collaboration: How Open-Source Projects are Advancing AI\". kdnuggets.com. - ^ Code, Linux (2024-11-03). \"A Brief History of Open Source\". TheLinuxCode. Retrieved 2024-11-24.[permanent dead link] - ^ \"Topic: (/)\". www.cs.cmu.edu. Retrieved 2025-09-11. - ^ Priya (2024-03-28). \"The Evolution of Open Source AI Libraries: From Basement Brawls to AI All-Stars\". TheGen.AI. Retrieved 2024-11-24. - ^ Pulli, Kari; Baksheev, Anatoly; Kornyakov, Kirill; Eruhimov, Victor (1 April 2012). \"Realtime Computer Vision with OpenCV\". ACM Queue. 10 (4): 40:40–40:56. doi:10.1145/2181796.2206309. - ^ Adrian Kaehler; Gary Bradski (14 December 2016). Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library. O'Reilly Media. pp. 26ff. ISBN 978-1-4919-3800-3. - ^ Costa, Carlos J.; Aparicio, Manuela; Aparicio, Sofia; Aparicio, Joao Tiago (January 2024). \"The Democratization of Artificial Intelligence: Theoretical Framework\". Applied Sciences. 14 (18): 8236. doi:10.3390/app14188236. hdl:10362/173131. ISSN 2076-3417. - ^ Lee, Timothy B. (2024-11-11). \"How a stubborn computer scientist accidentally launched the deep learning boom\". Ars Technica. Retrieved 2025-09-11. - ^ Metz, Rachel (2024-03-15). \"OpenAI and the Fierce AI Industry Debate Over Open Source\". Bloomberg.com. Retrieved 2025-12-16. - ^ a b Xiang, Chloe (2023-02-28). \"OpenAI Is Now Everything It Promised Not to Be: Corporate, Closed-Source, and For-Profit\". VICE. Retrieved 2024-11-14. - ^ Hao, Karen (September 23, 2020). \"OpenAI is giving Microsoft exclusive access to its GPT-3 language model\". MIT Technology Review. Archived from the original on 2021-02-05. Retrieved 2024-12-08. - ^ \"GPT-3's free alternative GPT-Neo is something to be excited about\". VentureBeat. 2021-05-15. Archived from the original on 9 March 2023. Retrieved 2023-04-14. - ^ \"EleutherAI: When OpenAI Isn't Open Enough\". IEEE Spectrum. 2021-06-02. Archived from the original on March 27, 2022. - ^ Heaven, Will (2022-05-03). \"Meta has built a massive new language AI—and it's giving it away for free\". MIT Technology Review. Retrieved 2023-12-26. - ^ Nicol-Schwarz, Kai (2025-12-02). \"French AI lab Mistral releases new AI models as it looks to keep pace with OpenAI and Google\". CNBC. Retrieved 2025-12-05. - ^ Heikkilä, Melissa (December 2, 2025). \"Mistral unveils new models in race to gain edge in 'open' AI\". Financial Times. Retrieved 2025-12-05. - ^ Nunez, Michael (2023-06-22). \"MosaicML challenges OpenAI with its new open-source language model\". VentureBeat. Retrieved 2025-07-21. - ^ Chen, Joanne (2023-07-19). \"MosaicML launches MPT-7B-8K, a 7B-parameter open-source LLM with 8k context length\". VentureBeat. Retrieved 2025-07-21. - ^ a b Mirjalili, Seyedali (2024-08-01). \"Meta just launched the largest 'open' AI model in history. Here's why it matters\". The Conversation. Retrieved 2024-11-14. - ^ Waters, Richard (2024-10-17). \"Meta under fire for 'polluting' open-source\". Financial Times. Retrieved 2024-11-14. - ^ Edwards, Benj (18 July 2023). \"Meta launches Llama 2, a source-available AI model that allows commercial applications\". Ars Technica. Archived from the original on 7 November 2023. Retrieved 14 December 2024. - ^ a b \"Meta offers Llama AI to US government for national security\". CIO. 5 November 2024. Archived from the original on 14 December 2024. Retrieved 14 December 2024. - ^ Chen, Caiwei (January 24, 2025). \"How a top Chinese AI model overcame US sanctions\". MIT Technology Review. Archived from the original on 2025-01-25. Retrieved 2025-02-03. - ^ Guo, Daya; et al. (18 September 2025). \"DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning\". Nature. 645 (8081): 633–638. Bibcode:2025Natur.645..633G. doi:10.1038/s41586-025-09422-z. PMC 12443585. PMID 40962978. - ^ a b Bloom, Peter (2025-02-12). \"DeepSeek: how China's embrace of open-source AI caused a geopolitical earthquake\". The Conversation. Retrieved 2025-12-09. - ^ Huang, Raffaele (2025-08-13). \"China's Lead in Open-Source AI Jolts Washington and Silicon Valley\". The Wall Street Journal. Retrieved 2025-12-09. - ^ Cui, Jasmine; Perlo, Jared (2025-11-30). \"More of Silicon Valley is building on free Chinese AI\". NBC News. Retrieved 2025-12-09. - ^ Welle, Elissa (2025-09-03). \"Switzerland releases an open-weight AI model\". The Verge. Retrieved 2025-10-08. - ^ Allen, Matthew (2025-09-02). \"Switzerland launches transparent ChatGPT alternative\". SWI swissinfo.ch. Retrieved 2025-10-08. - ^ Knight, Will. \"OpenAI, Anthropic, and Block Are Teaming Up to Make AI Agents Play Nice\". Wired. ISSN 1059-1028. Retrieved 2025-12-16. - ^ Claburn, Thomas (December 9, 2025). \"Linux Foundation aims to become the Switzerland of AI agents\". The Register. - ^ a b Esteva, Andre; Robicquet, Alexandre; Ramsundar, Bharath; Kuleshov, Volodymyr; DePristo, Mark; Chou, Katherine; Cui, Claire; Corrado, Greg; Thrun, Sebastian; Dean, Jeff (January 2019). \"A guide to deep learning in healthcare\". Nature Medicine. 25 (1): 24–29. Bibcode:2019NatMe..25...24E. doi:10.1038/s41591-018-0316-z. ISSN 1546-170X. PMID 30617335. - ^ Ashraf, Mudasir; Ahmad, Syed Mudasir; Ganai, Nazir Ahmad; Shah, Riaz Ahmad; Zaman, Majid; Khan, Sameer Ahmad; Shah, Aftab Aalam (2021). \"Prediction of Cardiovascular Disease Through Cutting-Edge Deep Learning Technologies: An Empirical Study Based on TENSORFLOW, PYTORCH and KERAS\". In Gupta, Deepak; Khanna, Ashish; Bhattacharyya, Siddhartha; Hassanien, Aboul Ella; Anand, Sameer; Jaiswal, Ajay (eds.). International Conference on Innovative Computing and Communications. Advances in Intelligent Systems and Computing. Vol. 1165. Singapore: Springer. pp. 239–255. doi:10.1007/978-981-15-5113-0_18. ISBN 978-981-15-5113-0. - ^ Korshunova, Maria; Ginsburg, Boris; Tropsha, Alexander; Isayev, Olexandr (2021-01-25). \"OpenChem: A Deep Learning Toolkit for Computational Chemistry and Drug Design\". Journal of Chemical Information and Modeling. 61 (1): 7–13. doi:10.1021/acs.jcim.0c00971. ISSN 1549-9596. PMID 33393291. - ^ Pomfret, James; Pang, Jessie; Pomfret, James; Pang, Jessie (2024-11-01). \"Exclusive: Chinese researchers develop AI model for military use on back of Meta's Llama\". Reuters. Retrieved 2024-11-16. - ^ a b c Roth, Emma (2024-11-04). \"Meta AI is ready for war\". The Verge. Retrieved 2024-11-16. - ^ a b c d e f Piper, Kelsey (2024-02-02). \"Should we make our most powerful AI models open source to all?\". Vox. Retrieved 2025-12-16. - ^ a b Dean, Jeffrey (2022-05-01). \"A Golden Decade of Deep Learning: Computing Systems & Applications\". Daedalus. 151 (2): 58–74. doi:10.1162/daed_a_01900. ISSN 0011-5266. - ^ Hassri, Myftahuddin Hazmi; Man, Mustafa (2023-12-07). \"The Impact of Open-Source Software on Artificial Intelligence\". Journal of Mathematical Sciences and Informatics. 3 (2). doi:10.46754/jmsi.2023.12.006. ISSN 2948-3697. - ^ Solaiman, Irene (May 24, 2023). \"Generative AI Systems Aren't Just Open or Closed Source\". Wired. Archived from the original on November 27, 2023. Retrieved July 20, 2023. - ^ a b c Lin, Belle (2024-03-21). \"Open-Source Companies Are Sharing Their AI Free. Can They Crack OpenAI's Dominance?\". Wall Street Journal. ISSN 0099-9660. Retrieved 2025-12-16. - ^ MACHADO, J. (2025). Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs. Conference Paper. arXiv:2505.10603. - ^ a b White, Matt; Haddad, Ibrahim; Osborne, Cailean; Xiao-Yang Yanglet Liu; Abdelmonsef, Ahmed; Varghese, Sachin; Arnaud Le Hors (2024). \"The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence\". arXiv:2403.13784 [cs.LG]. - ^ Gujar, Praveen. \"Council Post: Building Trust In AI: Overcoming Bias, Privacy And Transparency Challenges\". Forbes. Retrieved 2024-11-27. - ^ O'Brien, Matt (2024-07-30). \"White House says no need to restrict open-source AI, for now\". Associated Press. PBS News. Retrieved 2024-11-14. - ^ a b c d e Eiras, Francisco; Petrov, Aleksandar; Vidgen, Bertie; Schroeder, Christian; Pizzati, Fabio; Elkins, Katherine; Mukhopadhyay, Supratik; Bibi, Adel; Purewal, Aaron (2024-05-29). \"Risks and Opportunities of Open-Source Generative AI\". arXiv:2405.08597 [cs.LG]. - ^ Kathikar, Adhishree; Nair, Aishwarya; Lazarine, Ben (2023). \"Assessing the Vulnerabilities of the Open-Source Artificial Intelligence (AI) Landscape: A Large-Scale Analysis of the Hugging Face Platform\". 2023 IEEE International Conference on Intelligence and Security Informatics (ISI). pp. 1–6. doi:10.1109/ISI58743.2023.10297271. ISBN 979-8-3503-3773-0.",
    "text_length": 25463,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:30.803324"
  },
  {
    "id": "page_19",
    "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
    "domain": "en.wikipedia.org",
    "title": "Applications of artificial intelligence - Wikipedia",
    "text": "Applications of artificial intelligence This article has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these messages) | | Part of a series on | | Artificial intelligence (AI) | |---| Artificial intelligence is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. Artificial intelligence has been used in applications throughout industry and academia. Within the field of Artificial Intelligence, there are multiple subfields. The subfield of Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce. In recent years, there have been massive advancements in the field of generative artificial intelligence, which uses generative models to produce text, images, videos or other forms of data.[4] This article describes applications of AI in different sectors. Agriculture [edit]In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[5] AI has been used to attempt to classify livestock pig call emotions,[6] automate greenhouses,[7] detect diseases and pests,[8] and optimize irrigation.[9] Architecture and design [edit]Artificial intelligence in architecture is the use of artificial intelligence in automation, design, and planning in the architectural process or in assisting human skills in the field of architecture.[10] AI has been used by some architects for design, and has been proposed as a way to automate planning and routine tasks in the field.[11][12] Business [edit]A 2023 study found that generative AI increased productivity by 15% in contact centers.[13] Another 2023 study found it increased productivity by up to 40% in writing tasks.[14] An August 2025 review by MIT found that of surveyed companies, 95% did not report any improvement in revenue from the use of AI.[15] A September 2025 article by the Harvard Business Review describes how increased use of AI does not automatically lead to increases in revenue or actual productivity. Referring to \"AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task\" the article coins the term workslop. Per studies done in collaboration with the Stanford Social Media Lab, workslop does not improve productivity and undermines trust and collaboration among colleagues.[16] Computer science [edit]Programming assistance [edit]AI-assisted software development [edit]AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. AI-assisted software development systems differ in functionality, quality, speed, and approach to privacy. Creating software primarily via AI is known as \"vibe coding\". Code created or suggested by AI can be incorrect or inefficient.[17] The use of AI-assisted coding can potentially speed-up software development, but can also slow-down the process by creating more work when debugging and testing.[18][19] The rush to prematurely adopt AI technology can also incur additional technical debt.[18] AI also requires additional consideration and careful review for cybersecurity, since AI coding software is trained on a wide range of code of inconsistent quality and often replicates poor practices.[20][21] Neural network design [edit]AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[22] Quantum computing [edit]Research and development of quantum computers has been performed with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications.[23][24] The use of quantum machine learning for quantum simulators has been proposed for solving physics and chemistry problems.[25][26][better source needed] Historical contributions [edit]AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[27] - Time sharing - Interactive interpreters - Graphical user interfaces and the computer mouse - Rapid application development environments - The linked list data structure - Automatic storage management - Symbolic programming - Functional programming - Dynamic programming - Object-oriented programming - Optical character recognition - Constraint satisfaction Customer service [edit]Human resources [edit]Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[citation needed] Online and telephone customer service [edit]AI underlies avatars (automated online assistants) on web pages.[28] It can reduce operation and training costs.[28] Pypestream automated customer service for its mobile application to streamline communication with customers.[29] A Google app analyzes language and converts speech into text.[30] The platform can identify angry customers through their language and respond appropriately.[31] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[32] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[33] Hospitality [edit]In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[34] AI hotel services come in the form of a chatbot,[35] application, virtual voice assistant and service robots. Education [edit]In educational institutions, AI has been used to automate routine tasks like attendance tracking, grading and marking. AI tools have been used to attempt to monitor student progress and analyze learning behaviors, with the intention of facilitating interventions for students facing academic problems.[36] Energy and environment [edit]Energy system [edit]The U.S. Department of Energy wrote in an April 2024 report that AI may have applications in modeling power grids, reviewing federal permits with large language models, predicting levels of renewable energy production, and improving the planning process for electrical vehicle charging networks.[37] Other studies have suggested that machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[38][39][40][41][42] Environmental monitoring [edit]Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[43] or remote sensing and other applications of environmental monitoring make use of machine learning.[44][45][46][47] For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[48][49] Early-warning systems [edit]Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[50][51] earthquakes,[52][53][54] landslides,[55] heavy rainfall,[56] long-term water supply vulnerability,[57] tipping-points of ecosystem collapse,[58] cyanobacterial bloom outbreaks,[59] and droughts.[60][61][62] Economic and social challenges [edit]The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[63] Entertainment and media [edit]Media [edit]AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision. Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement. - Motion interpolation[64] - Pixel-art scaling algorithms[65] - Image scaling[66] - Image restoration[67][68] - Photo colorization[69] - Film restoration and video upscaling[70] - Photo tagging - Text-to-image models such as DALL-E, Midjourney and Stable Diffusion - Image to video[71] - Text to video such as Make-A-Video from Meta, Imagen video and Phenaki from Google - Text to music with AI models such as MusicLM[72][73] - Text to speech such as ElevenLabs and 15.ai - Motion capture[74] Deep-fakes [edit]Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes. Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[75] In January 2016, the Horizon 2020 program financed the InVID Project to help journalists and researchers detect fake documents, made available as browser plugins.[76][77] In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[78] a program that animates photographs of faces, mimicking the facial expressions of another person. In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[79] In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[80] DARPA gave 68 million dollars to work on deep-fake detection.[80] Audio deepfakes[81][82] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[83][84] Video surveillance analysis and manipulated media detection [edit]Artificial intelligence for video surveillance utilizes computer software programs that analyze the audio and images from video surveillance cameras in order to recognize humans, vehicles, objects and events. Security contractors program is the software to define restricted areas within the camera's view (such as a fenced off area, a parking lot but not the sidewalk or public street outside the lot) and program for times of day (such as after the close of business) for the property being protected by the camera surveillance. The artificial intelligence (\"A.I.\") sends an alert if it detects a trespasser breaking the \"rule\" set that no person is allowed in that area during that time of day. AI algorithms have been used to detect deepfake videos.[85][86] Video production [edit]Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[87] Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[87] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[87] Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[88] Music [edit]AI has been used to compose music of various genres. David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[89] The algorithm behind Emily Howell is registered as a US patent.[90] In 2012, AI Iamus created the first complete classical album.[91] AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[92] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[93] Melomics creates computer-generated music for stress and pain relief.[94] The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[95] and musicians such as Taryn Southern[96] collaborated with the project to create music. South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[97] Writing and reporting [edit]Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[98] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[99] Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[100] TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[101] While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[102] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[103] South Korean company Hanteo Global uses a journalism bot to write articles.[104] Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017–2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications. Sports writing [edit]In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[105] After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[106] UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[106] El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[106] A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[106] Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[107] Wikipedia [edit]| Part of a series on | | Artificial intelligence (AI) | |---| Artificial intelligence and machine learning have long been used in Wikimedia projects, primarily to help edit existing articles.[108] Some applications of artificial intelligence, like using large language models to create new articles from scratch, have been more controversial than others for the Wikipedia community. In August 2025, Wikipedia adopted a policy that allowed editors to nominate suspected AI-generated articles for speedy deletion. Wikipedia has also been a significant source of training data for some of the earliest artificial intelligence projects. This has received mixed reactions including concern about companies not citing Wikipedia when relying on it to answer a question as well as Wikipedia’s increased costs from data scraping. Millions of its articles have been edited by bots[109] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[110] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[111] detecting covert vandalism[112] or recommending articles and tasks to new editors. Machine translation [113][114] has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.Video games [edit]In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[115][116] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[117] Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[118] Go (AlphaGo),[119][120][121][122][123][124][125][excessive citations] poker (Pluribus[126] and Cepheus),[127] E-sports (StarCraft),[128][129] and general game playing (AlphaZero[130][131][132] and MuZero).[133][134][135][136][excessive citations] Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[137][138] Visual images [edit]The first AI art program, called AARON, was developed by Harold Cohen in 1968[139] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[140] AI platforms such as DALL-E,[141] Stable Diffusion,[141] Imagen,[142] and Midjourney[143] have been used for generating visual images from inputs such as text or other images.[144] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches. AI has been used to generate quantitative analysis of existing digital art collections.[145] Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[146] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork. Computer animation [edit]In 2023, Netflix of Japan's usage of AI to generate background images for short The Dog & the Boy was met with backlash online.[147] Finance [edit]Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[148] Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[149] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[150][151][152] The use of AI in applications such as online trading and decision-making has changed major economic theories.[153] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[154] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[155] Trading and investment [edit]Algorithmic trading involves using AI systems to make trading decisions at speeds of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have AI-managed portfolios. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[156] Large financial institutions use AI to assist with their investment practices.[157] BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[158] Underwriting [edit]Online lender Upstart uses machine learning for underwriting.[159] ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting.[160] This platform uses machine learning to analyze data, including purchase transactions and how a customer fills out a form, to score borrowers. The platform is handy for assigning credit scores to those with limited credit histories.[161] Audit [edit]AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[162][quantify] Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision-making.[163] Anti–money laundering [edit]AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti–money laundering (AML).[164][165] History [edit]In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[166] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[167] One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[168] In the 1990s, AI was applied to fraud detection. In 1993, FinCEN Artificial Intelligence System (FAIS) was launched. It was able to review over 200,000 transactions per week, and over two years, it helped identify 400 potential cases of money laundering equal to $1 billion.[169] These expert systems were later replaced by machine learning systems.[170] Outside finance, the late 1980s and early 1990s also saw expert systems used in technical and environmental domains. For example, researchers built a fishway design advisor to recommend fish passage structures under varying hydraulic and biological conditions using the VP-Expert shell.[171] Transportation researchers applied the same shell to balance airport capacity with noise-mitigation plans.[172] In agriculture, a potato insect expert system (PIES) supported pest management decisions for Colorado potato beetle.[173] The U.S. Environmental Protection Agency's CORMIX system for modeling pollutant discharges combined rules with Fortran hydrodynamic models.[174] AI can enhance entrepreneurial activity, and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[175] Regulatory developments in the EU [edit]In the European Union, the Artificial Intelligence Act (Regulation (EU) 2024/1689) classifies several finance‑sector uses of AI as \"high‑risk\", including systems used to evaluate the creditworthiness of natural persons or to establish a credit score and AI used for risk assessment and pricing in life or health insurance.[176][177][178] These systems must meet requirements on risk management, data governance, technical documentation and logging, transparency, and human oversight.[177][179] The Act's obligations are phased in: prohibitions and AI‑literacy rules apply from 2 February 2025, governance and most GPAI duties from 2 August 2025, the bulk of obligations from 2 August 2026, and certain safety‑component high‑risk obligations from 2 August 2027.[178] Health [edit]Healthcare [edit]AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[180] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[181] The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[182] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[183][184] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[185] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[186] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[187] Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[188] Artificial neural networks are used as clinical decision support systems for medical diagnosis,[189] such as in concept processing technology in EMR software. Other healthcare tasks thought suitable for an AI that are in development include: - Screening[190] - Companion robots for elder care[191] - Drug creation[192] (e.g. by identifying candidate drugs[193] and by using existing drug screening data such as in life extension research)[194] - Clinical training[195] - Identifying genomic pathogen signatures of novel pathogens[196] or identifying pathogens via physics-based fingerprints[197] (including pandemic pathogens) - Helping link genes to their functions,[198] otherwise analyzing genes[199] and identification of novel biological targets[200] - Help development of biomarkers[200] - Help tailor therapies to individuals in personalized medicine/precision medicine[200][201] Workplace health and safety [edit]AI-enabled chatbots decrease the need for humans to perform basic call center tasks, and machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[202] Decision support systems can potentially prevent industrial disasters and make disaster response more efficient.[203] For manual workers in material handling, predictive analytics has been proposed to reduce musculoskeletal injury.[204] AI can attempt to process workers' compensation claims.[205][206] AI has been proposed for detection of accident near misses, which are underreported.[207] Biochemistry [edit]Machine learning has been used for drug design,[42] drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials.[208] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[209] has been used in drug-syntheses, and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[210] It has also been used to explore the origins of life on Earth.[211] Deep learning has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[212] The AI program AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[213][214][215][216][excessive citations] Language processing [edit]Language translation [edit]Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[217] AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[218] Additionally, research and development are in progress to decode and conduct animal communication.[6][219] Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical machine translation (SMT) and neural machine translations (NMTs). The old method of performing translation was to use statistical methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[220] Law and government [edit]Government [edit]AI facial recognition systems are used for mass surveillance, notably in China.[221][222] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[223] Law [edit]Legal analysis [edit]AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[224] While its use is common, it is not expected to replace most work done by lawyers in the near future.[225] The electronic discovery industry uses machine learning to reduce manual searching.[226] Law enforcement and legal proceedings [edit]Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants.[227] COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[228] One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[229] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[228] In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[230]: 124 Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[230]: 124 Manufacturing [edit]Sensors [edit]Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[231][232] enable applications such as at-home water quality monitoring. Toys and games [edit]In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy. Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[233] Oil and gas [edit]Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[234][235] Mathematics [edit]AI tools have been used to translate mathematical proofs into formal proofs in order to automatically verify them.[236] Automated theorem proving [edit]Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major motivating factor for the development of computer science. Computational geometry [edit]AlphaGeometry is an artificial intelligence (AI) program that can solve hard problems in Euclidean geometry. The system comprises a data-driven large language model (LLM) and a rule-based symbolic engine (Deductive Database Arithmetic Reasoning). It was developed by DeepMind, a subsidiary of Google. The program solved 25 geometry problems out of 30 from the International Mathematical Olympiad (IMO) under competition time limits—a performance almost as good as the average human gold medallist. For comparison, the previous AI program, called Wu's method, managed to solve only 10 problems.[237][238] DeepMind published a paper about AlphaGeometry in the peer-reviewed journal Nature on 17 January 2024.[239] AlphaGeometry was featured in MIT Technology Review on the same day.[240] Traditional geometry programs are symbolic engines that rely exclusively on human-coded rules to generate rigorous proofs, which makes them lack flexibility in unusual situations. AlphaGeometry combines such a symbolic engine with a specialized large language model trained on synthetic data of geometrical proofs. When the symbolic engine doesn't manage to find a formal and rigorous proof on its own, it solicits the large language model, which suggests a geometrical construct to move forward. However, it is unclear how applicable this method is to other domains of mathematics or reasoning, because symbolic engines rely on domain-specific rules and because of the need for synthetic data.[241] Military [edit]Various countries are deploying AI military applications.[242] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[242] AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[242][243][244][245][excessive citations] Internet and e-commerce [edit]Procurement and Sourcing [edit]AI has been used in B2B sourcing and procurement.[246] Academic literature and industry reports point out uses of AI for supplier evaluation and selection, predictive demand forecasting, automated contract and invoice processing, and supply-chain risk assessment. Machine learning, natural language processing, and robotic process automation are commonly used in procurement.[247] E-commerce companies that have AI applications include Alibaba.com, Amazon Business, Shopify Plus, and TikTok Shop.[248] [249][250][251] Examples of AI being used in e-commerce include product sourcing engines and business research with tools like Accio[252] and Amazon Business Assistant for both Alibaba.com and Amazon[253], respectively. Web feeds and posts [edit]Machine learning has been used for recommendation systems in determining which posts should show up in social media feeds.[254][255] Various types of social media analysis also make use of machine learning[256][257] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[258][259][260] AI has been used to customize shopping options and personalize offers.[261] Online gambling companies have used AI for targeting gamblers.[262] Virtual assistants and search [edit]Intelligent personal assistants use AI to attempt to respond to natural language requests. Siri, released in 2010 for Apple smartphones, popularized the concept.[263] Bing Chat has used artificial intelligence as part of its search engine.[264] Spam filtering [edit]Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[265] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[266] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[267] Facial recognition and image labeling [edit]AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[268] China has used facial recognition and artificial intelligence technology in Xinjiang. In 2017, reporters visiting the region found surveillance cameras installed every hundred meters or so in several cities, as well as facial recognition checkpoints at areas like gas stations, shopping centers, and mosque entrances.[269][270] Human rights groups have criticized the Chinese government for using artificial intelligence facial recognition technology for use in political suppression.[271][272] The Netherlands has deployed facial recognition and artificial intelligence technology since 2016.[273] The database of the Dutch police currently contains over 2.2 million pictures of 1.3 million Dutch citizens. This accounts for about 8% of the population. In The Netherlands, face recognition is not used by the police on municipal CCTV.[274] Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[218] Scientific research [edit]Evidence of general impacts [edit]In April 2024, the Scientific Advice Mechanism to the European Commission published advice[275] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research. As benefits, the evidence review[276] highlighted: - its role in accelerating research and innovation - its capacity to automate workflows - enhancing dissemination of scientific work As challenges: - limitations and risks around transparency, reproducibility and interpretability - poor performance (inaccuracy) - risk of harm through misuse or unintended use - societal concerns including the spread of misinformation and increasing inequalities Archaeology, history and imaging of sites [edit]Machine learning can help to restore and attribute ancient texts.[277] It can help to index texts for example to enable better and easier searching and classification of fragments.[278] Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[279] It can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[280] Physics [edit]A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[281][282] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[283][284] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[283] Materials science [edit]In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that the AI system GNoME had documented over 2 million new materials. GNoME uses deep learning techniques to examine potential material structures, and identify stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, with a success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database.[285][286][287] Reverse engineering [edit]Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[288] and for quickly understanding the behavior of malware.[289][290][291] It can be used to reverse engineer artificial intelligence models.[292] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[293] or protein design for pre-specified functional sites.[294][295] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[296] Astronomy, space activities and ufology [edit]Artificial intelligence is used in astronomy to analyze increasing amounts of available data[297][298] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[299] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[300] and more autonomous operation.[301][302][47][298] In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[303][304] – such as real-time observations[305] – and other technosignatures, e.g. via anomaly detection.[306] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[307] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[308][309][310][311][312][excessive citations] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[313][314] Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[315] Chemistry and biology [edit]There is research about which types of computer-aided chemistry would benefit from machine learning.[316] A deep learning AI-based process has been developed that uses genome databases to design novel proteins based on evolutionary algorithms.[317][318] Machine learning has also been used for protein design with pre-specified functional sites,[294][295] predicting molecular properties, and exploring large chemical/reaction spaces.[319] Using drug discovery AI algorithms, researchers generated 40,000 potential chemical weapon candidates, helping in the regulation of such chemicals to prevent synthesizing them for real harm.[320][321][322] There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[323] or identifying functional DNA motifs.[324] It is widely used in genetic research.[325] There also is some use of machine learning in synthetic biology,[326][327] disease biology,[327] nanotechnology (e.g. nanostructured materials and bionanotechnology),[328][329] and materials science.[330][331][332] Security and surveillance [edit]Cyber security [edit]Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[333] Applications of AI in cyber security include: - Network protection: Machine learning improves intrusion detection systems by broadening the search beyond previously identified threats.[334] - Endpoint protection: Attacks such as ransomware can be thwarted by learning typical malware behaviors. - AI-related cyber security application cases vary in both benefit and complexity. Security features such as Security Orchestration, Automation, and Response (SOAR) and Extended Endpoint Detection and Response (XDR) offer significant benefits for businesses, but require significant integration and adaptation efforts.[335] - Application security: can help counterattacks such as server-side request forgery, SQL injection, cross-site scripting, and distributed denial-of-service. - AI technology can also be utilized to improve system security and safeguard our privacy. Randrianasolo (2012) suggested a security system based on artificial intelligence that can recognize intrusions and adapt to perform better.[336] In order to improve cloud computing security, Sahil (2015) created a user profile system for the cloud environment with AI techniques.[337] - Suspect user behavior: Machine learning can identify fraud or compromised applications as they occur.[338] Transportation and logistics [edit]Aviation [edit]The use of artificial intelligence is being observed in the aviation industry for predictive maintenance. This has been helpful in reducing delays. Companies such as Boeing and Airbus are using AI for optimizing repair procedures while maintaining operational performance[339]. Automotive and public transit [edit]Transportation's complexity means that in most cases, training an AI in a real-world driving environment is impractical, and is achieved through simulator-based testing.[340] AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[341] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[342][343] There are prototypes of autonomous automotive public transport vehicles such as autonomous rail transport in operation,[344][345][346] electric mini-buses,[347][348][349] and autonomous delivery vehicles,[350][351][343] including delivery robots.[352][353] Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[354] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[355] AI has been used to optimize traffic management, which can reduce wait times, energy use, and emissions.[356] Military [edit]Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated. AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[357] AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers. Speech recognition allows traffic controllers to give verbal directions to drones. Artificial intelligence supported design of aircraft,[358] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective. NASA [edit]In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[359] The software compensated for damaged components by relying on the remaining undamaged components.[360] The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[361] Maritime [edit]Neural networks are used by situational awareness systems in ships and boats.[362] There also are autonomous boats. See also [edit]- Applications of artificial intelligence to legal informatics - Applications of deep learning - Applications of machine learning - Artificial intelligence and elections - Collective intelligence § Applications - List of artificial intelligence projects - List of datasets for machine-learning research - Open data - Progress in artificial intelligence - Timeline of computing 2020–present Footnotes [edit]- ^ Brynjolfsson, Erik; Mitchell, Tom (22 December 2017). \"What can machine learning do? Workforce implications\". Science. 358 (6370): 1530–1534. Bibcode:2017Sci...358.1530B. doi:10.1126/science.aap8062. PMID 29269459. - ^ Shin, Minkyu; Kim, Jin; van Opheusden, Bas; Griffiths, Thomas L. (2023). \"Superhuman artificial intelligence can improve human decision-making by increasing novelty\". Proceedings of the National Academy of Sciences. 120 (12) e2214840120. arXiv:2303.07462. Bibcode:2023PNAS..12014840S. doi:10.1073/pnas.2214840120. PMC 10041097. PMID 36913582. - ^ Chen, Yiting; Liu, Tracy Xiao; Shan, You; Zhong, Songfa (2023). \"The emergence of economic rationality of GPT\". Proceedings of the National Academy of Sciences. 120 (51) e2316205120. arXiv:2305.12763. Bibcode:2023PNAS..12016205C. doi:10.1073/pnas.2316205120. PMC 10740389. PMID 38085780. - ^ \"What is Generative AI? | IBM\". www.ibm.com. 2024-03-22. Retrieved 2025-07-22. - ^ Gambhire, Akshaya; Shaikh Mohammad, Bilal N. (8 April 2020). Use of Artificial Intelligence in Agriculture. Proceedings of the 3rd International Conference on Advances in Science & Technology (ICAST) 2020. SSRN 3571733. - ^ a b Briefer, Elodie F.; Sypherd, Ciara C.-R.; Linhart, Pavel; Leliveld, Lisette M. C.; Padilla de la Torre, Monica; Read, Eva R.; Guérin, Carole; Deiss, Véronique; Monestier, Chloé; Rasmussen, Jeppe H.; Špinka, Marek; Düpjan, Sandra; Boissy, Alain; Janczak, Andrew M.; Hillmann, Edna; Tallet, Céline (7 March 2022). \"Classification of pig calls produced from birth to slaughter according to their emotional valence and context of production\". Scientific Reports. 12 (1): 3409. Bibcode:2022NatSR..12.3409B. doi:10.1038/s41598-022-07174-8. PMC 8901661. PMID 35256620. - ^ Moreno Millán, M; Sevilla Guzmán, E; Demyda, S E (2011). \"Population, Poverty, Production, Food Security, Food Sovereignty, Biotechnology and Sustainable Development: Challenges for the XXI Century\". Bulletin of University of Agricultural Sciences and Veterinary Medicine Cluj-Napoca. Veterinary Medicine. 1 (68). - ^ Liundi, Nicholas; Darma, Aditya Wirya; Gunarso, Rivaldi; Warnars, Harco Leslie Hendric Spits (2019). \"Improving Rice Productivity in Indonesia with Artificial Intelligence\". 2019 7th International Conference on Cyber and IT Service Management (CITSM). pp. 1–5. doi:10.1109/CITSM47753.2019.8965385. ISBN 978-1-7281-2909-9. - ^ Talaviya, Tanha; Shah, Dhara; Patel, Nivedita; Yagnik, Hiteshri; Shah, Manan (2020). \"Implementation of artificial intelligence in agriculture for optimisation of irrigation and application of pesticides and herbicides\". Artificial Intelligence in Agriculture. 4: 58–73. doi:10.1016/j.aiia.2020.04.002. - ^ Bernstein, Phillip (2022). Machine Learning: Architecture in the Age of Artificial Intelligence. London: RIBA Publishing. ISBN 978-1-914124-01-3. - ^ Heathcote, Edwin (20 January 2024). \"AI is coming for architecture\". Financial Times. Retrieved 2024-02-07. - ^ \"Will Artificial Intelligence Replace Architects?\". ArchDaily. 2023-10-18. Retrieved 2024-02-07. - ^ Brynjolfsson, Erik; Li, Danielle; Raymond, Lindsey (2025-02-04). \"Generative AI at Work\". The Quarterly Journal of Economics. 140 (2): 889–942. doi:10.1093/qje/qjae044. ISSN 0033-5533. Archived from the original on 2025-06-05. - ^ Noy, Shakked; Zhang, Whitney (2023-07-14). \"Experimental evidence on the productivity effects of generative artificial intelligence\". Science. 381 (6654): 187–192. Bibcode:2023Sci...381..187N. doi:10.1126/science.adh2586. PMID 37440646. - ^ Estrada, Sheryl (18 August 2025). \"MIT report: 95% of generative AI pilots at companies are failing\". Fortune. Retrieved 15 October 2025. - ^ Niederhoffer, Kate; Kellerman, Gabriella Rosen; Lee, Angela; Liebscher, Alex; Rapuano, Kristina; Hancock, Jeffrey T. (22 September 2025). \"AI-Generated \"Workslop\" Is Destroying Productivity\". Harvard Business Review. Retrieved 15 October 2025. - ^ Frąckowiak-Szymański, Przemysław (26 June 2025). \"The Pros and Cons of Vibe Coding\". Software Mind. Retrieved 7 November 2025. - ^ a b Nickelsburg, Monica (1 October 2025). \"The human coders hired to mop up AI slop\". www.kuow.org. NPR. Retrieved 25 October 2025. - ^ Davis, Dominic-Madori (14 September 2025). \"Vibe coding has turned senior devs into 'AI babysitters,' but they say it's worth it\". TechCrunch. Retrieved 25 October 2025. - ^ Newman, Lily Hay. \"Vibe Coding Is the New Open Source—in the Worst Way Possible\". Wired. Retrieved 25 October 2025. - ^ Tangermann, Victor (31 May 2025). \"Companies Are Discovering a Grim Problem With \"Vibe Coding\"\". Futurism. Retrieved 25 October 2025. - ^ \"Google AI creates its own \"child\" bot\". The Independent. 5 December 2017. Retrieved 5 February 2018. - ^ Spagnolo, Michele; Morris, Joshua; Piacentini, Simone; Antesberger, Michael; Massa, Francesco; Crespi, Andrea; Ceccarelli, Francesco; Osellame, Roberto; Walther, Philip (April 2022). \"Experimental photonic quantum memristor\". Nature Photonics. 16 (4): 318–323. arXiv:2105.04867. Bibcode:2022NaPho..16..318S. doi:10.1038/s41566-022-00973-5. - ^ Ramanathan, Shriram (July 2018). \"Quantum materials for brain sciences and artificial intelligence\". MRS Bulletin. 43 (7): 534–540. Bibcode:2018MRSBu..43..534R. doi:10.1557/mrs.2018.147. - ^ \"Artificial intelligence makes accurate quantum chemical simulations more affordable\". Nature Portfolio Chemistry Community. 2 December 2021. Retrieved 30 May 2022. - ^ Guan, Wen; Perdue, Gabriel; Pesah, Arthur; Schuld, Maria; Terashi, Koji; Vallecorsa, Sofia; Vlimant, Jean-Roch (March 2021). \"Quantum machine learning in high energy physics\". Machine Learning: Science and Technology. 2 (1): 011003. arXiv:2005.08582. doi:10.1088/2632-2153/abc17d. - ^ Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 - ^ a b Kongthon, Alisa; Sangkeettrakarn, Chatchawal; Kongyoung, Sarawoot; Haruechaiyasak, Choochart (2009). \"Implementing an online help desk system based on conversational agent\". Proceedings of the International Conference on Management of Emergent Digital EcoSystems. pp. 450–451. doi:10.1145/1643823.1643908. ISBN 978-1-60558-829-2. - ^ Sara Ashley O'Brien (12 January 2016). \"Is this app the call center of the future?\". CNN. Retrieved 26 September 2016. - ^ \"Using Google AI to convert speech to text\". Google Cloud. Retrieved 2025-09-07. - ^ Clark, Jack (20 July 2016). \"New Google AI Brings Automation to Customer Service\". Bloomberg.com. - ^ \"Amazon.com tests customer service chatbots\". Amazon Science. 25 February 2020. Retrieved 23 April 2021. - ^ Malatya Turgut Ozal University, Malatya, Turkey; Isguzar, Seda; Fendoglu, Eda; Malatya Turgut Ozal University, Malatya, Turkey; SimSek, Ahmed Ihsan (May 2024). \"Innovative Applications in Businesses: An Evaluation on Generative Artificial Intelligence\" (PDF). Amfiteatru Economic. 26 (66): 511. doi:10.24818/EA/2024/66/511. Retrieved 13 June 2024. {{cite journal}} : CS1 maint: multiple names: authors list (link) - ^ \"Advanced analytics in hospitality\". McKinsey & Company. 2017. Retrieved 14 January 2020. - ^ Zlatanov, Sonja; Popesku, Jovan (2019). \"Current Applications of Artificial Intelligence in Tourism and Hospitality\". Proceedings of the International Scientific Conference - Sinteza 2019. pp. 84–90. doi:10.15308/Sinteza-2019-84-90. ISBN 978-86-7912-703-7. - ^ \"The promises and perils of new technologies to improve education and employment opportunities\". Brookings. Retrieved 2024-04-20. - ^ \"Role of AI in Energy\". DOE. - ^ Bourhnane, Safae; Abid, Mohamed Riduan; Lghoul, Rachid; Zine-Dine, Khalid; Elkamoun, Najib; Benhaddou, Driss (30 January 2020). \"Machine learning for energy consumption prediction and scheduling in smart buildings\". SN Applied Sciences. 2 (2): 297. doi:10.1007/s42452-020-2024-9. - ^ Kanwal, Sidra; Khan, Bilal; Muhammad Ali, Sahibzada (February 2021). \"Machine learning based weighted scheduling scheme for active power control of hybrid microgrid\". International Journal of Electrical Power & Energy Systems. 125 106461. Bibcode:2021IJEPE.12506461K. doi:10.1016/j.ijepes.2020.106461. - ^ Mohanty, Prasanta Kumar; Jena, Premalata; Padhy, Narayana Prasad (2020). \"Home Electric Vehicle Charge Scheduling Using Machine Learning Technique\". 2020 IEEE International Conference on Power Systems Technology (POWERCON). pp. 1–5. doi:10.1109/POWERCON48463.2020.9230627. ISBN 978-1-7281-6350-5. - ^ Foster, Isabella (15 March 2021). \"Making Smart Grids Smarter with Machine Learning\". EIT | Engineering Institute of Technology. Retrieved 3 July 2022. - ^ a b Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI. Intellisemantic Editions. p. 211. ISBN 978-88-947876-0-3. - ^ Williams, Ben; Lamont, Timothy A. C.; Chapuis, Lucille; Harding, Harry R.; May, Eleanor B.; Prasetya, Mochyudho E.; Seraphim, Marie J.; Jompa, Jamaluddin; Smith, David J.; Janetski, Noel; Radford, Andrew N.; Simpson, Stephen D. (July 2022). \"Enhancing automated analysis of marine soundscapes using ecoacoustic indices and machine learning\". Ecological Indicators. 140 108986. Bibcode:2022EcInd.14008986W. doi:10.1016/j.ecolind.2022.108986. hdl:10871/129693. - ^ Hino, M.; Benami, E.; Brooks, N. (October 2018). \"Machine learning for environmental monitoring\". Nature Sustainability. 1 (10): 583–588. Bibcode:2018NatSu...1..583H. doi:10.1038/s41893-018-0142-9. - ^ \"How machine learning can help environmental regulators\". Stanford News. Stanford University. 8 April 2019. Retrieved 29 May 2022. - ^ \"AI empowers environmental regulators\". Stanford News. Stanford University. 19 April 2021. Retrieved 29 May 2022. - ^ a b \"Artificial intelligence in space\". www.esa.int. Retrieved 30 May 2022. - ^ Frost, Rosie (9 May 2022). \"Plastic waste can now be found and monitored from space\". euronews. Retrieved 24 June 2022. - ^ \"Global Plastic Watch\". www.globalplasticwatch.org. Retrieved 24 June 2022. - ^ \"AI may predict the next virus to jump from animals to humans\". Public Library of Science. Retrieved 19 October 2021. - ^ Mollentze, Nardus; Babayan, Simon A.; Streicker, Daniel G. (28 September 2021). \"Identifying and prioritizing potential human-infecting viruses from their genome sequences\". PLOS Biology. 19 (9) e3001390. doi:10.1371/journal.pbio.3001390. PMC 8478193. PMID 34582436. - ^ Li, Zefeng; Meier, Men-Andrin; Hauksson, Egill; Zhan, Zhongwen; Andrews, Jennifer (28 May 2018). \"Machine Learning Seismic Wave Discrimination: Application to Earthquake Early Warning\". Geophysical Research Letters. 45 (10): 4773–4779. Bibcode:2018GeoRL..45.4773L. doi:10.1029/2018GL077870. - ^ \"Machine learning and gravity signals could rapidly detect big earthquakes\". Science News. 11 May 2022. Retrieved 3 July 2022. - ^ Fauvel, Kevin; Balouek-Thomert, Daniel; Melgar, Diego; Silva, Pedro; Simonet, Anthony; Antoniu, Gabriel; Costan, Alexandru; Masson, Véronique; Parashar, Manish; Rodero, Ivan; Termier, Alexandre (3 April 2020). \"A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning\". Proceedings of the AAAI Conference on Artificial Intelligence. 34 (1): 403–411. doi:10.1609/aaai.v34i01.5376. - ^ Thirugnanam, Hemalatha; Ramesh, Maneesha Vinodini; Rangan, Venkat P. (September 2020). \"Enhancing the reliability of landslide early warning systems by machine learning\". Landslides. 17 (9): 2231–2246. Bibcode:2020Lands..17.2231T. doi:10.1007/s10346-020-01453-z. - ^ Moon, Seung-Hyun; Kim, Yong-Hyuk; Lee, Yong Hee; Moon, Byung-Ro (2019). \"Application of machine learning to an early warning system for very short-term heavy rainfall\". Journal of Hydrology. 568: 1042–1054. Bibcode:2019JHyd..568.1042M. doi:10.1016/j.jhydrol.2018.11.060. - ^ Robinson, Bethany; Cohen, Jonathan S.; Herman, Jonathan D. (September 2020). \"Detecting early warning signals of long-term water supply vulnerability using machine learning\". Environmental Modelling & Software. 131 104781. Bibcode:2020EnvMS.13104781R. doi:10.1016/j.envsoft.2020.104781. - ^ Bury, Thomas M.; Sujith, R. I.; Pavithran, Induja; Scheffer, Marten; Lenton, Timothy M.; Anand, Madhur; Bauch, Chris T. (28 September 2021). \"Deep learning for early warning signals of tipping points\". Proceedings of the National Academy of Sciences. 118 (39) e2106140118. Bibcode:2021PNAS..11806140B. doi:10.1073/pnas.2106140118. PMC 8488604. PMID 34544867. - ^ Park, Yongeun; Lee, Han Kyu; Shin, Jae-Ki; Chon, Kangmin; Kim, SungHwan; Cho, Kyung Hwa; Kim, Jin Hwi; Baek, Sang-Soo (15 June 2021). \"A machine learning approach for early warning of cyanobacterial bloom outbreaks in a freshwater reservoir\". Journal of Environmental Management. 288 112415. Bibcode:2021JEnvM.28812415P. doi:10.1016/j.jenvman.2021.112415. PMID 33774562. - ^ Li, Jun; Wang, Zhaoli; Wu, Xushu; Xu, Chong-Yu; Guo, Shenglian; Chen, Xiaohong; Zhang, Zhenxing (August 2021). \"Robust Meteorological Drought Prediction Using Antecedent SST Fluctuations and Machine Learning\". Water Resources Research. 57 (8) e2020WR029413. Bibcode:2021WRR....5729413L. doi:10.1029/2020WR029413. hdl:10852/92935. - ^ Khan, Najeebullah; Sachindra, D. A.; Shahid, Shamsuddin; Ahmed, Kamal; Shiru, Mohammed Sanusi; Nawaz, Nadeem (May 2020). \"Prediction of droughts over Pakistan using machine learning algorithms\". Advances in Water Resources. 139 103562. Bibcode:2020AdWR..13903562K. doi:10.1016/j.advwatres.2020.103562. - ^ Kaur, Amandeep; Sood, Sandeep K. (May 2020). \"Deep learning based drought assessment and prediction framework\". Ecological Informatics. 57 101067. Bibcode:2020EcInf..5701067K. doi:10.1016/j.ecoinf.2020.101067. - ^ Preparing for the future of artificial intelligence. National Science and Technology Council. p. 14. OCLC 965620122. Retrieved 7 December 2024. - ^ \"Research at NVIDIA: Transforming Standard Video Into Slow Motion with AI\". 18 June 2018. Archived from the original on 21 December 2021 – via YouTube. - ^ \"Artificial intelligence is helping old video games look like new\". The Verge. 18 April 2019. - ^ \"Review: Topaz Sharpen AI is Amazing\". petapixel.com. 4 March 2019. - ^ Griffin, Matthew (26 April 2018). \"AI can now restore your corrupted photos to their original condition\". - ^ \"NVIDIA's AI can fix bad photos by looking at other bad photos\". Engadget. 10 July 2018. - ^ \"Using AI to Colorize and Upscale a 109-Year-Old Video of New York City to 4K and 60fps\". petapixel.com. 24 February 2020. - ^ \"YouTubers are upscaling the past to 4K. Historians want them to stop\". Wired UK. - ^ \"Google's DeepMind AI can 'transframe' a single image into a video\". 18 August 2022. - ^ \"Google's new AI turns text into music\". 28 January 2023. - ^ \"Google's new AI music generator can create - and hold - a tune\". 30 January 2023. - ^ \"CSDL | IEEE Computer Society\". - ^ Jodka, Sara (February 1, 2024). \"Manipulating reality: the intersection of deepfakes and the law\". Reuters.com. Retrieved December 8, 2024. - ^ Teyssou, Denis (2019). \"Applying Design Thinking Methodology: The InVID Verification Plugin\". Video Verification in the Fake News Era. pp. 263–279. doi:10.1007/978-3-030-26752-0_9. ISBN 978-3-030-26751-3. - ^ \"Fake news debunker by InVID & WeVerify\". Retrieved 23 December 2021. - ^ \"TUM Visual Computing & Artificial Intelligence: Prof. Matthias Nießner\". niessnerlab.org. - ^ \"Will \"Deepfakes\" Disrupt the Midterm Election?\". Wired. November 2018. - ^ a b Afchar, Darius; Nozick, Vincent; Yamagishi, Junichi; Echizen, Isao (2018). \"MesoNet: A Compact Facial Video Forgery Detection Network\". 2018 IEEE International Workshop on Information Forensics and Security (WIFS). pp. 1–7. arXiv:1809.00888. doi:10.1109/WIFS.2018.8630761. ISBN 978-1-5386-6536-7. - ^ Lyons, Kim (29 January 2020). \"FTC says the tech behind audio deepfakes is getting better\". The Verge. - ^ \"Audio samples from \"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\"\". google.github.io. - ^ Strickland, Eliza (11 December 2019). \"Facebook AI Launches Its Deepfake Detection Challenge\". IEEE Spectrum. - ^ \"Contributing Data to Deepfake Detection Research\". ai.googleblog.com. 24 September 2019. - ^ Ober, Holly. \"New method detects deepfake videos with up to 99% accuracy\". University of California-Riverside. Retrieved 3 July 2022. - ^ \"AI algorithm detects deepfake videos with high accuracy\". techxplore.com. Retrieved 3 July 2022. - ^ a b c \"Welcome to the new surreal. How AI-generated video is changing film\". MIT Technology Review. Retrieved 2023-12-05. - ^ Bean, Thomas H. Davenport and Randy (2023-06-19). \"The Impact of Generative AI on Hollywood and Entertainment\". MIT Sloan Management Review. Retrieved 2023-12-05. - ^ Cheng, Jacqui (30 September 2009). \"Virtual composer makes beautiful music—and stirs controversy\". Ars Technica. - ^ US patent 7696426 - ^ \"Computer composer honours Turing's centenary\". New Scientist. 4 July 2012. Archived from the original on 2016-04-13. Retrieved 27 December 2021. - ^ Hick, Thierry (11 October 2016). \"La musique classique recomposée\". Luxemburger Wort. - ^ \"Résultats de recherche - La Sacem\". repertoire.sacem.fr. - ^ Requena, Gloria; Sánchez, Carlos; Corzo-Higueras, José Luis; Reyes-Alvarado, Sirenia; Rivas-Ruiz, Francisco; Vico, Francisco; Raglio, Alfredo (2014). \"Melomics music medicine (M3) to lessen pain perception during pediatric prick test procedure\". Pediatric Allergy and Immunology. 25 (7): 721–724. doi:10.1111/pai.12263. PMID 25115240. - ^ \"Watson Beat on GitHub\". GitHub. 10 October 2018. - ^ \"Songs in the Key of AI\". Wired. 17 May 2018. - ^ \"Hayeon, sister of Girls' Generation's Taeyeon, debuts with song made by AI\". koreajoongangdaily.joins.com. 7 October 2020. Retrieved 23 October 2020. - ^ business intelligence solutions Archived 3 November 2011 at the Wayback Machine. Narrative Science. Retrieved 21 July 2013. - ^ Eule, Alexander. \"Big Data and Yahoo's Quest for Mass Personalization\". Barron's. - ^ \"Artificial Intelligence Software that Writes like a Human Being\". Archived from the original on 12 April 2013. Retrieved 11 March 2013. - ^ Riedl, Mark Owen; Bulitko, Vadim (6 December 2012). \"Interactive Narrative: An Intelligent Systems Approach\". AI Magazine. 34 (1): 67. doi:10.1609/aimag.v34i1.2449. - ^ Callaway, Charles B.; Lester, James C. (August 2002). \"Narrative prose generation\". Artificial Intelligence. 139 (2): 213–252. doi:10.1016/S0004-3702(02)00230-8. - ^ \"A Japanese AI program just wrote a short novel, and it almost won a literary prize\". Digital Trends. 23 March 2016. Retrieved 18 November 2016. - ^ \"Bot News\". Hanteo News. 20 October 2020. Retrieved 20 October 2020. - ^ Canavilhas, João (September 2022). \"Artificial Intelligence and Journalism: Current Situation and Expectations in the Portuguese Sports Media\". Journalism and Media. 3 (3): 510–520. doi:10.3390/journalmedia3030035. hdl:10400.6/12308. - ^ a b c d Galily, Yair (August 2018). \"Artificial intelligence and sports journalism: Is it a sweeping change?\". Technology in Society. 54: 47–51. doi:10.1016/j.techsoc.2018.03.001. - ^ Wu, Daniel (2023-08-31). \"Gannett halts AI-written sports recaps after readers mocked the stories\". Washington Post. Retrieved 2023-10-31. - ^ Gertner, Jon (18 July 2023). \"Wikipedia's Moment of Truth\". New York Times. Retrieved 29 November 2024. - ^ \"Study reveals bot-on-bot editing wars raging on Wikipedia's pages\". The Guardian. 23 February 2017. Retrieved 10 January 2023. - ^ Cole, K. C. \"The Shaky Ground Truths of Wikipedia\". Wired. Retrieved 10 January 2023. - ^ \"AI can automatically rewrite outdated text in Wikipedia articles\". Engadget. Retrieved 10 January 2023. - ^ Metz, Cade. \"Wikipedia Deploys AI to Expand Its Ranks of Human Editors\". Wired. Retrieved 10 January 2023. - ^ \"Wikipedia taps Google to help editors translate articles\". VentureBeat. 9 January 2019. Retrieved 9 January 2023. - ^ Wilson, Kyle (8 May 2019). \"Wikipedia has a Google Translate problem\". The Verge. Retrieved 9 January 2023. - ^ \"Why AI researchers like video games\". The Economist. Archived from the original on 5 October 2017. - ^ Yannakakis, Geogios N. (2012). \"Game AI revisited\". Proceedings of the 9th conference on Computing Frontiers - CF '12. p. 285. doi:10.1145/2212908.2212954. ISBN 978-1-4503-1215-8. - ^ Maass, Laura E. Shummon (1 July 2019). \"Artificial Intelligence in Video Games\". Medium. Retrieved 23 April 2021. - ^ Markoff, John (16 February 2011). \"Computer Wins on 'Jeopardy!': Trivial, It's Not\". The New York Times. Archived from the original on 22 October 2014. Retrieved 25 October 2014. - ^ \"AlphaGo – Google DeepMind\". Archived from the original on 10 March 2016. - ^ \"Artificial intelligence: Google's AlphaGo beats Go master Lee Se-dol\". BBC News. 12 March 2016. Archived from the original on 26 August 2016. Retrieved 1 October 2016. - ^ Metz, Cade (27 May 2017). \"After Win in China, AlphaGo's Designers Explore New AI\". Wired. Archived from the original on 2 June 2017. - ^ \"World's Go Player Ratings\". May 2017. Archived from the original on 1 April 2017. - ^ \"柯洁迎19岁生日 雄踞人类世界排名第一已两年\" (in Chinese). May 2017. Archived from the original on 11 August 2017. - ^ \"MuZero: Mastering Go, chess, shogi and Atari without rules\". Deepmind. 23 December 2020. Retrieved 1 March 2021. - ^ Steven Borowiec; Tracey Lien (12 March 2016). \"AlphaGo beats human Go champ in milestone for artificial intelligence\". Los Angeles Times. Retrieved 13 March 2016. - ^ Solly, Meilan. \"This Poker-Playing A.I. Knows When to Hold 'Em and When to Fold 'Em\". Smithsonian. Pluribus has bested poker pros in a series of six-player no-limit Texas Hold'em games, reaching a milestone in artificial intelligence research. It is the first bot to beat humans in a complex multiplayer competition. - ^ Bowling, Michael; Burch, Neil; Johanson, Michael; Tammelin, Oskari (9 January 2015). \"Heads-up limit hold'em poker is solved\". Science. 347 (6218): 145–149. Bibcode:2015Sci...347..145B. doi:10.1126/science.1259433. PMID 25574016. - ^ Ontanon, Santiago; Synnaeve, Gabriel; Uriarte, Alberto; Richoux, Florian; Churchill, David; Preuss, Mike (December 2013). \"A Survey of Real-Time Strategy Game AI Research and Competition in StarCraft\". IEEE Transactions on Computational Intelligence and AI in Games. 5 (4): 293–311. doi:10.1109/TCIAIG.2013.2286295. - ^ \"Facebook Quietly Enters StarCraft War for AI Bots, and Loses\". WIRED. 2017. Retrieved 7 May 2018. - ^ Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (7 December 2018). \"A general reinforcement learning algorithm that masters chess, shogi, and go through self-play\". Science. 362 (6419): 1140–1144. Bibcode:2018Sci...362.1140S. doi:10.1126/science.aar6404. PMID 30523106. - ^ Sample, Ian (18 October 2017). \"'It's able to create knowledge itself': Google unveils AI that learns on its own\". The Guardian. Retrieved 7 May 2018. - ^ Appenzeller, Tim (7 July 2017). \"The AI revolution in science\". Science. doi:10.1126/science.aan7064. - ^ \"The superhero of artificial intelligence: can this genius keep it in check?\". The Guardian. 16 February 2016. Archived from the original on 23 April 2018. Retrieved 26 April 2018. - ^ Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Rusu, Andrei A.; Veness, Joel; Bellemare, Marc G.; Graves, Alex; Riedmiller, Martin; Fidjeland, Andreas K.; Ostrovski, Georg; Petersen, Stig; Beattie, Charles; Sadik, Amir; Antonoglou, Ioannis; King, Helen; Kumaran, Dharshan; Wierstra, Daan; Legg, Shane; Hassabis, Demis (26 February 2015). \"Human-level control through deep reinforcement learning\". Nature. 518 (7540): 529–533. Bibcode:2015Natur.518..529M. doi:10.1038/nature14236. PMID 25719670. - ^ Sample, Ian (14 March 2017). \"Google's DeepMind makes AI program that can learn like a human\". The Guardian. Archived from the original on 26 April 2018. Retrieved 26 April 2018. - ^ Schrittwieser, Julian; Antonoglou, Ioannis; Hubert, Thomas; Simonyan, Karen; Sifre, Laurent; Schmitt, Simon; Guez, Arthur; Lockhart, Edward; Hassabis, Demis; Graepel, Thore; Lillicrap, Timothy; Silver, David (24 December 2020). \"Mastering Atari, Go, chess and shogi by planning with a learned model\". Nature. 588 (7839): 604–609. arXiv:1911.08265. Bibcode:2020Natur.588..604S. doi:10.1038/s41586-020-03051-4. PMID 33361790. - ^ Ortiz, Sabrina. \"You can now chat with a famous AI character on Viber. Here's how\". zdnet.com. ZDNET. Retrieved 5 December 2024. ICONIQ created Kuki, an AI character whose sole purpose is to entertain humans and has even been used as a brand ambassador for H&M, modeled for Vogue, and starred in its own Roblox game. - ^ Lewis, Nell (19 August 2020). \"Robot friends: Why people talk to chatbots in times of trouble\". cnn.com. CNN. Retrieved 5 December 2024. Since 2016, when the bot landed on major messaging platforms, an estimated 5 million unique users hailing from all corners of the world have chatted with her. - ^ Poltronieri, Fabrizio Augusto; Hänska, Max (2019). \"Technical Images and Visual Art in the Era of Artificial Intelligence: From GOFAI to GANs\". Proceedings of the 9th International Conference on Digital and Interactive Arts. pp. 1–8. doi:10.1145/3359852.3359865. ISBN 978-1-4503-7250-3. - ^ \"Fine art print - crypto art\". Kate Vass Galerie. Retrieved 2022-05-07. - ^ a b \"Analysis | Is That Trump Photo Real? Free AI Tools Come With Risks\". Washington Post. Retrieved 30 August 2022. - ^ \"Google's image generator rivals DALL-E in shiba inu drawing\". TechCrunch. Retrieved 30 August 2022. - ^ \"Midjourney's enthralling AI art generator goes live for everyone\". PCWorld. - ^ \"After Photos, Here's How AI Made A Trippy Music Video Out Of Thin Air\". Fossbytes. 19 May 2022. Retrieved 30 May 2022. - ^ Cetinic, Eva; She, James (2022-02-16). \"Understanding and Creating Art with AI: Review and Outlook\". ACM Transactions on Multimedia Computing, Communications, and Applications. 18 (2): 66:1–66:22. arXiv:2102.09109. doi:10.1145/3475799. - ^ Lang, Sabine; Ommer, Bjorn (2018). \"Reflecting on How Artworks Are Processed and Analyzed by Computer Vision: Supplementary Material\". Proceedings of the European Conference on Computer Vision (ECCV) Workshops – via Computer Vision Foundation. - ^ Cole, Samantha (2023-02-01). \"Netflix Made an Anime Using AI Due to a 'Labor Shortage,' and Fans Are Pissed\". Vice. Retrieved 2023-12-04. - ^ Christy, Charles A. (17 January 1990). \"Impact of Artificial Intelligence on Banking\". Los Angeles Times. Retrieved 10 September 2019. - ^ O'Neill, Eleanor (31 July 2016). \"Accounting, automation and AI\". icas.com. Archived from the original on 18 November 2016. Retrieved 18 November 2016. - ^ \"CTO Corner: Artificial Intelligence Use in Financial Services – Financial Services Roundtable\". Financial Services Roundtable. 2 April 2015. Archived from the original on 18 November 2016. Retrieved 18 November 2016. - ^ \"Artificial Intelligence Solutions, AI Solutions\". sas.com. - ^ Chapman, Lizette (7 January 2019). \"Palantir once mocked the idea of salespeople. Now it's hiring them\". Los Angeles Times. Retrieved 28 February 2019. - ^ Artificial Intelligence and Economic Theory: Skynet in the Market. Advanced Information and Knowledge Processing. 2017. doi:10.1007/978-3-319-66104-9. ISBN 978-3-319-66103-2.[page needed] - ^ Marwala, Tshilidzi; Hurwitz, Evan (2017). \"Efficient Market Hypothesis\". Artificial Intelligence and Economic Theory: Skynet in the Market. Advanced Information and Knowledge Processing. pp. 101–110. doi:10.1007/978-3-319-66104-9_9. ISBN 978-3-319-66103-2. - ^ Shao, Jun; Lou, Zhukun; Wang, Chong; Mao, Jinye; Ye, Ailin (16 May 2022). \"The impact of artificial intelligence (AI) finance on financing constraints of non-SOE firms in emerging markets\". International Journal of Emerging Markets. 17 (4): 930–944. doi:10.1108/IJOEM-02-2021-0299. - ^ \"Algorithmic Trading\". Investopedia. 18 May 2005. - ^ \"The Financial Stability Implications of Artificial Intelligence\" (PDF). FSB. Retrieved 2025-09-07. - ^ \"Beyond Robo-Advisers: How AI Could Rewire Wealth Management\". 5 January 2017. - ^ Asatryan, Diana (3 April 2017). \"Machine Learning Is the Future of Underwriting, But Startups Won't be Driving It\". bankinnovation.net. Retrieved 15 April 2022. - ^ Laura, Blattner; Jann, Spiess. \"Explainability & Fairness in Machine Learning for Credit Underwriting\" (PDF). FinRegLab. Retrieved 2025-09-07. - ^ \"ZestFinance Introduces Machine Learning Platform to Underwrite Millennials and Other Consumers with Limited Credit History\" (Press release). 14 February 2017. - ^ Chang, Hsihui; Kao, Yi-Ching; Mashruwala, Raj; Sorensen, Susan M. (10 April 2017). \"Technical Inefficiency, Allocative Inefficiency, and Audit Pricing\". Journal of Accounting, Auditing & Finance. 33 (4): 580–600. doi:10.1177/0148558X17696760. - ^ Munoko, Ivy; Brown-Liburd, Helen L.; Vasarhelyi, Miklos (November 2020). \"The Ethical Implications of Using Artificial Intelligence in Auditing\". Journal of Business Ethics. 167 (2): 209–234. doi:10.1007/s10551-019-04407-1. - ^ Fadelli, Ingrid. \"LaundroGraph: Using deep learning to support anti–money laundering efforts\". techxplore.com. Retrieved 18 December 2022. - ^ Cardoso, Mário; Saleiro, Pedro; Bizarro, Pedro (2022). \"LaundroGraph: Self-Supervised Graph Representation Learning for Anti-Money Laundering\". Proceedings of the Third ACM International Conference on AI in Finance. pp. 130–138. arXiv:2210.14360. doi:10.1145/3533271.3561727. ISBN 978-1-4503-9376-8. - ^ Durkin, J. (2002). \"History and applications\". Expert Systems. Vol. 1. pp. 1–22. doi:10.1016/B978-012443880-4/50045-4. ISBN 978-0-12-443880-4. - ^ Chen, K.C.; Liang, Ting-peng (May 1989). \"Protrader: An Expert System for Program Trading\". Managerial Finance. 15 (5): 1–6. doi:10.1108/eb013623. - ^ Nielson, Norma; Brown, Carol E.; Phillips, Mary Ellen (July 1990). \"Expert Systems for Personal Financial Planning\". Journal of Financial Planning: 137–143. doi:10.11575/PRISM/33995. hdl:1880/48295. - ^ Senator, Ted E.; Goldberg, Henry G.; Wooton, Jerry; Cottini, Matthew A.; Khan, A.F. Umar; Kilinger, Christina D.; Llamas, Winston M.; Marrone, MichaeI P.; Wong, Raphael W.H. (1995). \"The FinCEN Artificial Intelligence System: Identifying Potential Money Laundering from Reports of Large Cash Transactions\" (PDF). IAAI-95 Proceedings. Archived from the original (PDF) on 2015-10-20. Retrieved 2019-01-14. - ^ Sutton, Steve G.; Holt, Matthew; Arnold, Vicky (September 2016). \"'The reports of my death are greatly exaggerated'—Artificial intelligence research in accounting\". International Journal of Accounting Information Systems. 22: 60–73. doi:10.1016/j.accinf.2016.07.005. - ^ Bender, Michael J.; Katopodis, Chris; Simonovic, Slobodan P. (1992). \"A prototype expert system for fishway design\". Environmental Monitoring and Assessment. 23 (1–3): 115–127. Bibcode:1992EMnAs..23..115B. doi:10.1007/BF00406956. PMID 24227094. - ^ Wayson, Roger L. (1989). \"Use of a Knowledge-Based Expert System to Maximize Airport Capacity in Harmony with Noise-Mitigation Plans\" (PDF). Transportation Research Record. 1218: 31–40. - ^ Vencill, A. M.; Speese, J. (1995). \"Potato Insect Expert System: Computerized Approach to Colorado Potato Beetle Management\". Journal of Economic Entomology. 88 (4): 944–954. doi:10.1093/jee/88.4.944. - ^ Jirka, Gerhard H.; Akar, Paul J. (1996). User's Manual for CORMIX: A Hydrodynamic Mixing Zone Model and Decision Support System for Pollutant Discharges into Surface Waters (PDF) (Report). U.S. Environmental Protection Agency. - ^ Chalmers, Dominic; MacKenzie, Niall G.; Carter, Sara (September 2021). \"Artificial Intelligence and Entrepreneurship: Implications for Venture Creation in the Fourth Industrial Revolution\". Entrepreneurship Theory and Practice. 45 (5): 1028–1053. doi:10.1177/1042258720934581. - ^ Thompsett, Louis (2025-02-04). \"What EU AI Act Means for Governance in Financial Sector\". fintechmagazine.com. Retrieved 2025-09-20. - ^ a b Szczytko, Jacek (2025-08-15). \"How will the AI Act alter the landscape for fintechs? Key requirements and penalties\". Dudkowiak & Putyra. Retrieved 2025-09-20. - ^ a b \"Regulation - EU - 2024/1689 - EN - EUR-Lex\". eur-lex.europa.eu. Retrieved 2025-09-20. - ^ \"AI Credit Regulations Affecting Lending Business 2025\". hesfintech. 10 October 2025. Archived from the original on 12 October 2025. Retrieved 10 October 2025. - ^ \"10 Promising AI Applications in Health Care\". Harvard Business Review. 10 May 2018. Archived from the original on 15 December 2018. Retrieved 28 August 2018. - ^ Lareyre, Fabien; Lê, Cong Duy; Ballaith, Ali; Adam, Cédric; Carrier, Marion; Amrani, Samantha; Caradu, Caroline; Raffort, Juliette (August 2022). \"Applications of Artificial Intelligence in Non-cardiac Vascular Diseases: A Bibliographic Analysis\". Angiology. 73 (7): 606–614. doi:10.1177/00033197211062280. PMID 34996315. - ^ \"What is artificial intelligence in medicine?\". IBM. 28 March 2024. Retrieved 19 April 2024. - ^ \"Microsoft Using AI to Accelerate Cancer Precision Medicine\". HealthITAnalytics. 29 October 2019. Retrieved 29 November 2020. - ^ Dina Bass (20 September 2016). \"Microsoft Develops AI to Help Cancer Doctors Find the Right Treatments\". Bloomberg L.P. Archived from the original on 11 May 2017. - ^ Gallagher, James (26 January 2017). \"Artificial intelligence 'as good as cancer doctors'\". BBC News. Archived from the original on 26 January 2017. Retrieved 26 January 2017. - ^ Langen, Pauline A.; Katz, Jeffrey S.; Dempsey, Gayle, eds. (18 October 1994), Remote monitoring of high-risk patients using artificial intelligence, archived from the original on 28 February 2017, retrieved 27 February 2017 - ^ Kermany, Daniel S.; Goldbaum, Michael; Cai, Wenjia; Valentim, Carolina C.S.; Liang, Huiying; Baxter, Sally L.; McKeown, Alex; Yang, Ge; Wu, Xiaokang; Yan, Fangbing; Dong, Justin; Prasadha, Made K.; Pei, Jacqueline; Ting, Magdalene Y.L.; Zhu, Jie; Li, Christina; Hewett, Sierra; Dong, Jason; Ziyar, Ian; Shi, Alexander; Zhang, Runze; Zheng, Lianghong; Hou, Rui; Shi, William; Fu, Xin; Duan, Yaou; Huu, Viet A.N.; Wen, Cindy; Zhang, Edward D.; Zhang, Charlotte L.; Li, Oulan; Wang, Xiaobo; Singer, Michael A.; Sun, Xiaodong; Xu, Jie; Tafreshi, Ali; Lewis, M. Anthony; Xia, Huimin; Zhang, Kang (February 2018). \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\". Cell. 172 (5): 1122–1131.e9. doi:10.1016/j.cell.2018.02.010. PMID 29474911. - ^ Senthilingam, Meera (12 May 2016). \"Are Autonomous Robots Your next Surgeons?\". CNN. Archived from the original on 3 December 2016. Retrieved 4 December 2016. - ^ Pumplun L, Fecho M, Wahl N, Peters F, Buxmann P (2021). \"Adoption of Machine Learning Systems for Medical Diagnostics in Clinics: Qualitative Interview Study\". Journal of Medical Internet Research. 23 (10) e29301. doi:10.2196/29301. PMC 8556641. PMID 34652275. - ^ Inglese, Marianna; Patel, Neva; Linton-Reid, Kristofer; Loreto, Flavia; Win, Zarni; Perry, Richard J.; Carswell, Christopher; Grech-Sollars, Matthew; Crum, William R.; Lu, Haonan; Malhotra, Paresh A.; Aboagye, Eric O. (20 June 2022). \"A predictive model using the mesoscopic architecture of the living brain to detect Alzheimer's disease\". Communications Medicine. 2 (1): 70. doi:10.1038/s43856-022-00133-4. PMC 9209493. PMID 35759330. - News report: \"Single MRI scan of the brain could detect Alzheimer's disease\". Physics World. 13 July 2022. Retrieved 19 July 2022. - ^ Yorita, Akihiro; Kubota, Naoyuki (2011). \"Cognitive Development in Partner Robots for Information Support to Elderly People\". IEEE Transactions on Autonomous Mental Development. 3 (1): 64–73. Bibcode:2011ITAMD...3...64Y. doi:10.1109/TAMD.2011.2105868. - ^ \"Artificial Intelligence Will Redesign Healthcare – The Medical Futurist\". The Medical Futurist. 4 August 2016. Retrieved 18 November 2016. - ^ Dönertaş, Handan Melike; Fuentealba, Matías; Partridge, Linda; Thornton, Janet M. (February 2019). \"Identifying Potential Ageing-Modulating Drugs In Silico\". Trends in Endocrinology & Metabolism. 30 (2): 118–131. doi:10.1016/j.tem.2018.11.005. PMC 6362144. PMID 30581056. - ^ Smer-Barreto, Vanessa; Quintanilla, Andrea; Elliot, Richard J. R.; Dawson, John C.; Sun, Jiugeng; Carragher, Neil O.; Acosta, Juan Carlos; Oyarzún, Diego A. (27 April 2022). \"Discovery of new senolytics using machine learning\". bioRxiv. doi:10.1101/2022.04.26.489505. hdl:10261/269843. - ^ Luxton, David D. (2014). \"Artificial intelligence in psychological practice: Current and future applications and implications\". Professional Psychology: Research and Practice. 45 (5): 332–339. doi:10.1037/a0034559. - ^ Randhawa, Gurjit S.; Soltysiak, Maximillian P. M.; Roz, Hadi El; Souza, Camila P. E. de; Hill, Kathleen A.; Kari, Lila (24 April 2020). \"Machine learning using intrinsic genomic signatures for rapid classification of novel pathogens: COVID-19 case study\". PLOS ONE. 15 (4) e0232391. Bibcode:2020PLoSO..1532391R. doi:10.1371/journal.pone.0232391. PMC 7182198. PMID 32330208. - ^ Ye, Jiarong; Yeh, Yin-Ting; Xue, Yuan; Wang, Ziyang; Zhang, Na; Liu, He; Zhang, Kunyan; Ricker, RyeAnne; Yu, Zhuohang; Roder, Allison; Perea Lopez, Nestor; Organtini, Lindsey; Greene, Wallace; Hafenstein, Susan; Lu, Huaguang; Ghedin, Elodie; Terrones, Mauricio; Huang, Shengxi; Huang, Sharon Xiaolei (7 June 2022). \"Accurate virus identification with interpretable Raman signatures by machine learning\". Proceedings of the National Academy of Sciences. 119 (23) e2118836119. arXiv:2206.02788. Bibcode:2022PNAS..11918836Y. doi:10.1073/pnas.2118836119. PMC 9191668. PMID 35653572. - ^ \"Artificial intelligence finds disease-related genes\". Linköping University. Retrieved 3 July 2022. - ^ \"Researchers use AI to detect new family of genes in gut bacteria\". UT Southwestern Medical Center. Retrieved 3 July 2022. - ^ a b c Zhavoronkov, Alex; Mamoshina, Polina; Vanhaelen, Quentin; Scheibye-Knudsen, Morten; Moskalev, Alexey; Aliper, Alex (2019). \"Artificial intelligence for aging and longevity research: Recent advances and perspectives\". Ageing Research Reviews. 49: 49–66. doi:10.1016/j.arr.2018.11.003. PMID 30472217. - ^ Adir, Omer; Poley, Maria; Chen, Gal; Froim, Sahar; Krinsky, Nitzan; Shklover, Jeny; Shainsky-Roitman, Janna; Lammers, Twan; Schroeder, Avi (April 2020). \"Integrating Artificial Intelligence and Nanotechnology for Precision Cancer Medicine\". Advanced Materials. 32 (13) 1901989. Bibcode:2020AdM....3201989A. doi:10.1002/adma.201901989. PMC 7124889. PMID 31286573. - ^ Moore, Phoebe V. (7 May 2019). \"OSH and the Future of Work: benefits and risks of artificial intelligence tools in workplaces\". EU-OSHA. pp. 3–7. Retrieved 30 July 2020. - ^ Howard, John (November 2019). \"Artificial intelligence: Implications for the future of work\". American Journal of Industrial Medicine. 62 (11): 917–926. Bibcode:2019AJIM...62..917H. doi:10.1002/ajim.23037. PMID 31436850. - ^ Gianatti, Toni-Louise (14 May 2020). \"How AI-Driven Algorithms Improve an Individual's Ergonomic Safety\". Occupational Health & Safety. Retrieved 30 July 2020. - ^ Meyers, Alysha R. (1 May 2019). \"AI and Workers' Comp\". NIOSH Science Blog. Retrieved 3 August 2020. - ^ Webb, Sydney; Siordia, Carlos; Bertke, Stephen; Bartlett, Diana; Reitz, Dan (26 February 2020). \"Artificial Intelligence Crowdsourcing Competition for Injury Surveillance\". NIOSH Science Blog. Retrieved 3 August 2020. - ^ Ferguson, Murray (19 April 2016). \"Artificial Intelligence: What's To Come for EHS... And When?\". EHS Today. Retrieved 30 July 2020. - ^ Paul, Debleena; Sanap, Gaurav; Shenoy, Snehal; Kalyane, Dnyaneshwar; Kalia, Kiran; Tekade, Rakesh K. (January 2021). \"Artificial intelligence in drug discovery and development\". Drug Discovery Today. 26 (1): 80–93. doi:10.1016/j.drudis.2020.10.010. PMC 7577280. PMID 33099022. - ^ \"Allchemy – Resource-aware AI for drug discovery\". Retrieved 29 May 2022. - ^ Wołos, Agnieszka; Koszelewski, Dominik; Roszak, Rafał; Szymkuć, Sara; Moskal, Martyna; Ostaszewski, Ryszard; Herrera, Brenden T.; Maier, Josef M.; Brezicki, Gordon; Samuel, Jonathon; Lummiss, Justin A. M.; McQuade, D. Tyler; Rogers, Luke; Grzybowski, Bartosz A. (April 2022). \"Computer-designed repurposing of chemical wastes into drugs\". Nature. 604 (7907): 668–676. Bibcode:2022Natur.604..668W. doi:10.1038/s41586-022-04503-9. PMID 35478240. - ^ Wołos, Agnieszka; Roszak, Rafał; Żądło-Dobrowolska, Anna; Beker, Wiktor; Mikulak-Klucznik, Barbara; Spólnik, Grzegorz; Dygas, Mirosław; Szymkuć, Sara; Grzybowski, Bartosz A. (25 September 2020). \"Synthetic connectivity, emergence, and self-regeneration in the network of prebiotic chemistry\". Science. 369 (6511) eaaw1955. doi:10.1126/science.aaw1955. PMID 32973002. - ^ Zhavoronkov, Alex; Ivanenkov, Yan A.; Aliper, Alex; Veselov, Mark S.; Aladinskiy, Vladimir A.; Aladinskaya, Anastasiya V.; Terentiev, Victor A.; Polykovskiy, Daniil A.; Kuznetsov, Maksim D.; Asadulaev, Arip; Volkov, Yury; Zholus, Artem; Shayakhmetov, Rim R.; Zhebrak, Alexander; Minaeva, Lidiya I.; Zagribelnyy, Bogdan A.; Lee, Lennart H.; Soll, Richard; Madge, David; Xing, Li; Guo, Tao; Aspuru-Guzik, Alán (September 2019). \"Deep learning enables rapid identification of potent DDR1 kinase inhibitors\". Nature Biotechnology. 37 (9): 1038–1040. doi:10.1038/s41587-019-0224-x. PMID 31477924. - ^ \"DeepMind is answering one of biology's biggest challenges\". The Economist. 30 November 2020. Retrieved 30 November 2020. - ^ Jeremy Kahn, Lessons from DeepMind's breakthrough in protein-folding A.I., Fortune, 1 December 2020 - ^ \"DeepMind uncovers structure of 200m proteins in scientific leap forward\". The Guardian. 2022-07-28. Retrieved 2022-07-28. - ^ \"AlphaFold reveals the structure of the protein universe\". DeepMind. 2022-07-28. Retrieved 2022-07-28. - ^ Nakamura, Satoshi (2009). \"Overcoming the language barrier with speech translation technology\". Science & Technology Trends Quarterly Review (31): 35–48. CORE output ID 236667511. - ^ a b Clark, Jack (8 December 2015). \"Why 2015 Was a Breakthrough Year in Artificial Intelligence\". Bloomberg.com. - ^ \"Can artificial intelligence really help us talk to the animals?\". The Guardian. 31 July 2022. Retrieved 30 August 2022. - ^ K. Mandal, G. S. Pradeep Ghantasala, Firoz Khan, R. Sathiyaraj, B. Balamurugan (2020). Natural Language Processing in Artificial Intelligence (1st ed.). Apple Academic Press. pp. 53–54. ISBN 978-0-367-80849-5. {{cite book}} : CS1 maint: multiple names: authors list (link) - ^ Buckley, Chris; Mozur, Paul (22 May 2019). \"How China Uses High-Tech Surveillance to Subdue Minorities\". The New York Times. - ^ \"Security lapse exposed a Chinese smart city surveillance system\". 3 May 2019. Archived from the original on 7 March 2021. Retrieved 14 September 2020. - ^ \"AI traffic signals to be installed in Bengaluru soon\". NextBigWhat. 24 September 2019. Retrieved 1 October 2019. - ^ Ashley, Kevin D. (2017). Artificial Intelligence and Legal Analytics. doi:10.1017/9781316761380. ISBN 978-1-107-17150-3.[page needed] - ^ Lohr, Steve (19 March 2017). \"A.I. Is Doing Legal Work. But It Won't Replace Lawyers, Yet\". The New York Times. - ^ Croft, Jane (2 May 2019). \"AI learns to read Korean, so you don't have to\". Financial Times. Retrieved 19 December 2019. - ^ Kleider-Offutt, Heather; Stevens, Beth; Mickes, Laura; Boogert, Stewart (3 April 2024). \"Application of artificial intelligence to eyewitness identification\". Cognitive Research: Principles and Implications. 9 (1): 19. doi:10.1186/s41235-024-00542-0. PMC 10991253. PMID 38568356. - ^ a b Jeff Larson; Julia Angwin (23 May 2016). \"How We Analyzed the COMPAS Recidivism Algorithm\". ProPublica. Archived from the original on 29 April 2019. Retrieved 19 June 2020. - ^ \"Commentary: Bad news. Artificial intelligence is biased\". CNA. 12 January 2019. Archived from the original on 12 January 2019. Retrieved 19 June 2020. - ^ a b Šimalčík, Matej (2023). \"Rule by Law\". In Kironska, Kristina; Turscanyi, Richard Q. (eds.). Contemporary China: a New Superpower?. Routledge. ISBN 978-1-03-239508-1. - ^ \"Digital Spectrometry\". 8 October 2018. - ^ US 9967696B2, \"Digital Spectrometry Patent\", published 2018-10-08 - ^ \"How artificial intelligence is moving from the lab to your kid's playroom\". The Washington Post. Retrieved 18 November 2016. - ^ \"Application of artificial intelligence in oil and gas industry: Exploring its impact\". 15 May 2019. - ^ Salvaterra, Neanda (14 October 2019). \"Oil and Gas Companies Turn to AI to Cut Costs\". The Wall Street Journal. - ^ Ornes, Stephen (August 27, 2020). \"Quanta Magazine – How Close Are Computers to Automating Mathematical Reasoning?\". - ^ \"AlphaGeometry: An Olympiad-level AI system for geometry\". Deepmind. Retrieved 26 January 2024. - ^ Roberts, Siobhan (17 January 2024). \"A.I.'s Latest Challenge: the Math Olympics\". The New York Times. Retrieved 26 January 2024. - ^ Trinh, Trieu H.; Wu, Yuhuai; Le, Quoc V.; He, He; Luong, Thang (2024). \"Solving olympiad geometry without human demonstrations\". Nature. 625 (7995): 476–482. Bibcode:2024Natur.625..476T. doi:10.1038/s41586-023-06747-5. PMC 10794143. PMID 38233616. - ^ \"Google DeepMind's new AI system can solve complex geometry problems\". MIT Technology Review. Retrieved 26 January 2024. - ^ Zia, Tehseen (January 24, 2024). \"AlphaGeometry: DeepMind's AI Masters Geometry Problems at Olympiad Levels\". Unite.ai. Retrieved 2024-05-03. - ^ a b c Congressional Research Service (2019). Artificial Intelligence and National Security (PDF). Washington, DC: Congressional Research Service.PD-notice - ^ Iraqi, Amjad (2024-04-03). \"'Lavender': The AI machine directing Israel's bombing spree in Gaza\". +972 Magazine. Retrieved 2024-04-06. - ^ Davies, Harry; McKernan, Bethan; Sabbagh, Dan (2023-12-01). \"'The Gospel': how Israel uses AI to select bombing targets in Gaza\". The Guardian. Retrieved 2023-12-04. - ^ Marti, J Werner (10 August 2024). \"Drohnen haben den Krieg in der Ukraine revolutioniert, doch sie sind empfindlich auf Störsender – deshalb sollen sie jetzt autonom operieren\". Neue Zürcher Zeitung (in German). Retrieved 10 August 2024. - ^ Guida, Michela; Caniato, Federico; Moretto, Antonella; Ronchi, Stefano (2023-03-01). \"The role of artificial intelligence in the procurement process: State of the art and research agenda\". Journal of Purchasing and Supply Management. 29 (2): 100823. doi:10.1016/j.pursup.2023.100823. ISSN 1478-4092. Archived from the original on December 29, 2025. Retrieved December 29, 2025. {{cite journal}} : CS1 maint: article number as page number (link) - ^ \"AI in Procurement | IBM\". www.ibm.com. 2023-08-02. Archived from the original on December 29, 2025. Retrieved 2025-12-29. - ^ Cheng, Evelyn (2024-11-12). \"China's Alibaba releases AI search tool for small businesses in Europe and the Americas\". CNBC. Retrieved 2026-01-06. - ^ \"Amazon Business Unveils Next Generation AI Solutions to Help Organizations and Small Businesses Save Time and Money\". November 12, 2025. Archived from the original on December 28, 2025. - ^ \"AI is transforming the way Shopify merchants do business\". Shopify. Archived from the original on 2025-12-11. Retrieved 2026-01-06. - ^ \"TikTok Adds AI Assistance Tools for Creators | Social Media Today\". www.socialmediatoday.com. Retrieved 2026-01-06. - ^ Brohan, Mark (2025-03-11). \"Alibaba's AI search engine surpasses 1 million users\". Digital Commerce 360. Retrieved 2026-01-07. - ^ Brohan, Mark (2025-11-12). \"Amazon expands AI footprint in B2B buying with launch of new tool\". Digital Commerce 360. Retrieved 2026-01-07. - ^ \"What are the security risks of open sourcing the Twitter algorithm?\". VentureBeat. 27 May 2022. Retrieved 29 May 2022. - ^ \"Examining algorithmic amplification of political content on Twitter\". Retrieved 29 May 2022. - ^ Park, SoHyun; Oh, Heung-Kwon; Park, Gibeom; Suh, Bongwon; Bae, Woo Kyung; Kim, Jin Won; Yoon, Hyuk; Kim, Duck-Woo; Kang, Sung-Bum (February 2016). \"The Source and Credibility of Colorectal Cancer Information on Twitter\". Medicine. 95 (7) e2775. doi:10.1097/MD.0000000000002775. PMC 4998625. PMID 26886625. - ^ Efthimion, Phillip; Payne, Scott; Proferes, Nicholas (20 July 2018). \"Supervised Machine Learning Bot Detection Techniques to Identify Social Twitter Bots\". SMU Data Science Review. 1 (2). - ^ \"The online information environment\" (PDF). Retrieved 21 February 2022. - ^ Islam, Md Rafiqul; Liu, Shaowu; Wang, Xianzhi; Xu, Guandong (29 September 2020). \"Deep learning for misinformation detection on online social networks: a survey and new perspectives\". Social Network Analysis and Mining. 10 (1): 82. doi:10.1007/s13278-020-00696-x. PMC 7524036. PMID 33014173. - ^ Mohseni, Sina; Ragan, Eric (4 December 2018). \"Combating Fake News with Interpretable News Feed Algorithms\". arXiv:1811.12349 [cs.SI]. - ^ \"How artificial intelligence may be making you buy things\". BBC News. 9 November 2020. Retrieved 9 November 2020. - ^ Busby, Mattha (30 April 2018). \"Revealed: how bookies use AI to keep gamblers hooked\". The Guardian. - ^ Rowinski, Dan (15 January 2013). \"Virtual Personal Assistants & The Future Of Your Smartphone [Infographic]\". ReadWrite. Archived from the original on 22 December 2015. - ^ Roose, Kevin (16 February 2023). \"Bing's A.I. Chat: 'I Want to Be Alive. 😈'\". The New York Times. - ^ Galego Hernandes, Paulo R.; Floret, Camila P.; Cardozo De Almeida, Katia F.; Da Silva, Vinicius Camargo; Papa, Joso Paulo; Pontara Da Costa, Kelton A. (2021). \"Phishing Detection Using URL-based XAI Techniques\". 2021 IEEE Symposium Series on Computational Intelligence (SSCI). pp. 01–06. doi:10.1109/SSCI50451.2021.9659981. ISBN 978-1-7281-9048-8. - ^ Jáñez-Martino, Francisco; Alaiz-Rodríguez, Rocío; González-Castro, Víctor; Fidalgo, Eduardo; Alegre, Enrique (2023-02-01). \"A review of spam email detection: analysis of spammer strategies and the dataset shift problem\". Artificial Intelligence Review. 56 (2): 1145–1173. doi:10.1007/s10462-022-10195-4. hdl:10612/14967. - ^ Kapan, Sibel; Sora Gunal, Efnan (January 2023). \"Improved Phishing Attack Detection with Machine Learning: A Comprehensive Evaluation of Classifiers and Features\". Applied Sciences. 13 (24) 13269. doi:10.3390/app132413269. - ^ Heath, Nick (11 December 2020). \"What is AI? Everything you need to know about Artificial Intelligence\". ZDNet. Retrieved 1 March 2021. - ^ \"China's massive investment in artificial intelligence has an insidious downside\". Science AAAS. February 7, 2018. Retrieved February 23, 2018. - ^ \"China bets on facial recognition in big drive for total surveillance\". The Washington Post. 2018. Retrieved February 23, 2018. - ^ \"Facial recognition forced on 800 million Chinese internet users\". Radio France Internationale. 15 October 2019. Retrieved April 21, 2024. - ^ \"Country policy and information note: Falun Gong, China, November 2023 (accessible)\". The United Kingdom Government. April 4, 2024. Retrieved April 21, 2024. - ^ Techredacteur, Joost Schellevis (December 16, 2016). \"Politie gaat verdachten opsporen met gezichtsherkenning\". nos.nl (in Dutch). Retrieved September 22, 2019. - ^ Boon, Lex (August 25, 2018). \"Meekijken met de 226 gemeentecamera's\". Het Parool (in Dutch). Retrieved September 22, 2019. - ^ \"Successful and timely uptake of artificial intelligence in science in the EU – Scientific Advice Mechanism\". Retrieved 2024-04-16. - ^ \"AI in science evidence review report – Scientific Advice Mechanism\". Retrieved 2024-04-16. - ^ Assael, Yannis; Sommerschield, Thea; Shillingford, Brendan; Bordbar, Mahyar; Pavlopoulos, John; Chatzipanagiotou, Marita; Androutsopoulos, Ion; Prag, Jonathan; de Freitas, Nando (March 2022). \"Restoring and attributing ancient texts using deep neural networks\". Nature. 603 (7900): 280–283. Bibcode:2022Natur.603..280A. doi:10.1038/s41586-022-04448-z. PMC 8907065. PMID 35264762. - ^ Mantovan, Lorenzo; Nanni, Loris (September 2020). \"The Computerization of Archaeology: Survey on Artificial Intelligence Techniques\". SN Computer Science. 1 (5) 267. arXiv:2005.02863. doi:10.1007/s42979-020-00286-w. - ^ Mondal, Mayukh; Bertranpetit, Jaume; Lao, Oscar (December 2019). \"Approximate Bayesian computation with deep learning supports a third archaic introgression in Asia and Oceania\". Nature Communications. 10 (1): 246. Bibcode:2019NatCo..10..246M. doi:10.1038/s41467-018-08089-7. PMC 6335398. PMID 30651539. - ^ Tanti, Marc; Berruyer, Camille; Tafforeau, Paul; Muscat, Adrian; Farrugia, Reuben; Scerri, Kenneth; Valentino, Gianluca; Solé, V. Armando; Briffa, Johann A. (15 December 2021). \"Automated segmentation of microtomography imaging of Egyptian mummies\". PLOS ONE. 16 (12) e0260707. arXiv:2105.06738. Bibcode:2021PLoSO..1660707T. doi:10.1371/journal.pone.0260707. PMC 8673632. PMID 34910736. - ^ \"DeepMind AI learns physics by watching videos that don't make sense\". New Scientist. Retrieved 21 August 2022. - ^ Piloto, Luis S.; Weinstein, Ari; Battaglia, Peter; Botvinick, Matthew (11 July 2022). \"Intuitive physics learning in a deep-learning model inspired by developmental psychology\". Nature Human Behaviour. 6 (9): 1257–1267. doi:10.1038/s41562-022-01394-8. PMC 9489531. PMID 35817932. - ^ a b Feldman, Andrey (11 August 2022). \"Artificial physicist to unravel the laws of nature\". Advanced Science News. Retrieved 21 August 2022. - ^ Chen, Boyuan; Huang, Kuang; Raghupathi, Sunand; Chandratreya, Ishaan; Du, Qiang; Lipson, Hod (July 2022). \"Automated discovery of fundamental variables hidden in experimental data\". Nature Computational Science. 2 (7): 433–442. doi:10.1038/s43588-022-00281-6. PMID 38177869. - ^ Nuñez, Michael (2023-11-29). \"Google DeepMind's materials AI has already discovered 2.2 million new crystals\". VentureBeat. Retrieved 2023-12-19. - ^ Merchant, Amil; Batzner, Simon; Schoenholz, Samuel S.; Aykol, Muratahan; Cheon, Gowoon; Cubuk, Ekin Dogus (December 2023). \"Scaling deep learning for materials discovery\". Nature. 624 (7990): 80–85. Bibcode:2023Natur.624...80M. doi:10.1038/s41586-023-06735-9. PMC 10700131. PMID 38030720. - ^ Peplow, Mark (29 November 2023). \"Google AI and robots join forces to build new materials\". Nature. doi:10.1038/d41586-023-03745-5. PMID 38030771. - ^ Yanamandra, Kaushik; Chen, Guan Lin; Xu, Xianbo; Mac, Gary; Gupta, Nikhil (29 September 2020). \"Reverse engineering of additive manufactured composite part by toolpath reconstruction using imaging and machine learning\". Composites Science and Technology. 198 108318. doi:10.1016/j.compscitech.2020.108318. - ^ Anderson, Blake; Storlie, Curtis; Yates, Micah; McPhall, Aaron (2014). \"Automating Reverse Engineering with Machine Learning Techniques\". Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop. pp. 103–112. doi:10.1145/2666652.2666665. ISBN 978-1-4503-3153-1. - ^ Liu, Wenye; Chang, Chip-Hong; Wang, Xueyang; Liu, Chen; Fung, Jason M.; Ebrahimabadi, Mohammad; Karimi, Naghmeh; Meng, Xingyu; Basu, Kanad (June 2021). \"Two Sides of the Same Coin: Boons and Banes of Machine Learning in Hardware Security\". IEEE Journal on Emerging and Selected Topics in Circuits and Systems. 11 (2): 228–251. Bibcode:2021IJEST..11..228L. doi:10.1109/JETCAS.2021.3084400. hdl:10356/155876. - ^ \"DARPA Taps GrammaTech for Artificial Intelligence Exploration (AIE) Program\". www.businesswire.com. 7 January 2021. Retrieved 10 January 2023. - ^ Greenberg, Andy. \"How to Steal an AI\". Wired. Retrieved 10 January 2023. - ^ Sanchez-Lengeling, Benjamin; Aspuru-Guzik, Alán (27 July 2018). \"Inverse molecular design using machine learning: Generative models for matter engineering\". Science. 361 (6400): 360–365. Bibcode:2018Sci...361..360S. doi:10.1126/science.aat2663. PMID 30049875. - ^ a b \"Biologists train AI to generate medicines and vaccines\". University of Washington-Harborview Medical Center. - ^ a b Wang, Jue; Lisanza, Sidney; Juergens, David; Tischer, Doug; Watson, Joseph L.; Castro, Karla M.; Ragotte, Robert; Saragovi, Amijai; Milles, Lukas F.; Baek, Minkyung; Anishchenko, Ivan; Yang, Wei; Hicks, Derrick R.; Expòsit, Marc; Schlichthaerle, Thomas; Chun, Jung-Ho; Dauparas, Justas; Bennett, Nathaniel; Wicky, Basile I. M.; Muenks, Andrew; DiMaio, Frank; Correia, Bruno; Ovchinnikov, Sergey; Baker, David (22 July 2022). \"Scaffolding protein functional sites using deep learning\". Science. 377 (6604): 387–394. Bibcode:2022Sci...377..387W. doi:10.1126/science.abn2100. PMC 9621694. PMID 35862514. - ^ Teemu, Rintala (17 June 2019). Using Boolean network extraction of trained neural networks to reverse-engineer gene-regulatory networks from time-series data (Master's in Life Science Technologies thesis). Aalto University.[page needed] - ^ Ball, Nicholas M.; Brunner, Robert J. (July 2010). \"Data mining and machine learning in astronomy\". International Journal of Modern Physics D. 19 (7): 1049–1106. arXiv:0906.2173. Bibcode:2010IJMPD..19.1049B. doi:10.1142/S0218271810017160. - ^ a b Shekhtman, Svetlana (15 November 2019). \"NASA Applying AI Technologies to Problems in Space Science\". NASA. Retrieved 30 May 2022. - ^ Fluke, Christopher J.; Jacobs, Colin (March 2020). \"Surveying the reach and maturity of machine learning and artificial intelligence in astronomy\". WIREs Data Mining and Knowledge Discovery. 10 (2) e1349. arXiv:1912.02934. Bibcode:2020WDMKD..10.1349F. doi:10.1002/widm.1349. - ^ Pultarova, Tereza (29 April 2021). \"Artificial intelligence is learning how to dodge space junk in orbit\". Space.com. Retrieved 3 July 2022. - ^ Mohan, Jaya Preethi; Tejaswi, N. (2020). \"A Study on Embedding the Artificial Intelligence and Machine Learning into Space Exploration and Astronomy\". Emerging Trends in Computing and Expert Technology. Lecture Notes on Data Engineering and Communications Technologies. Vol. 35. pp. 1295–1302. doi:10.1007/978-3-030-32150-5_131. ISBN 978-3-030-32149-9. - ^ Rees, Martin (30 April 2022). \"Could space-going billionaires be the vanguard of a cosmic revolution? | Martin Rees\". The Guardian. Retrieved 29 May 2022. - ^ Gutowska, Małgorzata; Scriney, Michael; McCarren, Andrew (December 2019). Identifying extra-terrestrial intelligence using machine learning. 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science. - ^ Zhang, Yunfan Gerry; Gajjar, Vishal; Foster, Griffin; Siemion, Andrew; Cordes, James; Law, Casey; Wang, Yu (2018). \"Fast Radio Burst 121102 Pulse Detection and Periodicity: A Machine Learning Approach\". The Astrophysical Journal. 866 (2): 149. arXiv:1809.03043. Bibcode:2018ApJ...866..149Z. doi:10.3847/1538-4357/aadf31. - ^ Nanda, Lakshay; V, Santhi (2019). \"SETI (Search for Extra Terrestrial Intelligence) Signal Classification using Machine Learning\". 2019 International Conference on Smart Systems and Inventive Technology (ICSSIT). pp. 499–504. doi:10.1109/ICSSIT46314.2019.8987793. ISBN 978-1-7281-2119-2. - ^ Gajjar, Vishal; Siemion, Andrew; Croft, Steve; Brzycki, Bryan; Burgay, Marta; Carozzi, Tobia; Concu, Raimondo; Czech, Daniel; DeBoer, David; DeMarines, Julia; Drew, Jamie; Enriquez, J. Emilio; Fawcett, James; Gallagher, Peter; Garrett, Michael; Gizani, Nectaria; Hellbourg, Greg; Holder, Jamie; Isaacson, Howard; Kudale, Sanjay; Lacki, Brian; Lebofsky, Matthew; Li, Di; MacMahon, David H. E.; McCauley, Joe; Melis, Andrea; Molinari, Emilio; Murphy, Pearse; Perrodin, Delphine; Pilia, Maura; Price, Danny C.; Webb, Claire; Werthimer, Dan; Williams, David; Worden, Pete; Zarka, Philippe; Zhang, Yunfan Gerry (2 August 2019). \"The Breakthrough Listen Search for Extraterrestrial Intelligence\". Bulletin of the American Astronomical Society. 51 (7): 223. arXiv:1907.05519. Bibcode:2019BAAS...51g.223G. - ^ \"SkyCAM-5 - Chair of Computer Science VIII - Aerospace Information Technology\". University of Würzburg. Retrieved 29 May 2022. - ^ \"Project Galileo: The search for alien tech hiding in our Solar System\". BBC Science Focus Magazine. Retrieved 29 May 2022. - ^ \"'Something's coming': is America finally ready to take UFOs seriously?\". The Guardian. 5 February 2022. Retrieved 29 May 2022. - ^ David, Leonard (27 January 2022). \"2022 could be a turning point in the study of UFOs\". livescience.com. Retrieved 29 May 2022. - ^ Gritz, Jennie Rothenberg. \"The Wonder of Avi Loeb\". Retrieved 29 May 2022. - ^ Mann, Adam. \"Avi Loeb's Galileo Project Will Search for Evidence of Alien Visitation\". Scientific American. Retrieved 29 May 2022. - ^ \"Galileo Project – Activities\". projects.iq.harvard.edu. Retrieved 29 May 2022. - ^ \"The Galileo Project: Harvard researchers to search for signs of alien technology\". Sky News. - ^ Zapata Trujillo, Juan C.; Syme, Anna-Maree; Rowell, Keiran N.; Burns, Brendan P.; Clark, Ebubekir S.; Gorman, Maire N.; Jacob, Lorrie S. D.; Kapodistrias, Panayioti; Kedziora, David J.; Lempriere, Felix A. R.; Medcraft, Chris; O'Sullivan, Jensen; Robertson, Evan G.; Soares, Georgia G.; Steller, Luke; Teece, Bronwyn L.; Tremblay, Chenoa D.; Sousa-Silva, Clara; McKemmish, Laura K. (2021). \"Computational Infrared Spectroscopy of 958 Phosphorus-Bearing Molecules\". Frontiers in Astronomy and Space Sciences. 8 639068: 43. arXiv:2105.08897. Bibcode:2021FrASS...8...43Z. doi:10.3389/fspas.2021.639068. - ^ \"Chemists debate machine learning's future in synthesis planning and ask for open data\". cen.acs.org. Retrieved 29 May 2022. - ^ \"Machine learning reveals recipe for building artificial proteins\". phys.org. Retrieved 17 August 2020. - ^ Russ, William P.; Figliuzzi, Matteo; Stocker, Christian; Barrat-Charlaix, Pierre; Socolich, Michael; Kast, Peter; Hilvert, Donald; Monasson, Remi; Cocco, Simona; Weigt, Martin; Ranganathan, Rama (2020). \"An evolution-based model for designing chorismatemutase enzymes\". Science. 369 (6502): 440–445. Bibcode:2020Sci...369..440R. doi:10.1126/science.aba3304. PMID 32703877. S2CID 220714458. - ^ Stocker, Sina; Csányi, Gábor; Reuter, Karsten; Margraf, Johannes T. (30 October 2020). \"Machine learning in chemical reaction space\". Nature Communications. 11 (1): 5505. Bibcode:2020NatCo..11.5505S. doi:10.1038/s41467-020-19267-x. PMC 7603480. PMID 33127879. - ^ Yirka, Bob. \"Repurposed drug-seeking AI system generates 40,000 possible chemical weapons in just six hours\". techxplore.com. Retrieved 19 April 2022. - ^ Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (March 2022). \"Dual use of artificial-intelligence-powered drug discovery\". Nature Machine Intelligence. 4 (3): 189–191. doi:10.1038/s42256-022-00465-9. ISSN 2522-5839. PMC 9544280. PMID 36211133. S2CID 247302391. - ^ \"AI drug algorithms can be flipped to generate bioweapons\". www.theregister.com. Retrieved 24 April 2022. - ^ Hansen, Justine Y.; Markello, Ross D.; Vogel, Jacob W.; Seidlitz, Jakob; Bzdok, Danilo; Misic, Bratislav (September 2021). \"Mapping gene transcription and neurocognition across human neocortex\". Nature Human Behaviour. 5 (9): 1240–1250. doi:10.1038/s41562-021-01082-z. PMID 33767429. - ^ Vo ngoc, Long; Huang, Cassidy Yunjing; Cassidy, California Jack; Medrano, Claudia; Kadonaga, James T. (September 2020). \"Identification of the human DPR core promoter element using machine learning\". Nature. 585 (7825): 459–463. Bibcode:2020Natur.585..459V. doi:10.1038/s41586-020-2689-7. PMC 7501168. PMID 32908305. - ^ Bijun, Zhang; Ting, Fan (2022). \"Knowledge structure and emerging trends in the application of deep learning in genetics research: A bibliometric analysis [2000–2021]\". Frontiers in Genetics. 13 951939. doi:10.3389/fgene.2022.951939. PMC 9445221. PMID 36081985. - ^ Radivojević, Tijana; Costello, Zak; Workman, Kenneth; Garcia Martin, Hector (25 September 2020). \"A machine learning Automated Recommendation Tool for synthetic biology\". Nature Communications. 11 (1): 4879. arXiv:1911.11091. Bibcode:2020NatCo..11.4879R. doi:10.1038/s41467-020-18008-4. PMC 7519645. PMID 32978379. - ^ a b Pablo Carbonell; Tijana Radivojevic; Héctor García Martín* (2019). \"Opportunities at the Intersection of Synthetic Biology, Machine Learning, and Automation\". ACS Synthetic Biology. 8 (7): 1474–1477. doi:10.1021/acssynbio.8b00540. hdl:20.500.11824/998. PMID 31319671. - ^ Gadzhimagomedova, Z. M.; Pashkov, D. M.; Kirsanova, D. Yu.; Soldatov, S. A.; Butakova, M. A.; Chernov, A. V.; Soldatov, A. V. (February 2022). \"Artificial Intelligence for Nanostructured Materials\". Nanobiotechnology Reports. 17 (1): 1–9. doi:10.1134/S2635167622010049. - ^ Mirzaei, Mahsa; Furxhi, Irini; Murphy, Finbarr; Mullins, Martin (July 2021). \"A Machine Learning Tool to Predict the Antibacterial Capacity of Nanoparticles\". Nanomaterials. 11 (7): 1774. doi:10.3390/nano11071774. PMC 8308172. PMID 34361160. - ^ Chen, Angela (25 April 2018). \"How AI is helping us discover materials faster than ever\". The Verge. Retrieved 30 May 2022. - ^ Talapatra, Anjana; Boluki, S.; Duong, T.; Qian, X.; Dougherty, E.; Arróyave, R. (26 November 2018). \"Autonomous efficient experiment design for materials discovery with Bayesian model averaging\". Physical Review Materials. 2 (11) 113803. arXiv:1803.05460. Bibcode:2018PhRvM...2k3803T. doi:10.1103/PhysRevMaterials.2.113803. - ^ Zhao, Yicheng; Zhang, Jiyun; Xu, Zhengwei; Sun, Shijing; Langner, Stefan; Hartono, Noor Titan Putri; Heumueller, Thomas; Hou, Yi; Elia, Jack; Li, Ning; Matt, Gebhard J.; Du, Xiaoyan; Meng, Wei; Osvet, Andres; Zhang, Kaicheng; Stubhan, Tobias; Feng, Yexin; Hauch, Jens; Sargent, Edward H.; Buonassisi, Tonio; Brabec, Christoph J. (13 April 2021). \"Discovery of temperature-induced stability reversal in perovskites using high-throughput robotic learning\". Nature Communications. 12 (1): 2191. Bibcode:2021NatCo..12.2191Z. doi:10.1038/s41467-021-22472-x. PMC 8044090. PMID 33850155. - ^ Anne Johnson; Emily Grumbling (2019). Implications of artificial intelligence for cybersecurity: proceedings of a workshop. Washington, DC: National Academies Press. pp. 4–5. ISBN 978-0-309-49451-9. OCLC 1134854973. Retrieved 2025-05-12. - ^ Kocher, Geeta; Kumar, Gulshan (August 2021). \"Machine learning and deep learning methods for intrusion detection systems: recent developments and challenges\". Soft Computing. 25 (15): 9731–9763. doi:10.1007/s00500-021-05893-0. - ^ Kant, Daniel; Johannsen, Andreas (16 January 2022). \"Evaluation of AI-based use cases for enhancing the cyber security defense of small and medium-sized companies (SMEs)\". Electronic Imaging. 34 (3): 387–1–387–8. doi:10.2352/EI.2022.34.3.MOBMU-387. - ^ Randrianasolo, Arisoa (2012). Artificial intelligence in computer security: Detection, temporary repair and defense (Thesis). p. vii. hdl:2346/45196. - ^ Sahil; Sood, Sandeep; Mehmi, Sandeep; Dogra, Shikha (2015). \"Artificial intelligence for designing user profiling system for cloud computing security: Experiment\". 2015 International Conference on Advances in Computer Engineering and Applications. pp. 51–58. doi:10.1109/ICACEA.2015.7164645. ISBN 978-1-4673-6911-4. - ^ Parisi, Alessandro (2019). Hands-On Artificial Intelligence for Cybersecurity: Implement smart AI systems for preventing ",
    "text_length": 120000,
    "depth": 1,
    "crawled_at": "2026-01-09T19:31:36.065689"
  }
]